"""
å®Œæ•´çš„ä¸Šä¸‹æ–‡å·¥ç¨‹æ¼”ç¤º (Complete Context Engineering Demo)
åœ¨ä¸€ä¸ªæ¼”ç¤ºä¸­å±•ç¤ºæ‰€æœ‰å››ä¸ªæ ¸å¿ƒæ¦‚å¿µï¼šå†™å…¥ã€é€‰æ‹©ã€å‹ç¼©ã€éš”ç¦»

è¿™ä¸ªæ¼”ç¤ºæ¨¡æ‹Ÿä¸€ä¸ªæ™ºèƒ½ç ”ç©¶åŠ©æ‰‹ï¼Œå±•ç¤ºï¼š
1. Write Context: å°†ä¿¡æ¯å†™å…¥çŠ¶æ€å’Œé•¿æœŸè®°å¿†
2. Select Context: ä»çŠ¶æ€ã€è®°å¿†ã€å·¥å…·å’ŒçŸ¥è¯†åº“ä¸­é€‰æ‹©ç›¸å…³ä¿¡æ¯
3. Compress Context: å‹ç¼©é•¿å¯¹è¯å’Œå·¥å…·è¾“å‡º
4. Isolate Context: ä½¿ç”¨å¤šæ™ºèƒ½ä½“å’Œæ²™ç›’éš”ç¦»ä¸åŒç±»å‹çš„å¤„ç†
"""

import os
import uuid
import json
from typing import TypedDict, List, Dict, Any, Literal
from langchain.chat_models import init_chat_model
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage
from langchain_core.vectorstores import InMemoryVectorStore
from langchain_community.document_loaders import WebBaseLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.embeddings import init_embeddings
from langchain.tools.retriever import create_retriever_tool
from langgraph.graph import StateGraph, START, END, MessagesState
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.store.memory import InMemoryStore
from langgraph.store.base import BaseStore
from langgraph.prebuilt import create_react_agent
from rich.console import Console
from rich.panel import Panel
from rich.table import Table
from rich.progress import track
from rich.logging import RichHandler
import time
import logging

console = Console()

# è®¾ç½®æ—¥å¿—è®°å½•
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[RichHandler(console=console, rich_tracebacks=True)]
)
logger = logging.getLogger("ContextEngineering")

# ================================
# 1. ç¯å¢ƒè®¾ç½®å’Œå·¥å…·å‡½æ•°
# ================================

def setup_environment():
    """è®¾ç½®å¿…è¦çš„ç¯å¢ƒå˜é‡"""
    required_keys = ["ANTHROPIC_API_KEY", "OPENAI_API_KEY"]
    for key in required_keys:
        if not os.environ.get(key):
            api_key = input(f"è¯·è¾“å…¥æ‚¨çš„ {key}: ")
            os.environ[key] = api_key

def log_context_operation(operation_type: str, location: str, key: str, content_preview: str):
    """è®°å½•ä¸Šä¸‹æ–‡æ“ä½œåˆ°æ—¥å¿—"""
    logger.info(f"ğŸ”„ [{operation_type}] {location} | Key: {key} | Content: {content_preview[:100]}...")

def log_context_selection(source: str, selected_items: int, details: str = ""):
    """è®°å½•ä¸Šä¸‹æ–‡é€‰æ‹©æ“ä½œ"""
    logger.info(f"ğŸ¯ [SELECT] From {source} | Selected: {selected_items} items | {details}")

def display_section_header(title: str, description: str, color: str = "blue"):
    """æ˜¾ç¤ºç« èŠ‚æ ‡é¢˜"""
    console.print(Panel(
        f"[bold]{title}[/bold]\n{description}",
        border_style=color,
        padding=(1, 2)
    ))

# ================================
# 2. çŠ¶æ€å®šä¹‰
# ================================

class ComprehensiveState(TypedDict):
    """ç»¼åˆæ¼”ç¤ºçš„çŠ¶æ€å®šä¹‰"""
    # åŸºæœ¬ä¿¡æ¯
    research_topic: str
    current_query: str
    conversation_round: int
    
    # Write Context - å†™å…¥çš„ä¿¡æ¯
    initial_analysis: str
    research_plan: str
    collected_data: List[str]
    tool_results: List[Dict]
    
    # Select Context - é€‰æ‹©çš„ä¸Šä¸‹æ–‡
    selected_memories: List[Dict]
    relevant_tools: List[str]
    retrieved_knowledge: str
    context_selections: List[Dict]
    
    # Compress Context - å‹ç¼©çš„ä¿¡æ¯
    conversation_summary: str
    compressed_findings: str
    
    # Isolate Context - éš”ç¦»çš„å¤„ç†ç»“æœ
    specialist_reports: Dict[str, str]
    
    # æ¶ˆæ¯å†å²å’Œå¯¹è¯ç®¡ç†
    messages: List[Any]
    conversation_history: List[Dict]
    
    # å…ƒæ•°æ®å’Œæ—¥å¿—
    processing_steps: List[str]
    context_operations_log: List[Dict]
    token_usage: Dict[str, int]

# ================================
# 3. æ ¸å¿ƒç»„ä»¶åˆå§‹åŒ–
# ================================

def create_research_tools():
    """åˆ›å»ºå¤šä¸ªç ”ç©¶å·¥å…·"""
    def web_search_tool(query: str) -> str:
        """æ¨¡æ‹Ÿç½‘ç»œæœç´¢å·¥å…·"""
        search_results = {
            "äººå·¥æ™ºèƒ½": "AIæŠ€æœ¯åœ¨æ•™è‚²ã€åŒ»ç–—ã€é‡‘èç­‰é¢†åŸŸå¹¿æ³›åº”ç”¨ï¼Œé¢„è®¡2024å¹´å¸‚åœºè§„æ¨¡å°†è¾¾åˆ°5000äº¿ç¾å…ƒã€‚",
            "æ•™è‚²": "åœ¨çº¿æ•™è‚²å¸‚åœºå¿«é€Ÿå¢é•¿ï¼Œä¸ªæ€§åŒ–å­¦ä¹ æˆä¸ºä¸»æµè¶‹åŠ¿ï¼ŒAIè¾…å¯¼ç³»ç»Ÿæ•ˆæœæ˜¾è‘—ã€‚",
            "æœºå™¨å­¦ä¹ ": "æ·±åº¦å­¦ä¹ ã€å¼ºåŒ–å­¦ä¹ ç­‰æŠ€æœ¯ä¸æ–­å‘å±•ï¼Œåœ¨å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸå–å¾—çªç ´ã€‚",
            "default": f"å…³äº'{query}'çš„æœç´¢ç»“æœï¼šè¿™æ˜¯ä¸€ä¸ªé‡è¦çš„ç ”ç©¶é¢†åŸŸï¼Œå…·æœ‰å¹¿é˜”çš„å‘å±•å‰æ™¯å’Œåº”ç”¨ä»·å€¼ã€‚"
        }
        for key in search_results:
            if key in query.lower():
                return search_results[key]
        return search_results["default"]
    
    def data_analysis_tool(data_type: str) -> str:
        """æ¨¡æ‹Ÿæ•°æ®åˆ†æå·¥å…·"""
        analysis_results = {
            "æ•™è‚²æ•°æ®": "å­¦ç”Ÿå­¦ä¹ æ•ˆæœæå‡25%ï¼Œæ•™å¸ˆå·¥ä½œæ•ˆç‡æé«˜40%ï¼Œä¸ªæ€§åŒ–æ¨èå‡†ç¡®ç‡è¾¾åˆ°85%ã€‚",
            "å¸‚åœºæ•°æ®": "AIæ•™è‚²å¸‚åœºå¹´å¤åˆå¢é•¿ç‡15%ï¼Œå¤´éƒ¨ä¼ä¸šå æ®60%å¸‚åœºä»½é¢ã€‚",
            "ç”¨æˆ·æ•°æ®": "ç”¨æˆ·æ»¡æ„åº¦92%ï¼Œæ—¥æ´»è·ƒç”¨æˆ·å¢é•¿30%ï¼Œå¹³å‡å­¦ä¹ æ—¶é•¿å¢åŠ 45åˆ†é’Ÿã€‚",
            "default": f"å¯¹{data_type}çš„åˆ†ææ˜¾ç¤ºï¼šæ•°æ®è´¨é‡è‰¯å¥½ï¼Œè¶‹åŠ¿å‘ä¸Šï¼Œå…·æœ‰é‡è¦å‚è€ƒä»·å€¼ã€‚"
        }
        for key in analysis_results:
            if key in data_type:
                return analysis_results[key]
        return analysis_results["default"]
    
    def literature_search_tool(topic: str) -> str:
        """æ¨¡æ‹Ÿæ–‡çŒ®æœç´¢å·¥å…·"""
        return f"æ‰¾åˆ°å…³äº'{topic}'çš„ç›¸å…³æ–‡çŒ®15ç¯‡ï¼ŒåŒ…æ‹¬é¡¶çº§æœŸåˆŠè®ºæ–‡8ç¯‡ï¼Œä¼šè®®è®ºæ–‡7ç¯‡ã€‚ä¸»è¦ç ”ç©¶æ–¹å‘åŒ…æ‹¬æŠ€æœ¯åˆ›æ–°ã€åº”ç”¨å®è·µã€æ•ˆæœè¯„ä¼°ç­‰ã€‚"
    
    return {
        "web_search": web_search_tool,
        "data_analysis": data_analysis_tool,
        "literature_search": literature_search_tool
    }

def initialize_components():
    """åˆå§‹åŒ–æ‰€æœ‰å¿…è¦çš„ç»„ä»¶"""
    setup_environment()
    
    # LLM
    llm = init_chat_model("anthropic:claude-3-5-sonnet-20241022", temperature=0.7)
    
    # å­˜å‚¨ç»„ä»¶
    checkpointer = InMemorySaver()
    memory_store = InMemoryStore()
    namespace = ("demo_user", "research_assistant")
    
    # åµŒå…¥å’Œå‘é‡å­˜å‚¨
    embeddings = init_embeddings("openai:text-embedding-3-small")
    
    # æ‰©å±•çš„çŸ¥è¯†åº“
    sample_docs = [
        "äººå·¥æ™ºèƒ½åœ¨æ•™è‚²ä¸­çš„åº”ç”¨åŒ…æ‹¬ä¸ªæ€§åŒ–å­¦ä¹ ã€æ™ºèƒ½è¾…å¯¼ç³»ç»Ÿå’Œè‡ªåŠ¨è¯„ä¼°ã€‚ç ”ç©¶è¡¨æ˜AIå¯ä»¥æé«˜å­¦ä¹ æ•ˆç‡30%ã€‚",
        "æœºå™¨å­¦ä¹ ç®—æ³•å¯ä»¥åˆ†æå­¦ç”Ÿçš„å­¦ä¹ æ¨¡å¼ï¼Œæä¾›å®šåˆ¶åŒ–çš„å­¦ä¹ è·¯å¾„ã€‚adaptive learningç³»ç»Ÿå·²åœ¨å¤šæ‰€å­¦æ ¡è¯•ç‚¹ã€‚",
        "è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯ä½¿å¾—æ™ºèƒ½èŠå¤©æœºå™¨äººèƒ½å¤Ÿå›ç­”å­¦ç”Ÿçš„é—®é¢˜ã€‚ChatGPTç­‰å¤§æ¨¡å‹åœ¨æ•™è‚²é¢†åŸŸåº”ç”¨å¹¿æ³›ã€‚",
        "è®¡ç®—æœºè§†è§‰å¯ä»¥ç”¨äºè‡ªåŠ¨æ‰¹æ”¹ä½œä¸šå’Œæ£€æµ‹å­¦æœ¯ä¸ç«¯è¡Œä¸ºã€‚å‡†ç¡®ç‡å·²è¾¾åˆ°95%ä»¥ä¸Šã€‚",
        "æ·±åº¦å­¦ä¹ åœ¨è¯­éŸ³è¯†åˆ«å’Œè¯­è¨€ç¿»è¯‘æ–¹é¢æœ‰é‡è¦åº”ç”¨ã€‚å®æ—¶ç¿»è¯‘æŠ€æœ¯æ”¯æŒå¤šè¯­è¨€æ•™å­¦ã€‚",
        "åŒºå—é“¾æŠ€æœ¯å¯ç”¨äºå­¦å†è®¤è¯å’Œæˆç»©ç®¡ç†ï¼Œç¡®ä¿æ•™è‚²æ•°æ®çš„çœŸå®æ€§å’Œä¸å¯ç¯¡æ”¹æ€§ã€‚",
        "è™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®ä¸ºæ•™è‚²å¸¦æ¥æ²‰æµ¸å¼å­¦ä¹ ä½“éªŒï¼Œç‰¹åˆ«é€‚ç”¨äºå†å²ã€åœ°ç†ç­‰å­¦ç§‘ã€‚",
        "å¤§æ•°æ®åˆ†æå¸®åŠ©æ•™è‚²æœºæ„äº†è§£å­¦ä¹ è¶‹åŠ¿ï¼Œä¼˜åŒ–è¯¾ç¨‹è®¾è®¡å’Œæ•™å­¦æ–¹æ³•ã€‚"
    ]
    
    # åˆ›å»ºå‘é‡å­˜å‚¨
    vectorstore = InMemoryVectorStore.from_texts(sample_docs, embeddings)
    retriever = vectorstore.as_retriever()
    
    # åˆ›å»ºæ£€ç´¢å·¥å…·
    retriever_tool = create_retriever_tool(
        retriever,
        "knowledge_retriever",
        "æœç´¢å’Œæ£€ç´¢ç›¸å…³çš„çŸ¥è¯†ä¿¡æ¯"
    )
    
    # åˆ›å»ºç ”ç©¶å·¥å…·
    research_tools = create_research_tools()
    
    return llm, checkpointer, memory_store, namespace, retriever_tool, embeddings, research_tools

# ================================
# 4. Write Context - å†™å…¥ä¸Šä¸‹æ–‡
# ================================

def write_initial_analysis(state: ComprehensiveState, store: BaseStore) -> Dict:
    """å†™å…¥åˆå§‹åˆ†æåˆ°çŠ¶æ€å’Œé•¿æœŸè®°å¿†"""
    display_section_header(
        "ğŸ“ WRITE CONTEXT - å†™å…¥ä¸Šä¸‹æ–‡", 
        "å°†åˆå§‹åˆ†æå†™å…¥çŠ¶æ€ï¼ˆè‰ç¨¿æœ¬ï¼‰å’Œé•¿æœŸè®°å¿†"
    )
    
    llm, _, _, namespace, _, _, _ = initialize_components()
    
    console.print("ğŸ§  æ­£åœ¨ç”Ÿæˆåˆå§‹åˆ†æ...")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å†å²ä¸Šä¸‹æ–‡
    round_num = state.get("conversation_round", 1)
    current_query = state.get("current_query", state['research_topic'])
    
    prompt = f"""ä½œä¸ºä¸€ä¸ªç ”ç©¶åŠ©æ‰‹ï¼Œè¯·å¯¹ä¸»é¢˜ "{state['research_topic']}" è¿›è¡Œåˆ†æã€‚
    
å½“å‰æŸ¥è¯¢ï¼š{current_query}
å¯¹è¯è½®æ¬¡ï¼šç¬¬ {round_num} è½®

è¯·æä¾›ï¼š
1. ä¸»é¢˜çš„å…³é”®æ¦‚å¿µ
2. å¯èƒ½çš„ç ”ç©¶æ–¹å‘  
3. éœ€è¦å…³æ³¨çš„é‡ç‚¹é¢†åŸŸ
4. é’ˆå¯¹å½“å‰æŸ¥è¯¢çš„å…·ä½“å»ºè®®"""
    
    response = llm.invoke(prompt)
    analysis = response.content
    
    # è®°å½•å†™å…¥æ“ä½œåˆ°æ—¥å¿—
    log_context_operation(
        "WRITE", 
        "Long-term Memory", 
        f"analysis_{state['research_topic'].replace(' ', '_')}_round_{round_num}",
        analysis
    )
    
    # å†™å…¥é•¿æœŸè®°å¿†
    memory_key = f"analysis_{state['research_topic'].replace(' ', '_')}_round_{round_num}"
    store.put(
        namespace,
        memory_key,
        {
            "topic": state['research_topic'],
            "analysis": analysis,
            "timestamp": time.time(),
            "round": round_num,
            "query": current_query,
            "type": "initial_analysis"
        }
    )
    
    # è®°å½•å†™å…¥æ“ä½œåˆ°çŠ¶æ€
    context_op = {
        "operation": "WRITE",
        "location": "State + Long-term Memory", 
        "key": "initial_analysis",
        "content_length": len(analysis),
        "timestamp": time.time(),
        "round": round_num
    }
    
    log_context_operation("WRITE", "State", "initial_analysis", analysis)
    
    console.print(Panel(analysis[:200] + "...", title="âœ… åˆå§‹åˆ†æå·²å†™å…¥", border_style="green"))
    
    return {
        "initial_analysis": analysis,
        "conversation_round": round_num,
        "processing_steps": state.get("processing_steps", []) + [f"ç¬¬{round_num}è½®ï¼šå†™å…¥åˆå§‹åˆ†æ"],
        "context_operations_log": state.get("context_operations_log", []) + [context_op],
        "messages": state.get("messages", []) + [AIMessage(content=f"ç¬¬{round_num}è½®åˆ†æï¼š{analysis[:100]}...")]
    }

def multi_tool_research(state: ComprehensiveState, store: BaseStore) -> Dict:
    """å¤šè½®å·¥å…·è°ƒç”¨è¿›è¡Œç ”ç©¶"""
    display_section_header(
        "ğŸ”§ MULTI-TOOL RESEARCH - å¤šå·¥å…·ç ”ç©¶",
        "ä½¿ç”¨å¤šä¸ªå·¥å…·è¿›è¡Œæ·±å…¥ç ”ç©¶å¹¶è®°å½•æ‰€æœ‰æ“ä½œ"
    )
    
    llm, _, _, namespace, retriever_tool, _, research_tools = initialize_components()
    round_num = state.get("conversation_round", 1)
    
    console.print(f"ğŸ” ç¬¬{round_num}è½®ï¼šå¼€å§‹å¤šå·¥å…·ç ”ç©¶...")
    
    tool_results = []
    
    # å·¥å…·è°ƒç”¨åºåˆ—
    tools_to_use = [
        ("knowledge_retriever", state['research_topic']),
        ("web_search", state['research_topic']),
        ("data_analysis", "æ•™è‚²æ•°æ®"),
        ("literature_search", state['research_topic'])
    ]
    
    for i, (tool_name, query) in enumerate(tools_to_use, 1):
        console.print(f"ğŸ› ï¸  è°ƒç”¨å·¥å…· {i}/{len(tools_to_use)}: {tool_name}")
        
        try:
            if tool_name == "knowledge_retriever":
                result = retriever_tool.invoke({"query": query})
            else:
                result = research_tools[tool_name](query)
            
            tool_result = {
                "tool": tool_name,
                "query": query,
                "result": result,
                "timestamp": time.time(),
                "round": round_num,
                "call_order": i
            }
            
            tool_results.append(tool_result)
            
            # è®°å½•å·¥å…·è°ƒç”¨åˆ°æ—¥å¿—å’Œé•¿æœŸè®°å¿†
            log_context_operation(
                "WRITE",
                "Tool Results",
                f"{tool_name}_round_{round_num}_call_{i}",
                str(result)
            )
            
            # å°†å·¥å…·ç»“æœå†™å…¥é•¿æœŸè®°å¿†
            store.put(
                namespace,
                f"tool_result_{tool_name}_round_{round_num}_call_{i}",
                tool_result
            )
            
            console.print(f"âœ… {tool_name} è°ƒç”¨å®Œæˆï¼Œç»“æœå·²ä¿å­˜")
            time.sleep(0.5)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
            
        except Exception as e:
            console.print(f"âŒ {tool_name} è°ƒç”¨å¤±è´¥: {str(e)}")
            tool_results.append({
                "tool": tool_name,
                "query": query,
                "result": f"Error: {str(e)}",
                "timestamp": time.time(),
                "round": round_num,
                "call_order": i,
                "status": "failed"
            })
    
    # è®°å½•æ‰€æœ‰å·¥å…·è°ƒç”¨æ“ä½œ
    context_op = {
        "operation": "WRITE",
        "location": "Tool Results + Long-term Memory",
        "key": f"multi_tool_results_round_{round_num}",
        "content_length": sum(len(str(r['result'])) for r in tool_results),
        "timestamp": time.time(),
        "round": round_num,
        "tools_used": len(tool_results)
    }
    
    console.print(Panel(
        f"å®Œæˆ {len(tool_results)} ä¸ªå·¥å…·è°ƒç”¨ï¼Œæ‰€æœ‰ç»“æœå·²å†™å…¥ä¸Šä¸‹æ–‡",
        title="âœ… å¤šå·¥å…·ç ”ç©¶å®Œæˆ",
        border_style="green"
    ))
    
    return {
        "tool_results": state.get("tool_results", []) + tool_results,
        "processing_steps": state["processing_steps"] + [f"ç¬¬{round_num}è½®ï¼šå¤šå·¥å…·ç ”ç©¶({len(tool_results)}ä¸ªå·¥å…·)"],
        "context_operations_log": state.get("context_operations_log", []) + [context_op]
    }

# ================================
# 5. Select Context - é€‰æ‹©ä¸Šä¸‹æ–‡
# ================================

def comprehensive_context_selection(state: ComprehensiveState, store: BaseStore) -> Dict:
    """ç»¼åˆä¸Šä¸‹æ–‡é€‰æ‹© - ä»å¤šä¸ªæ¥æºé€‰æ‹©ç›¸å…³ä¿¡æ¯"""
    display_section_header(
        "ğŸ¯ SELECT CONTEXT - é€‰æ‹©ä¸Šä¸‹æ–‡",
        "ä»çŠ¶æ€ã€è®°å¿†ã€å·¥å…·ç»“æœä¸­æ™ºèƒ½é€‰æ‹©ç›¸å…³ä¿¡æ¯"
    )
    
    _, _, _, namespace, _, _, _ = initialize_components()
    round_num = state.get("conversation_round", 1)
    
    console.print(f"ğŸ” ç¬¬{round_num}è½®ï¼šå¼€å§‹ç»¼åˆä¸Šä¸‹æ–‡é€‰æ‹©...")
    
    # 1. ä»é•¿æœŸè®°å¿†ä¸­é€‰æ‹©
    console.print("ğŸ“š ä»é•¿æœŸè®°å¿†ä¸­é€‰æ‹©ç›¸å…³ä¿¡æ¯...")
    stored_items = list(store.search(namespace))
    selected_memories = []
    memory_selection_details = []
    
    for item in stored_items:
        # é€‰æ‹©ä¸å½“å‰ä¸»é¢˜å’Œè½®æ¬¡ç›¸å…³çš„è®°å¿†
        if (state['research_topic'].replace(' ', '_') in item.key or 
            f"round_{round_num}" in item.key or
            any(keyword in item.key for keyword in ["analysis", "tool_result", "plan"])):
            
            memory_item = {
                "key": item.key,
                "type": item.value.get("type", "unknown"),
                "round": item.value.get("round", "unknown"),
                "timestamp": item.value.get("timestamp", 0),
                "content_preview": str(item.value)[:150] + "..."
            }
            selected_memories.append(memory_item)
            memory_selection_details.append(f"Selected: {item.key} (type: {memory_item['type']})")
    
    log_context_selection("Long-term Memory", len(selected_memories), 
                         f"Topics: {[m['type'] for m in selected_memories]}")
    
    # 2. ä»çŠ¶æ€ä¸­é€‰æ‹©å½“å‰è½®æ¬¡çš„ä¿¡æ¯
    console.print("ğŸ“ ä»å½“å‰çŠ¶æ€ä¸­é€‰æ‹©ä¿¡æ¯...")
    state_selections = []
    
    # é€‰æ‹©å½“å‰åˆ†æ
    if state.get("initial_analysis"):
        state_selections.append({
            "source": "current_state",
            "key": "initial_analysis", 
            "content_length": len(state["initial_analysis"]),
            "preview": state["initial_analysis"][:100] + "..."
        })
        log_context_selection("Current State", 1, "initial_analysis")
    
    # 3. ä»å·¥å…·ç»“æœä¸­é€‰æ‹©
    console.print("ğŸ› ï¸  ä»å·¥å…·ç»“æœä¸­é€‰æ‹©ç›¸å…³ä¿¡æ¯...")
    tool_selections = []
    recent_tool_results = state.get("tool_results", [])[-5:]  # é€‰æ‹©æœ€è¿‘5ä¸ªå·¥å…·ç»“æœ
    
    for tool_result in recent_tool_results:
        if tool_result.get("round") == round_num:
            tool_selections.append({
                "tool": tool_result["tool"],
                "query": tool_result["query"],
                "result_preview": str(tool_result["result"])[:100] + "...",
                "timestamp": tool_result["timestamp"]
            })
    
    log_context_selection("Tool Results", len(tool_selections),
                         f"Tools: {[t['tool'] for t in tool_selections]}")
    
    # 4. æ™ºèƒ½ç›¸å…³æ€§è¯„åˆ†å’Œè¿‡æ»¤
    console.print("ğŸ§  åŸºäºç›¸å…³æ€§è¿›è¡Œæ™ºèƒ½è¿‡æ»¤...")
    
    # æ¨¡æ‹Ÿæ™ºèƒ½é€‰æ‹©é€»è¾‘
    current_query = state.get("current_query", state['research_topic'])
    relevant_keywords = current_query.lower().split()
    
    filtered_memories = []
    for memory in selected_memories:
        relevance_score = sum(1 for keyword in relevant_keywords 
                            if keyword in memory["key"].lower() or 
                               keyword in memory.get("content_preview", "").lower())
        if relevance_score > 0:
            memory["relevance_score"] = relevance_score
            filtered_memories.append(memory)
    
    # æŒ‰ç›¸å…³æ€§æ’åº
    filtered_memories.sort(key=lambda x: x.get("relevance_score", 0), reverse=True)
    
    # è®°å½•é€‰æ‹©æ“ä½œ
    context_selections = {
        "memories": filtered_memories[:10],  # æœ€å¤šé€‰æ‹©10ä¸ªæœ€ç›¸å…³çš„è®°å¿†
        "state_info": state_selections,
        "tool_results": tool_selections,
        "selection_criteria": {
            "round": round_num,
            "query": current_query,
            "keywords": relevant_keywords
        },
        "total_selected": len(filtered_memories) + len(state_selections) + len(tool_selections)
    }
    
    context_op = {
        "operation": "SELECT",
        "sources": ["Long-term Memory", "Current State", "Tool Results"],
        "total_items": len(stored_items),
        "selected_items": context_selections["total_selected"],
        "timestamp": time.time(),
        "round": round_num,
        "selection_details": memory_selection_details
    }
    
    log_context_selection("Comprehensive Selection", 
                         context_selections["total_selected"],
                         f"From {len(stored_items)} available items")
    
    console.print(Panel(
        f"""é€‰æ‹©å®Œæˆç»Ÿè®¡ï¼š
ğŸ“š é•¿æœŸè®°å¿†: {len(filtered_memories)} é¡¹
ğŸ“ å½“å‰çŠ¶æ€: {len(state_selections)} é¡¹  
ğŸ› ï¸  å·¥å…·ç»“æœ: {len(tool_selections)} é¡¹
ğŸ¯ æ€»è®¡é€‰æ‹©: {context_selections['total_selected']} é¡¹""",
        title="âœ… ç»¼åˆä¸Šä¸‹æ–‡é€‰æ‹©å®Œæˆ",
        border_style="green"
    ))
    
    return {
        "selected_memories": filtered_memories,
        "context_selections": [context_selections],
        "processing_steps": state["processing_steps"] + [f"ç¬¬{round_num}è½®ï¼šç»¼åˆä¸Šä¸‹æ–‡é€‰æ‹©({context_selections['total_selected']}é¡¹)"],
        "context_operations_log": state.get("context_operations_log", []) + [context_op]
    }

def select_tools_and_knowledge(state: ComprehensiveState) -> Dict:
    """é€‰æ‹©ç›¸å…³å·¥å…·å¹¶æ£€ç´¢çŸ¥è¯†"""
    _, _, _, _, retriever_tool, _, _ = initialize_components()
    
    console.print("ğŸ”§ æ­£åœ¨é€‰æ‹©ç›¸å…³å·¥å…·å¹¶æ£€ç´¢çŸ¥è¯†...")
    
    # æ¨¡æ‹Ÿå·¥å…·é€‰æ‹©ï¼ˆåŸºäºä¸»é¢˜ï¼‰
    available_tools = [
        "æ•°æ®åˆ†æå·¥å…·", "æ–‡çŒ®æ£€ç´¢å·¥å…·", "ç»Ÿè®¡åˆ†æå·¥å…·", 
        "å¯è§†åŒ–å·¥å…·", "æœºå™¨å­¦ä¹ å·¥å…·", "è‡ªç„¶è¯­è¨€å¤„ç†å·¥å…·"
    ]
    
    # ç®€å•çš„å·¥å…·é€‰æ‹©é€»è¾‘
    relevant_tools = []
    topic_lower = state['research_topic'].lower()
    if "äººå·¥æ™ºèƒ½" in topic_lower or "ai" in topic_lower:
        relevant_tools.extend(["æœºå™¨å­¦ä¹ å·¥å…·", "è‡ªç„¶è¯­è¨€å¤„ç†å·¥å…·"])
    if "æ•™è‚²" in topic_lower:
        relevant_tools.extend(["æ•°æ®åˆ†æå·¥å…·", "ç»Ÿè®¡åˆ†æå·¥å…·"])
    if "åˆ†æ" in topic_lower:
        relevant_tools.extend(["æ•°æ®åˆ†æå·¥å…·", "å¯è§†åŒ–å·¥å…·"])
    
    # ä½¿ç”¨æ£€ç´¢å·¥å…·è·å–çŸ¥è¯†
    try:
        retrieved_docs = retriever_tool.invoke({"query": state['research_topic']})
        retrieved_knowledge = retrieved_docs[:300] + "..."
    except Exception as e:
        retrieved_knowledge = f"æ£€ç´¢çŸ¥è¯†æ—¶å‡ºé”™: {str(e)}"
    
    console.print(Panel(
        f"é€‰æ‹©çš„å·¥å…·: {', '.join(relevant_tools)}\næ£€ç´¢çš„çŸ¥è¯†: {retrieved_knowledge[:100]}...",
        title="âœ… å·¥å…·å’ŒçŸ¥è¯†é€‰æ‹©å®Œæˆ",
        border_style="green"
    ))
    
    return {
        "relevant_tools": relevant_tools,
        "retrieved_knowledge": retrieved_knowledge,
        "processing_steps": state["processing_steps"] + ["é€‰æ‹©å·¥å…·å’Œæ£€ç´¢çŸ¥è¯†"]
    }

# ================================
# 6. Compress Context - å‹ç¼©ä¸Šä¸‹æ–‡
# ================================

def compress_conversation(state: ComprehensiveState) -> Dict:
    """å‹ç¼©å¯¹è¯å†å²"""
    display_section_header(
        "ğŸ—œï¸ COMPRESS CONTEXT - å‹ç¼©ä¸Šä¸‹æ–‡",
        "å‹ç¼©é•¿å¯¹è¯å†å²å’Œå·¥å…·è¾“å‡ºä»¥èŠ‚çœtoken"
    )
    
    llm, _, _, _, _, _, _ = initialize_components()
    
    console.print("ğŸ“„ æ­£åœ¨å‹ç¼©å¯¹è¯å†å²...")
    
    # æ¨¡æ‹Ÿé•¿å¯¹è¯å†å²
    long_conversation = f"""
ç”¨æˆ·æŸ¥è¯¢: {state['user_query']}
åˆå§‹åˆ†æ: {state['initial_analysis']}
ç ”ç©¶è®¡åˆ’: {state['research_plan']}
é€‰æ‹©çš„è®°å¿†: {json.dumps(state.get('selected_memories', []), ensure_ascii=False)}
ç›¸å…³å·¥å…·: {', '.join(state.get('relevant_tools', []))}
æ£€ç´¢çš„çŸ¥è¯†: {state.get('retrieved_knowledge', '')}
"""
    
    prompt = f"""è¯·å°†ä»¥ä¸‹é•¿å¯¹è¯å†å²å‹ç¼©æˆç®€æ´çš„æ‘˜è¦ï¼Œä¿ç•™å…³é”®ä¿¡æ¯ï¼š

{long_conversation}

è¯·æä¾›ä¸€ä¸ªç®€æ´ä½†å®Œæ•´çš„æ‘˜è¦ï¼ŒåŒ…å«æ‰€æœ‰é‡è¦çš„å†³ç­–å’Œå‘ç°ã€‚"""
    
    response = llm.invoke(prompt)
    summary = response.content
    
    console.print(Panel(
        f"åŸå§‹é•¿åº¦: {len(long_conversation)} å­—ç¬¦\nå‹ç¼©å: {len(summary)} å­—ç¬¦\nå‹ç¼©ç‡: {(1 - len(summary)/len(long_conversation))*100:.1f}%",
        title="âœ… å¯¹è¯å†å²å‹ç¼©å®Œæˆ",
        border_style="green"
    ))
    
    return {
        "conversation_summary": summary,
        "processing_steps": state["processing_steps"] + ["å‹ç¼©å¯¹è¯å†å²"]
    }

def compress_findings(state: ComprehensiveState) -> Dict:
    """å‹ç¼©ç ”ç©¶å‘ç°"""
    llm, _, _, _, _, _, _ = initialize_components()
    
    console.print("ğŸ“Š æ­£åœ¨å‹ç¼©ç ”ç©¶å‘ç°...")
    
    # æ¨¡æ‹Ÿè¯¦ç»†çš„ç ”ç©¶å‘ç°
    detailed_findings = f"""
è¯¦ç»†ç ”ç©¶å‘ç°ï¼š

åŸºäºåˆå§‹åˆ†æ: {state['initial_analysis']}

æ ¹æ®ç ”ç©¶è®¡åˆ’: {state['research_plan']}

åˆ©ç”¨é€‰æ‹©çš„å·¥å…·: {', '.join(state.get('relevant_tools', []))}

å‚è€ƒæ£€ç´¢çš„çŸ¥è¯†: {state.get('retrieved_knowledge', '')}

ç»“åˆå†å²è®°å¿†: {len(state.get('selected_memories', []))} é¡¹ç›¸å…³è®°å½•

ç»¼åˆåˆ†æè¡¨æ˜è¯¥ç ”ç©¶ä¸»é¢˜å…·æœ‰é‡è¦æ„ä¹‰å’Œå¹¿é˜”çš„åº”ç”¨å‰æ™¯ã€‚
"""
    
    prompt = f"""è¯·å°†ä»¥ä¸‹è¯¦ç»†çš„ç ”ç©¶å‘ç°å‹ç¼©æˆæ ¸å¿ƒè¦ç‚¹ï¼š

{detailed_findings}

è¯·æå–æœ€é‡è¦çš„3-5ä¸ªå…³é”®å‘ç°ï¼Œæ¯ä¸ªç”¨ä¸€å¥è¯æ¦‚æ‹¬ã€‚"""
    
    response = llm.invoke(prompt)
    compressed = response.content
    
    console.print(Panel(
        compressed,
        title="âœ… ç ”ç©¶å‘ç°å‹ç¼©å®Œæˆ",
        border_style="green"
    ))
    
    return {
        "compressed_findings": compressed,
        "processing_steps": state["processing_steps"] + ["å‹ç¼©ç ”ç©¶å‘ç°"]
    }

# ================================
# 7. Isolate Context - éš”ç¦»ä¸Šä¸‹æ–‡
# ================================

def isolate_with_specialists(state: ComprehensiveState) -> Dict:
    """ä½¿ç”¨ä¸“é—¨çš„æ™ºèƒ½ä½“éš”ç¦»ä¸åŒç±»å‹çš„åˆ†æ"""
    display_section_header(
        "ğŸ—ï¸ ISOLATE CONTEXT - éš”ç¦»ä¸Šä¸‹æ–‡",
        "ä½¿ç”¨å¤šä¸ªä¸“é—¨çš„æ™ºèƒ½ä½“éš”ç¦»ä¸åŒç±»å‹çš„å¤„ç†"
    )
    
    llm, _, _, _, _, _, _ = initialize_components()
    
    console.print("ğŸ‘¥ æ­£åœ¨åˆ›å»ºä¸“é—¨çš„æ™ºèƒ½ä½“è¿›è¡Œéš”ç¦»åˆ†æ...")
    
    # å®šä¹‰ä¸åŒçš„ä¸“å®¶è§’è‰²
    specialists = {
        "æŠ€æœ¯ä¸“å®¶": "åˆ†ææŠ€æœ¯å®ç°å’ŒæŒ‘æˆ˜",
        "æ•™è‚²ä¸“å®¶": "è¯„ä¼°æ•™è‚²å½±å“å’Œåº”ç”¨",
        "å•†ä¸šä¸“å®¶": "åˆ†æå¸‚åœºå‰æ™¯å’Œå•†ä¸šä»·å€¼"
    }
    
    specialist_reports = {}
    
    for specialist, role in track(specialists.items(), description="ä¸“å®¶åˆ†æä¸­..."):
        prompt = f"""ä½ æ˜¯ä¸€ä¸ª{specialist}ï¼Œä¸“é—¨è´Ÿè´£{role}ã€‚

ç ”ç©¶ä¸»é¢˜ï¼š{state['research_topic']}
å‹ç¼©çš„å‘ç°ï¼š{state.get('compressed_findings', '')}

è¯·ä»ä½ çš„ä¸“ä¸šè§’åº¦æä¾›åˆ†ææŠ¥å‘Šï¼Œé‡ç‚¹å…³æ³¨ä½ çš„ä¸“ä¸šé¢†åŸŸã€‚"""
        
        response = llm.invoke(prompt)
        specialist_reports[specialist] = response.content
        
        console.print(f"âœ… {specialist}æŠ¥å‘Šå®Œæˆ")
        time.sleep(0.5)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
    
    console.print(Panel(
        f"å®Œæˆäº† {len(specialist_reports)} ä¸ªä¸“å®¶çš„éš”ç¦»åˆ†æ",
        title="âœ… ä¸“å®¶éš”ç¦»åˆ†æå®Œæˆ",
        border_style="green"
    ))
    
    return {
        "specialist_reports": specialist_reports,
        "processing_steps": state["processing_steps"] + ["å¤šæ™ºèƒ½ä½“éš”ç¦»åˆ†æ"]
    }

def sandbox_simulation(state: ComprehensiveState) -> Dict:
    """æ¨¡æ‹Ÿæ²™ç›’ç¯å¢ƒéš”ç¦»è®¡ç®—"""
    console.print("ğŸ–ï¸ æ­£åœ¨æ¨¡æ‹Ÿæ²™ç›’ç¯å¢ƒè¿›è¡Œéš”ç¦»è®¡ç®—...")
    
    # æ¨¡æ‹Ÿæ²™ç›’ä¸­çš„è®¡ç®—
    sandbox_results = {
        "data_processing": "åœ¨æ²™ç›’ä¸­å®‰å…¨å¤„ç†äº†æ•æ„Ÿæ•°æ®",
        "model_training": "åœ¨éš”ç¦»ç¯å¢ƒä¸­è®­ç»ƒäº†æœºå™¨å­¦ä¹ æ¨¡å‹",
        "security_check": "é€šè¿‡äº†å®‰å…¨æ£€æŸ¥ï¼Œæ²¡æœ‰å‘ç°æ¶æ„ä»£ç "
    }
    
    console.print(Panel(
        "\n".join([f"â€¢ {k}: {v}" for k, v in sandbox_results.items()]),
        title="âœ… æ²™ç›’éš”ç¦»è®¡ç®—å®Œæˆ",
        border_style="green"
    ))
    
    return {
        "processing_steps": state["processing_steps"] + ["æ²™ç›’éš”ç¦»è®¡ç®—"]
    }

# ================================
# 8. ä¸»å·¥ä½œæµæ„å»º
# ================================

def conversation_router(state: ComprehensiveState) -> Literal["continue_conversation", "end_conversation"]:
    """å†³å®šæ˜¯å¦ç»§ç»­å¯¹è¯"""
    round_num = state.get("conversation_round", 1)
    max_rounds = 3  # æœ€å¤š3è½®å¯¹è¯
    
    if round_num < max_rounds:
        return "continue_conversation"
    else:
        return "end_conversation"

def prepare_next_round(state: ComprehensiveState) -> Dict:
    """å‡†å¤‡ä¸‹ä¸€è½®å¯¹è¯"""
    current_round = state.get("conversation_round", 1)
    next_round = current_round + 1
    
    # æ¨¡æ‹Ÿæ–°çš„ç”¨æˆ·æŸ¥è¯¢
    next_queries = [
        "è¯·è¯¦ç»†åˆ†ææŠ€æœ¯å®ç°çš„æŒ‘æˆ˜",
        "åˆ†æå¸‚åœºå‰æ™¯å’Œå•†ä¸šä»·å€¼", 
        "æ€»ç»“å…³é”®å‘ç°å’Œå»ºè®®"
    ]
    
    if next_round <= len(next_queries):
        next_query = next_queries[next_round - 1]
    else:
        next_query = "è¯·æä¾›æœ€ç»ˆæ€»ç»“"
    
    console.print(Panel(
        f"ğŸ”„ å‡†å¤‡ç¬¬ {next_round} è½®å¯¹è¯\næ–°æŸ¥è¯¢: {next_query}",
        title="å¤šè½®å¯¹è¯",
        border_style="cyan"
    ))
    
    return {
        "conversation_round": next_round,
        "current_query": next_query,
        "processing_steps": state["processing_steps"] + [f"å¼€å§‹ç¬¬{next_round}è½®å¯¹è¯"]
    }

def build_comprehensive_workflow():
    """æ„å»ºå®Œæ•´çš„ä¸Šä¸‹æ–‡å·¥ç¨‹å·¥ä½œæµ"""
    llm, checkpointer, memory_store, namespace, retriever_tool, embeddings, research_tools = initialize_components()
    
    # åˆ›å»ºçŠ¶æ€å›¾
    workflow = StateGraph(ComprehensiveState)
    
    # æ·»åŠ æ‰€æœ‰èŠ‚ç‚¹
    workflow.add_node("write_analysis", write_initial_analysis)
    workflow.add_node("multi_tool_research", multi_tool_research)
    workflow.add_node("select_context", comprehensive_context_selection)
    workflow.add_node("compress_conv", compress_conversation)
    workflow.add_node("compress_findings", compress_findings)
    workflow.add_node("isolate_specialists", isolate_with_specialists)
    workflow.add_node("prepare_next_round", prepare_next_round)
    workflow.add_node("final_summary", sandbox_simulation)
    
    # å®šä¹‰å·¥ä½œæµç¨‹ - æ”¯æŒå¤šè½®å¯¹è¯
    workflow.add_edge(START, "write_analysis")
    workflow.add_edge("write_analysis", "multi_tool_research")
    workflow.add_edge("multi_tool_research", "select_context")
    workflow.add_edge("select_context", "compress_conv")
    workflow.add_edge("compress_conv", "compress_findings")
    workflow.add_edge("compress_findings", "isolate_specialists")
    
    # æ¡ä»¶è·¯ç”±ï¼šå†³å®šæ˜¯å¦ç»§ç»­å¯¹è¯
    workflow.add_conditional_edges(
        "isolate_specialists",
        conversation_router,
        {
            "continue_conversation": "prepare_next_round",
            "end_conversation": "final_summary"
        }
    )
    
    # ç»§ç»­å¯¹è¯çš„å¾ªç¯
    workflow.add_edge("prepare_next_round", "write_analysis")
    workflow.add_edge("final_summary", END)
    
    # ç¼–è¯‘å·¥ä½œæµ
    app = workflow.compile(checkpointer=checkpointer, store=memory_store)
    
    return app, memory_store, namespace

# ================================
# 9. ç»“æœå±•ç¤ºå’Œåˆ†æ
# ================================

def display_comprehensive_results(result: Dict, memory_store: InMemoryStore, namespace):
    """å±•ç¤ºå®Œæ•´çš„æ¼”ç¤ºç»“æœ"""
    console.print(Panel(
        "ğŸ‰ å¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡å·¥ç¨‹æ¼”ç¤ºç»“æœ",
        title="æ¼”ç¤ºå®Œæˆ",
        border_style="bold green",
        padding=(1, 2)
    ))
    
    # åˆ›å»ºç»“æœè¡¨æ ¼
    table = Table(title="ä¸Šä¸‹æ–‡å·¥ç¨‹æ¼”ç¤ºæ€»ç»“")
    table.add_column("åŠŸèƒ½", style="cyan", no_wrap=True)
    table.add_column("å®ç°", style="magenta")
    table.add_column("ç»“æœ", style="green")
    
    table.add_row(
        "Write Context",
        "å¤šè½®å†™å…¥çŠ¶æ€å’Œé•¿æœŸè®°å¿†",
        f"âœ… åˆ†æ: {len(result.get('initial_analysis', ''))} å­—ç¬¦\nâœ… å·¥å…·ç»“æœ: {len(result.get('tool_results', []))} ä¸ª\nâœ… è½®æ¬¡: {result.get('conversation_round', 1)} è½®"
    )
    
    table.add_row(
        "Select Context", 
        "æ™ºèƒ½é€‰æ‹©å¤šæºä¸Šä¸‹æ–‡",
        f"âœ… è®°å¿†: {len(result.get('selected_memories', []))} é¡¹\nâœ… ä¸Šä¸‹æ–‡é€‰æ‹©: {len(result.get('context_selections', []))} æ¬¡\nâœ… é€‰æ‹©æ“ä½œ: {len(result.get('context_operations_log', []))} ä¸ª"
    )
    
    table.add_row(
        "Compress Context",
        "å‹ç¼©å¯¹è¯å’Œå‘ç°", 
        f"âœ… å¯¹è¯æ‘˜è¦: {len(result.get('conversation_summary', ''))} å­—ç¬¦\nâœ… å‹ç¼©å‘ç°: {len(result.get('compressed_findings', ''))} å­—ç¬¦"
    )
    
    table.add_row(
        "Isolate Context",
        "å¤šæ™ºèƒ½ä½“å’Œæ²™ç›’éš”ç¦»",
        f"âœ… ä¸“å®¶æŠ¥å‘Š: {len(result.get('specialist_reports', {}))} ä¸ª\nâœ… æ²™ç›’éš”ç¦»: å®Œæˆ"
    )
    
    console.print(table)
    
    # æ˜¾ç¤ºå¤„ç†æ­¥éª¤
    console.print(Panel(
        "\n".join([f"{i+1}. {step}" for i, step in enumerate(result.get('processing_steps', []))]),
        title="ğŸ”„ å¤„ç†æ­¥éª¤",
        border_style="blue"
    ))
    
    # æ˜¾ç¤ºä¸Šä¸‹æ–‡æ“ä½œæ—¥å¿—
    if result.get('context_operations_log'):
        console.print(Panel(
            "\n".join([
                f"[{op.get('round', '?')}] {op.get('operation', 'UNKNOWN')} -> {op.get('location', 'Unknown')} "
                f"({op.get('selected_items', op.get('content_length', '?'))} items/chars)"
                for op in result.get('context_operations_log', [])
            ]),
            title="ğŸ“ ä¸Šä¸‹æ–‡æ“ä½œæ—¥å¿—",
            border_style="yellow"
        ))
    
    # æ˜¾ç¤ºé•¿æœŸè®°å¿†çŠ¶æ€
    stored_items = list(memory_store.search(namespace))
    console.print(Panel(
        f"é•¿æœŸè®°å¿†ä¸­ä¿å­˜äº† {len(stored_items)} é¡¹è®°å½•:\n" + 
        "\n".join([
            f"â€¢ {item.key}: {item.value.get('type', 'unknown')} "
            f"(Round {item.value.get('round', '?')})"
            for item in stored_items
        ]),
        title="ğŸ§  é•¿æœŸè®°å¿†çŠ¶æ€",
        border_style="purple"
    ))

# ================================
# 10. ä¸»æ¼”ç¤ºå‡½æ•°
# ================================

def run_complete_demo():
    """è¿è¡Œå®Œæ•´çš„å¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡å·¥ç¨‹æ¼”ç¤º"""
    console.print(Panel(
        "[bold blue]ğŸš€ LangGraph å¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡å·¥ç¨‹æ¼”ç¤º[/bold blue]\n\n"
        "è¿™ä¸ªæ¼”ç¤ºå°†å±•ç¤ºï¼š\n"
        "ğŸ”„ å¤šè½®å¯¹è¯ç®¡ç†\n"
        "ğŸ› ï¸  å¤šæ¬¡å·¥å…·è°ƒç”¨\n"
        "ğŸ“ Write Context - å†™å…¥ä¸Šä¸‹æ–‡ï¼ˆå¸¦æ—¥å¿—ï¼‰\n"
        "ğŸ¯ Select Context - é€‰æ‹©ä¸Šä¸‹æ–‡ï¼ˆå¸¦æ—¥å¿—ï¼‰\n" 
        "ğŸ—œï¸ Compress Context - å‹ç¼©ä¸Šä¸‹æ–‡\n"
        "ğŸ—ï¸ Isolate Context - éš”ç¦»ä¸Šä¸‹æ–‡\n\n"
        "[yellow]æ³¨æ„ï¼šæ‰€æœ‰ä¸Šä¸‹æ–‡æ“ä½œéƒ½ä¼šè®°å½•åˆ°æ—¥å¿—ä¸­[/yellow]",
        title="å¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡å·¥ç¨‹æ¼”ç¤º",
        border_style="bold blue",
        padding=(2, 4)
    ))
    
    # æ„å»ºå·¥ä½œæµ
    app, memory_store, namespace = build_comprehensive_workflow()
    
    # é…ç½®
    config = {"configurable": {"thread_id": "multi_round_demo_1"}}
    
    # åˆå§‹è¾“å…¥
    initial_state = {
        "research_topic": "äººå·¥æ™ºèƒ½åœ¨æ•™è‚²ä¸­çš„åº”ç”¨",
        "current_query": "è¯·å¸®æˆ‘æ·±å…¥åˆ†æäººå·¥æ™ºèƒ½åœ¨æ•™è‚²é¢†åŸŸçš„åº”ç”¨ç°çŠ¶",
        "conversation_round": 1,
        "processing_steps": [],
        "context_operations_log": [],
        "conversation_history": [],
        "messages": [HumanMessage(content="å¼€å§‹å¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡å·¥ç¨‹æ¼”ç¤º")],
        "tool_results": [],
        "token_usage": {"input": 0, "output": 0}
    }
    
    console.print(f"\nğŸ“š ç ”ç©¶ä¸»é¢˜: {initial_state['research_topic']}")
    console.print(f"â“ åˆå§‹æŸ¥è¯¢: {initial_state['current_query']}")
    console.print(f"ğŸ”„ å°†è¿›è¡Œå¤šè½®å¯¹è¯ï¼Œæ¯è½®éƒ½ä¼šè¿›è¡Œå®Œæ•´çš„ä¸Šä¸‹æ–‡å·¥ç¨‹æ“ä½œ\n")
    
    # è¿è¡Œå·¥ä½œæµ
    logger.info("å¼€å§‹æ‰§è¡Œå¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡å·¥ç¨‹æµç¨‹")
    console.print("ğŸ”„ å¼€å§‹æ‰§è¡Œå¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡å·¥ç¨‹æµç¨‹...\n")
    
    result = app.invoke(initial_state, config)
    
    # æ˜¾ç¤ºç»“æœ
    display_comprehensive_results(result, memory_store, namespace)
    
    # æ˜¾ç¤ºæœ€ç»ˆçš„æ—¥å¿—æ‘˜è¦
    console.print(Panel(
        f"""ğŸ“Š æ¼”ç¤ºç»Ÿè®¡ï¼š
ğŸ”„ å¯¹è¯è½®æ¬¡: {result.get('conversation_round', 1)} è½®
ğŸ› ï¸  å·¥å…·è°ƒç”¨: {len(result.get('tool_results', []))} æ¬¡
ğŸ“ ä¸Šä¸‹æ–‡æ“ä½œ: {len(result.get('context_operations_log', []))} ä¸ª
ğŸ§  é•¿æœŸè®°å¿†é¡¹: {len(list(memory_store.search(namespace)))} ä¸ª
ğŸ“‹ å¤„ç†æ­¥éª¤: {len(result.get('processing_steps', []))} æ­¥

âœ… æ‰€æœ‰ä¸Šä¸‹æ–‡å†™å…¥å’Œé€‰æ‹©æ“ä½œéƒ½å·²è®°å½•åˆ°æ—¥å¿—ä¸­ï¼""",
        title="ğŸ“ˆ æ¼”ç¤ºå®Œæˆç»Ÿè®¡",
        border_style="bold cyan"
    ))
    
    return result, memory_store

if __name__ == "__main__":
    try:
        result, store = run_complete_demo()
        console.print("\nâœ¨ æ¼”ç¤ºå®Œæˆï¼æ‰€æœ‰ä¸Šä¸‹æ–‡å·¥ç¨‹åŠŸèƒ½éƒ½å·²æˆåŠŸå±•ç¤ºã€‚")
    except KeyboardInterrupt:
        console.print("\nâŒ æ¼”ç¤ºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        console.print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        console.print("è¯·æ£€æŸ¥ç¯å¢ƒé…ç½®å’Œä¾èµ–åŒ…æ˜¯å¦æ­£ç¡®å®‰è£…ã€‚")
