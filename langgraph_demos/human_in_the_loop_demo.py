"""
LangGraph äººæœºäº¤äº’æ¼”ç¤º
åŸºäºå®˜æ–¹æ•™ç¨‹ï¼šhttps://langchain-ai.github.io/langgraph/tutorials/get-started/4-human-in-the-loop/

è¿™ä¸ªæ¼”ç¤ºå±•ç¤ºäº†å¦‚ä½•åœ¨ LangGraph ä¸­å®ç°äººæœºäº¤äº’åŠŸèƒ½ï¼Œå…è®¸ AI ä»£ç†åœ¨éœ€è¦æ—¶è¯·æ±‚äººå·¥ååŠ©ã€‚
"""

import os
from typing import Annotated, Dict, Any, List
from typing_extensions import TypedDict

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import HumanMessage, AIMessage
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, START, END
from dotenv import load_dotenv
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


class HumanInLoopState(TypedDict):
    """
    äººæœºäº¤äº’çŠ¶æ€å®šä¹‰
    
    åŒ…å«äº†æ•´ä¸ªäººæœºäº¤äº’è¿‡ç¨‹ä¸­éœ€è¦çš„æ‰€æœ‰çŠ¶æ€ï¼š
    - task: ç”¨æˆ·ä»»åŠ¡
    - ai_response: AIçš„å›å¤
    - human_input_needed: æ˜¯å¦éœ€è¦äººå·¥è¾“å…¥
    - human_feedback: äººå·¥åé¦ˆ
    - final_output: æœ€ç»ˆè¾“å‡º
    - steps: æ‰§è¡Œæ­¥éª¤çš„å†å²è®°å½•
    """
    task: str
    ai_response: str
    human_input_needed: bool
    human_feedback: str
    final_output: str
    steps: List[Dict]


class HumanInLoopDemo:
    """
    äººæœºäº¤äº’æ¼”ç¤ºç±»
    
    è¿™ä¸ªç±»å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ LangGraph æ„å»ºä¸€ä¸ªå…·æœ‰äººæœºäº¤äº’åŠŸèƒ½çš„ç³»ç»Ÿã€‚
    ç³»ç»Ÿå¯ä»¥åœ¨éœ€è¦æ—¶æš‚åœæ‰§è¡Œï¼Œç­‰å¾…äººå·¥è¾“å…¥ï¼Œç„¶åç»§ç»­å¤„ç†ã€‚
    """
    
    def __init__(self):
        """
        åˆå§‹åŒ–æ¼”ç¤ºå®ä¾‹
        
        è®¾ç½®ï¼š
        1. åˆå§‹åŒ–å¤§è¯­è¨€æ¨¡å‹
        2. åˆ›å»ºå„ç§æç¤ºæ¨¡æ¿
        3. åˆ›å»ºå·¥ä½œæµå›¾
        """
        logger.info("åˆå§‹åŒ– HumanInLoopDemo...")
        
        # åˆå§‹åŒ–å¤§è¯­è¨€æ¨¡å‹
        self.llm = ChatOpenAI(
            model="deepseek-chat",
            openai_api_key=os.getenv("LLM_API_KEY"),
            base_url=os.getenv("LLM_BASE_URL")
        )
        
        # åˆ›å»ºAIåˆ†ææç¤ºæ¨¡æ¿
        self.ai_prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œè´Ÿè´£åˆ†æç”¨æˆ·çš„è¯·æ±‚ã€‚
å½“ç”¨æˆ·è¯·æ±‚ä¸“å®¶å»ºè®®æˆ–éœ€è¦äººå·¥ååŠ©æ—¶ï¼Œä½ åº”è¯¥è¯†åˆ«å‡ºè¿™ç§éœ€æ±‚ã€‚
å¦‚æœç”¨æˆ·æ˜ç¡®è¦æ±‚ä¸“å®¶å»ºè®®æˆ–äººå·¥ååŠ©ï¼Œè¯·åœ¨å›å¤ä¸­åŒ…å«"éœ€è¦äººå·¥ååŠ©"è¿™ä¸ªçŸ­è¯­ã€‚
å¦åˆ™ï¼Œç›´æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"""),
            ("user", "{task}")
        ])
        
        # åˆ›å»ºæœ€ç»ˆå›å¤æç¤ºæ¨¡æ¿
        self.final_prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œç°åœ¨éœ€è¦åŸºäºäººå·¥ä¸“å®¶çš„åé¦ˆç»™ç”¨æˆ·æœ€ç»ˆå›å¤ã€‚
è¯·æ•´åˆä¸“å®¶çš„å»ºè®®ï¼Œç»™å‡ºå®Œæ•´ã€æœ‰ç”¨çš„å›ç­”ã€‚"""),
            ("user", "ç”¨æˆ·ä»»åŠ¡ï¼š{task}\n\nä¸“å®¶åé¦ˆï¼š{human_feedback}")
        ])
        
        # åˆ›å»ºå·¥ä½œæµå›¾
        self.workflow = self._create_workflow()
        logger.info("å·¥ä½œæµåˆ›å»ºå®Œæˆ")
    
    async def ai_analysis_step(self, state: HumanInLoopState) -> Dict:
        """
        AIåˆ†ææ­¥éª¤
        
        åˆ†æç”¨æˆ·è¯·æ±‚ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦äººå·¥ååŠ©
        """
        try:
            logger.info("æ‰§è¡ŒAIåˆ†ææ­¥éª¤...")
            
            # åˆ›å»ºåˆ†æé“¾ï¼šprompt + LLM
            analysis_chain = self.ai_prompt | self.llm
            # è·å–åˆ†æç»“æœ
            result = await analysis_chain.ainvoke({
                "task": state["task"]
            })
            
            ai_response = result.content if hasattr(result, 'content') else str(result)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦äººå·¥ååŠ©
            need_human = "éœ€è¦äººå·¥ååŠ©" in ai_response or "ä¸“å®¶å»ºè®®" in state["task"] or "ååŠ©" in state["task"]
            
            return {
                "ai_response": ai_response,
                "human_input_needed": need_human,
                "steps": [{"ai_analysis": ai_response}]
            }
        except Exception as e:
            logger.error(f"AIåˆ†ææ­¥éª¤å‡ºé”™: {str(e)}")
            return {
                "ai_response": "åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ã€‚",
                "human_input_needed": False,
                "steps": [{"ai_analysis": f"é”™è¯¯: {str(e)}"}]
            }
    
    def human_input_step(self, state: HumanInLoopState) -> Dict:
        """
        äººå·¥è¾“å…¥æ­¥éª¤
        
        æ¨¡æ‹Ÿç­‰å¾…äººå·¥è¾“å…¥çš„è¿‡ç¨‹
        """
        logger.info("ç­‰å¾…äººå·¥è¾“å…¥...")
        
        print(f"\nğŸ¤– AIåˆ†æç»“æœ: {state['ai_response']}")
        print("="*50)
        print("ğŸ¤– ç³»ç»Ÿæç¤º: AIä»£ç†è¯·æ±‚äººå·¥ååŠ©")
        print(f"ğŸ“ ç”¨æˆ·ä»»åŠ¡: {state['task']}")
        
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œä¼šæš‚åœç­‰å¾…çœŸå®çš„äººå·¥è¾“å…¥
        # è¿™é‡Œæˆ‘ä»¬æ¨¡æ‹Ÿäººå·¥ä¸“å®¶çš„å›å¤
        if "AIä»£ç†" in state["task"] or "agent" in state["task"].lower():
            human_feedback = """ä¸“å®¶å»ºè®®ï¼š
1. ä½¿ç”¨ LangGraph æ„å»ºAIä»£ç†æ˜¯ä¸ªå¾ˆå¥½çš„é€‰æ‹©ï¼Œå®ƒæä¾›äº†ï¼š
   - çŠ¶æ€ç®¡ç†åŠŸèƒ½
   - äººæœºäº¤äº’æ”¯æŒ  
   - å·¥ä½œæµæ§åˆ¶
   - æ£€æŸ¥ç‚¹å’Œæ¢å¤æœºåˆ¶

2. æ¨èçš„å¼€å‘æ­¥éª¤ï¼š
   - å®šä¹‰çŠ¶æ€ç»“æ„
   - åˆ›å»ºå·¥ä½œæµèŠ‚ç‚¹
   - è®¾ç½®èŠ‚ç‚¹é—´çš„è¿æ¥
   - æ·»åŠ æ¡ä»¶åˆ†æ”¯é€»è¾‘

3. æœ€ä½³å®è·µï¼š
   - ä¿æŒçŠ¶æ€ç®€æ´
   - åˆç†è®¾è®¡èŠ‚ç‚¹ç²’åº¦
   - æ·»åŠ é”™è¯¯å¤„ç†
   - ä½¿ç”¨æ—¥å¿—è®°å½•æ‰§è¡Œè¿‡ç¨‹"""
        else:
            human_feedback = f"ä¸“å®¶å»ºè®®ï¼šå…³äº '{state['task']}'ï¼Œå»ºè®®æ‚¨æä¾›æ›´å…·ä½“çš„éœ€æ±‚æè¿°ï¼Œè¿™æ ·æˆ‘ä»¬å¯ä»¥ç»™å‡ºæ›´ç²¾å‡†çš„å»ºè®®ã€‚"
        
        print(f"ğŸ‘¨â€ğŸ’¼ ä¸“å®¶åé¦ˆ: {human_feedback}")
        
        return {
            "human_feedback": human_feedback,
            "steps": [{"human_input": human_feedback}]
        }
    
    async def final_response_step(self, state: HumanInLoopState) -> Dict:
        """
        æœ€ç»ˆå›å¤æ­¥éª¤
        
        åŸºäºäººå·¥åé¦ˆç”Ÿæˆæœ€ç»ˆå›å¤
        """
        try:
            logger.info("ç”Ÿæˆæœ€ç»ˆå›å¤...")
            
            # åˆ›å»ºæœ€ç»ˆå›å¤é“¾ï¼šprompt + LLM
            final_chain = self.final_prompt | self.llm
            # è·å–æœ€ç»ˆå›å¤
            result = await final_chain.ainvoke({
                "task": state["task"],
                "human_feedback": state["human_feedback"]
            })
            
            final_output = result.content if hasattr(result, 'content') else str(result)
            
            return {
                "final_output": final_output,
                "steps": [{"final_response": final_output}]
            }
        except Exception as e:
            logger.error(f"æœ€ç»ˆå›å¤æ­¥éª¤å‡ºé”™: {str(e)}")
            return {
                "final_output": f"ç”Ÿæˆæœ€ç»ˆå›å¤æ—¶å‡ºç°é”™è¯¯: {str(e)}",
                "steps": [{"final_response": f"é”™è¯¯: {str(e)}"}]
            }
    
    def decide_next_step(self, state: HumanInLoopState) -> str:
        """
        å†³å®šä¸‹ä¸€æ­¥æ“ä½œ
        
        åŸºäºå½“å‰çŠ¶æ€å†³å®šæ˜¯å¦éœ€è¦äººå·¥è¾“å…¥
        """
        if state.get("human_input_needed", False) and not state.get("human_feedback"):
            return "human_input"
        else:
            return "final_response"
    
    def _create_workflow(self) -> StateGraph:
        """
        åˆ›å»ºå·¥ä½œæµå›¾
        
        å®šä¹‰äº†å·¥ä½œæµçš„ç»“æ„ï¼š
        1. åˆ›å»ºçŠ¶æ€å›¾
        2. æ·»åŠ å·¥ä½œæµèŠ‚ç‚¹
        3. å®šä¹‰èŠ‚ç‚¹ä¹‹é—´çš„è¿æ¥å…³ç³»
        4. ç¼–è¯‘å·¥ä½œæµå›¾
        """
        workflow = StateGraph(HumanInLoopState)
        
        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("ai_analysis", self.ai_analysis_step)
        workflow.add_node("human_input", self.human_input_step)
        workflow.add_node("final_response", self.final_response_step)
        
        # æ·»åŠ è¾¹ï¼šå®šä¹‰èŠ‚ç‚¹ä¹‹é—´çš„è¿æ¥å…³ç³»
        workflow.add_edge(START, "ai_analysis")
        
        # æ·»åŠ æ¡ä»¶è¾¹ï¼šæ ¹æ®æ˜¯å¦éœ€è¦äººå·¥è¾“å…¥å†³å®šä¸‹ä¸€æ­¥
        workflow.add_conditional_edges(
            "ai_analysis",
            self.decide_next_step,
            {
                "human_input": "human_input",
                "final_response": "final_response"
            }
        )
        
        workflow.add_edge("human_input", "final_response")
        workflow.add_edge("final_response", END)
        
        return workflow.compile()
    
    async def run(self, task: str) -> Dict[str, Any]:
        """
        è¿è¡Œå·¥ä½œæµ
        
        æ‰§è¡Œäººæœºäº¤äº’è¿‡ç¨‹ï¼š
        1. åˆ›å»ºåˆå§‹çŠ¶æ€
        2. è¿è¡Œå·¥ä½œæµ
        3. æ”¶é›†æ¯ä¸ªæ­¥éª¤çš„ç»“æœ
        4. è¿”å›å®Œæ•´çš„æ‰§è¡Œè¿‡ç¨‹
        """
        # åˆ›å»ºåˆå§‹çŠ¶æ€
        inputs = HumanInLoopState(
            task=task,
            ai_response="",
            human_input_needed=False,
            human_feedback="",
            final_output="",
            steps=[]
        )
        
        all_steps = []
        async for event in self.workflow.astream(inputs):
            for k, v in event.items():
                if k != "__end__":
                    all_steps.append({k: v})
        
        return {
            "task": task,
            "steps": all_steps
        }


async def demo_human_in_loop():
    """æ¼”ç¤ºäººæœºäº¤äº’åŠŸèƒ½"""
    print("ğŸš€ LangGraph äººæœºäº¤äº’æ¼”ç¤º")
    print("="*50)
    
    demo = HumanInLoopDemo()
    
    # æµ‹è¯•ä»»åŠ¡åˆ—è¡¨
    test_tasks = [
        "æˆ‘éœ€è¦ä¸€äº›å…³äºæ„å»ºAIä»£ç†çš„ä¸“å®¶å»ºè®®ã€‚ä½ èƒ½ä¸ºæˆ‘è¯·æ±‚ååŠ©å—ï¼Ÿ",
        "å¦‚ä½•ä¼˜åŒ–Pythonä»£ç æ€§èƒ½ï¼Ÿ",
        "è¯·å¸®æˆ‘åˆ†æä¸€ä¸‹æœºå™¨å­¦ä¹ é¡¹ç›®çš„æ¶æ„è®¾è®¡ï¼Œéœ€è¦ä¸“å®¶å»ºè®®ã€‚"
    ]
    
    for i, task in enumerate(test_tasks, 1):
        print(f"\nğŸ“ æµ‹è¯•ä»»åŠ¡ {i}: {task}")
        print("-" * 60)
        
        try:
            result = await demo.run(task)
            
            print(f"\nâœ… ä»»åŠ¡å®Œæˆï¼æ‰§è¡Œæ­¥éª¤:")
            for j, step in enumerate(result["steps"], 1):
                step_name = list(step.keys())[0]
                step_data = list(step.values())[0]
                
                print(f"\nç¬¬{j}æ­¥ - {step_name}:")
                if isinstance(step_data, dict):
                    for key, value in step_data.items():
                        if key == "steps" and isinstance(value, list):
                            continue  # è·³è¿‡åµŒå¥—çš„steps
                        print(f"  {key}: {value}")
                else:
                    print(f"  ç»“æœ: {step_data}")
            
            print("=" * 60)
            
        except Exception as e:
            logger.error(f"æ‰§è¡Œä»»åŠ¡æ—¶å‡ºé”™: {str(e)}")
            print(f"âŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")


async def interactive_demo():
    """äº¤äº’å¼æ¼”ç¤º"""
    print("ğŸ¯ äº¤äº’å¼äººæœºäº¤äº’æ¼”ç¤º")
    print("è¾“å…¥ 'quit' é€€å‡º")
    print("="*50)
    
    demo = HumanInLoopDemo()
    
    while True:
        try:
            user_input = input("\nğŸ‘¤ è¯·è¾“å…¥æ‚¨çš„ä»»åŠ¡: ")
            if user_input.lower() in ['quit', 'exit', 'é€€å‡º']:
                break
            
            print("\nğŸ”„ å¤„ç†ä¸­...")
            result = await demo.run(user_input)
            
            print(f"\nâœ… å¤„ç†å®Œæˆï¼")
            print(f"ğŸ“ ä»»åŠ¡: {result['task']}")
            
            # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
            final_step = None
            for step in result["steps"]:
                if "final_response" in step:
                    final_step = step["final_response"]
                    break
            
            if final_step and isinstance(final_step, dict) and "final_output" in final_step:
                print(f"\nğŸ¯ æœ€ç»ˆå›å¤: {final_step['final_output']}")
            
        except KeyboardInterrupt:
            print("\nğŸ‘‹ å†è§ï¼")
            break
        except Exception as e:
            logger.error(f"äº¤äº’æ¼”ç¤ºå‡ºé”™: {str(e)}")
            print(f"âŒ é”™è¯¯: {e}")


async def main():
    """ä¸»å‡½æ•°"""
    print("é€‰æ‹©æ¼”ç¤ºæ¨¡å¼:")
    print("1. è‡ªåŠ¨æ¼”ç¤º")
    print("2. äº¤äº’å¼æ¼”ç¤º")
    
    choice = input("è¯·é€‰æ‹© (1/2): ").strip()
    
    if choice == "1":
        await demo_human_in_loop()
    elif choice == "2":
        await interactive_demo()
    else:
        print("æ— æ•ˆé€‰æ‹©ï¼Œè¿è¡Œè‡ªåŠ¨æ¼”ç¤º...")
        await demo_human_in_loop()


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
