# LangGraph Context å¿«é€Ÿå…¥é—¨

## 5åˆ†é’Ÿç†è§£Contextç³»ç»Ÿ

### ğŸ¯ ä»€ä¹ˆæ˜¯Contextï¼Ÿ

Contextï¼ˆä¸Šä¸‹æ–‡ï¼‰æ˜¯æ™ºèƒ½ä½“è·å–å’Œä½¿ç”¨ä¿¡æ¯çš„æ ¸å¿ƒæœºåˆ¶ã€‚å®ƒè®©AIèƒ½å¤Ÿï¼š
- ğŸ” **äº†è§£ç”¨æˆ·**: "è¿™æ˜¯Aliceï¼Œå¥¹å–œæ¬¢ç®€æ´å›ç­”"
- ğŸ”§ **ä½¿ç”¨å·¥å…·**: "è¿™æ˜¯æ•°æ®åº“è¿æ¥ï¼Œè¿™æ˜¯APIå¯†é’¥"
- ğŸ’¾ **è®°ä½çŠ¶æ€**: "æˆ‘ä»¬åˆšæ‰è®¨è®ºåˆ°å“ªé‡Œäº†"
- ğŸŒ **è·¨ä¼šè¯å­¦ä¹ **: "ç”¨æˆ·å†å²ä¸Šå–œæ¬¢ä»€ä¹ˆé£æ ¼"

### ğŸš« Context â‰  LLM Context

âš ï¸ **é‡è¦åŒºåˆ«**ï¼š
```python
# Runtime Context - ä»£ç è¿è¡Œæ—¶éœ€è¦çš„æ•°æ®
context = Context(user_id="alice", api_key="sk-...")

# LLM Context - ä¼ ç»™AIçš„æç¤ºè¯
llm_prompt = f"ç”¨æˆ·ID: {context.user_id}, è¯·å›ç­”: Hello"
```

## ğŸ“Š Contextçš„åˆ†ç±»ä½“ç³»

### æŒ‰å¯å˜æ€§åˆ†ç±»
```
é™æ€Context                    åŠ¨æ€Context
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ä¸å˜çš„é…ç½®ä¿¡æ¯   â”‚           â”‚ ä¼šå˜åŒ–çš„æ•°æ®     â”‚
â”‚ â€¢ ç”¨æˆ·ID        â”‚           â”‚ â€¢ å¯¹è¯å†å²       â”‚
â”‚ â€¢ APIå¯†é’¥       â”‚           â”‚ â€¢ ä¸­é—´ç»“æœ       â”‚
â”‚ â€¢ æ•°æ®åº“è¿æ¥     â”‚           â”‚ â€¢ è®¡ç®—çŠ¶æ€       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æŒ‰ç”Ÿå‘½å‘¨æœŸåˆ†ç±»
```
å•æ¬¡è¿è¡Œ                      è·¨ä¼šè¯æŒä¹…åŒ–
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ä¸€æ¬¡å¯¹è¯èŒƒå›´     â”‚           â”‚ å¤šæ¬¡å¯¹è¯èŒƒå›´     â”‚
â”‚ â€¢ å½“å‰æ¶ˆæ¯       â”‚           â”‚ â€¢ ç”¨æˆ·åå¥½       â”‚
â”‚ â€¢ ä¸´æ—¶å˜é‡       â”‚           â”‚ â€¢ å†å²è®°å¿†       â”‚
â”‚ â€¢ å·¥å…·ç»“æœ       â”‚           â”‚ â€¢ å­¦ä¹ ç»éªŒ       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ® ä¸‰ç§Contextç±»å‹

| ç±»å‹ | ç”¨é€” | ç¤ºä¾‹ | è®¿é—®æ–¹å¼ |
|------|------|------|----------|
| **é™æ€è¿è¡Œæ—¶** | é…ç½®ä¿¡æ¯ | ç”¨æˆ·IDã€æ¨¡å‹å | `context`å‚æ•° |
| **åŠ¨æ€è¿è¡Œæ—¶** | å¯¹è¯çŠ¶æ€ | æ¶ˆæ¯å†å²ã€ä¸­é—´ç»“æœ | Stateå¯¹è±¡ |
| **è·¨ä¼šè¯å­˜å‚¨** | é•¿æœŸè®°å¿† | ç”¨æˆ·åå¥½ã€å†å²ç»éªŒ | Storeå­˜å‚¨ |

## ğŸš€ å®æˆ˜ç¤ºä¾‹

### 1. é™æ€è¿è¡Œæ—¶Context

```python
from dataclasses import dataclass

@dataclass
class MyContext:
    user_name: str
    user_id: str
    preferred_language: str

# ä½¿ç”¨Context
await graph.ainvoke(
    {"messages": [("user", "Hello")]},
    context={
        "user_name": "Alice",
        "user_id": "alice-123",
        "preferred_language": "ä¸­æ–‡"
    }
)
```

**åœ¨èŠ‚ç‚¹ä¸­è®¿é—®**ï¼š
```python
from langgraph.runtime import Runtime

def my_node(state, runtime: Runtime[MyContext]):
    user_name = runtime.context.user_name
    language = runtime.context.preferred_language
    
    response = f"ä½ å¥½ {user_name}ï¼æˆ‘ä¼šç”¨{language}å›ç­”ã€‚"
    return {"response": response}
```

### 2. åŠ¨æ€è¿è¡Œæ—¶Context (çŠ¶æ€)

```python
from typing_extensions import TypedDict

class MyState(TypedDict):
    messages: list
    user_mood: str        # ç”¨æˆ·æƒ…ç»ª
    step_count: int       # å¯¹è¯æ­¥æ•°
    tool_results: list    # å·¥å…·ç»“æœ

def analyze_mood(state: MyState):
    last_message = state["messages"][-1]
    
    # åˆ†æç”¨æˆ·æƒ…ç»ªï¼ˆç®€åŒ–ç¤ºä¾‹ï¼‰
    if "å¼€å¿ƒ" in last_message.content:
        mood = "happy"
    elif "æ²®ä¸§" in last_message.content:
        mood = "sad"
    else:
        mood = "neutral"
    
    return {
        "user_mood": mood,
        "step_count": state.get("step_count", 0) + 1
    }
```

### 3. è·¨ä¼šè¯å­˜å‚¨Context

```python
from langgraph.store.memory import InMemoryStore

async def save_user_preference(user_id: str, preference: dict):
    store = InMemoryStore()
    
    # å­˜å‚¨ç”¨æˆ·åå¥½
    await store.aput(
        namespace=("preferences", user_id),
        key="communication_style",
        value=preference
    )

async def get_user_preference(user_id: str):
    store = InMemoryStore()
    
    # è·å–ç”¨æˆ·åå¥½
    result = await store.aget(
        namespace=("preferences", user_id),
        key="communication_style"
    )
    return result.value if result else {}
```

## ğŸ”§ é¡¹ç›®ä¸­çš„Contextå®ç°

### Contextå®šä¹‰

```python
# src/memory_agent/context.py
@dataclass(kw_only=True)
class Context:
    user_id: str = "default"
    model: str = "anthropic/claude-3-5-sonnet-20240620"
    system_prompt: str = SYSTEM_PROMPT
    
    def __post_init__(self):
        # è‡ªåŠ¨ä»ç¯å¢ƒå˜é‡åŠ è½½
        for f in fields(self):
            env_value = os.environ.get(f.name.upper(), getattr(self, f.name))
            setattr(self, f.name, env_value)
```

### Contextä½¿ç”¨æµç¨‹

```python
# src/memory_agent/graph.py
async def call_model(state: State, runtime: Runtime[Context]) -> dict:
    # 1. è·å–é™æ€Context
    user_id = runtime.context.user_id
    model = runtime.context.model
    
    # 2. è·å–è·¨ä¼šè¯æ•°æ®
    memories = await runtime.store.asearch(
        ("memories", user_id),
        query="æœ€è¿‘å¯¹è¯å†…å®¹"
    )
    
    # 3. ç»“åˆæ‰€æœ‰Context
    system_prompt = f"""
    ç”¨æˆ·ID: {user_id}
    ç›¸å…³è®°å¿†: {format_memories(memories)}
    å½“å‰æ—¶é—´: {datetime.now()}
    """
    
    # 4. è°ƒç”¨LLM
    response = await get_model(model).ainvoke([
        {"role": "system", "content": system_prompt},
        *state["messages"]  # åŠ¨æ€çŠ¶æ€
    ])
    
    return {"messages": [response]}
```

### å®é™…è°ƒç”¨ç¤ºä¾‹

```python
# æµ‹è¯•ä¸­çš„ä½¿ç”¨
async def test_context():
    # åˆ›å»ºå­˜å‚¨ï¼ˆè·¨ä¼šè¯ï¼‰
    store = InMemoryStore()
    
    # ç¼–è¯‘å›¾ï¼ˆçŠ¶æ€ç®¡ç†ï¼‰
    graph = builder.compile(store=store, checkpointer=MemorySaver())
    
    # è°ƒç”¨æ—¶ä¼ å…¥é™æ€Context
    result = await graph.ainvoke(
        {"messages": [("user", "æˆ‘å«Alice")]},    # åŠ¨æ€çŠ¶æ€
        {"thread_id": "conversation-1"},         # çŠ¶æ€æ ‡è¯†
        context=Context(                         # é™æ€Context
            user_id="alice-123",
            model="azure_openai/gpt-4o"
        )
    )
```

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. Contextè®¾è®¡åŸåˆ™

```python
# âœ… å¥½çš„Contextè®¾è®¡
@dataclass
class GoodContext:
    # å¿…éœ€å­—æ®µ
    user_id: str
    
    # å¯é€‰é…ç½®
    model: str = "gpt-4"
    language: str = "zh-CN"
    
    # åŠŸèƒ½å¼€å…³
    enable_memory: bool = True
    enable_tools: bool = True

# âŒ é¿å…çš„è®¾è®¡
@dataclass
class BadContext:
    everything: dict  # å¤ªå®½æ³›
    config: str      # ç±»å‹ä¸æ˜ç¡®
```

### 2. å®‰å…¨è®¿é—®Context

```python
def safe_get_context(runtime: Runtime[Context]):
    try:
        user_id = runtime.context.user_id
        model = getattr(runtime.context, 'model', 'default')
        return user_id, model
    except AttributeError:
        return "default-user", "default-model"
```

### 3. ContextéªŒè¯

```python
def validate_context(context: Context):
    if not context.user_id:
        raise ValueError("user_idä¸èƒ½ä¸ºç©º")
    
    if not context.model:
        raise ValueError("modelä¸èƒ½ä¸ºç©º")
    
    return True
```

## ğŸ› å¸¸è§é—®é¢˜

### Q: Contextä¿¡æ¯æ²¡æœ‰ä¼ é€’åˆ°å·¥å…·ä¸­ï¼Ÿ
**A**: ä½¿ç”¨`get_runtime()`è·å–Contextï¼š
```python
from langgraph.runtime import get_runtime

@tool
def my_tool():
    runtime = get_runtime(MyContext)
    user_id = runtime.context.user_id
    return f"å¤„ç†ç”¨æˆ· {user_id} çš„è¯·æ±‚"
```

### Q: ä¸åŒç»„ä»¶ä¸­Contextä¸ä¸€è‡´ï¼Ÿ
**A**: ç¡®ä¿ä½¿ç”¨ç›¸åŒçš„Context Schemaï¼š
```python
# åœ¨æ‰€æœ‰ç»„ä»¶ä¸­ä½¿ç”¨ç›¸åŒçš„Schema
from langgraph.runtime import Runtime

def node1(state, runtime: Runtime[Context]): pass
def node2(state, runtime: Runtime[Context]): pass  # ç›¸åŒçš„Contextç±»å‹
```

### Q: Contextæ•°æ®è¿‡å¤§å½±å“æ€§èƒ½ï¼Ÿ
**A**: ä½¿ç”¨å»¶è¿ŸåŠ è½½ï¼š
```python
@dataclass
class LazyContext:
    user_id: str
    _user_profile: Optional[dict] = None
    
    async def get_user_profile(self):
        if self._user_profile is None:
            self._user_profile = await load_profile(self.user_id)
        return self._user_profile
```

## ğŸ¯ Contextç»„åˆä½¿ç”¨

### å®Œæ•´ç¤ºä¾‹ï¼šä¸ªæ€§åŒ–æ™ºèƒ½åŠ©æ‰‹

```python
@dataclass
class AssistantContext:
    user_id: str
    language: str = "zh-CN"
    tone: str = "friendly"

class AssistantState(TypedDict):
    messages: list
    user_mood: str
    conversation_summary: str

async def personalized_assistant():
    # 1. è®¾ç½®é™æ€Context
    context = AssistantContext(
        user_id="alice-123",
        language="ä¸­æ–‡",
        tone="professional"
    )
    
    # 2. åˆ›å»ºè·¨ä¼šè¯å­˜å‚¨
    store = InMemoryStore()
    await store.aput(
        ("preferences", "alice-123"),
        "style",
        {"detail_level": "concise", "examples": True}
    )
    
    # 3. è¿è¡Œå›¾
    result = await graph.ainvoke(
        {
            "messages": [("user", "è§£é‡Šä¸€ä¸‹æœºå™¨å­¦ä¹ ")],
            "user_mood": "curious"
        },
        context=context,
        store=store
    )
    
    return result
```

## ğŸ”— è¿›ä¸€æ­¥å­¦ä¹ 

- ğŸ“– [è¯¦ç»†æŒ‡å—](./LangGraph_Context_æ·±å…¥æŒ‡å—.md) - å®Œæ•´çš„Contextç³»ç»Ÿæ–‡æ¡£
- ğŸŒ [å®˜æ–¹æ–‡æ¡£](https://langchain-ai.github.io/langgraph/agents/context/) - LangGraph Contextæ¦‚å¿µ
- ğŸ’» [é¡¹ç›®æºç ](../src/memory_agent/context.py) - å®é™…å®ç°ç¤ºä¾‹
- ğŸ§  [Memoryç³»ç»Ÿ](./å¿«é€Ÿå…¥é—¨_Memoryç³»ç»Ÿ.md) - Contextä¸Memoryçš„ç»“åˆä½¿ç”¨

---

**è®°ä½**: Contextæ˜¯æ™ºèƒ½ä½“çš„"æ„ŸçŸ¥å™¨å®˜"ï¼Œè®©AIèƒ½å¤Ÿç†è§£å’Œé€‚åº”ä¸åŒçš„ä½¿ç”¨åœºæ™¯ã€‚ä»ç®€å•çš„ç”¨æˆ·IDå¼€å§‹ï¼Œé€æ­¥æ„å»ºæ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡æ„ŸçŸ¥èƒ½åŠ›ï¼
