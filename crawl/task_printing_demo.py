#!/usr/bin/env python3
"""
ä»»åŠ¡æ‰“å°æ¼”ç¤ºè„šæœ¬

å±•ç¤ºæ™ºèƒ½æŠ“å–å™¨å¦‚ä½•è¯¦ç»†æ‰“å°æ¯ä¸ªä»»åŠ¡æ­¥éª¤çš„ä¿¡æ¯
"""

import asyncio
from amazon_scraper_with_llm import AmazonScraperWithLLM


async def demo_task_printing():
    """æ¼”ç¤ºä»»åŠ¡æ‰“å°åŠŸèƒ½"""
    print("ğŸ–¨ï¸  ä»»åŠ¡æ‰“å°åŠŸèƒ½æ¼”ç¤º")
    print("=" * 80)
    
    # åˆ›å»ºæŠ“å–å™¨å®ä¾‹
    scraper = AmazonScraperWithLLM(use_azure=True)
    
    try:
        # åˆå§‹åŒ–
        print("ğŸ”„ æ­£åœ¨åˆå§‹åŒ–æŠ“å–å™¨...")
        if not await scraper.initialize():
            print("âŒ åˆå§‹åŒ–å¤±è´¥")
            return
        
        print("âœ… æŠ“å–å™¨åˆå§‹åŒ–æˆåŠŸ")
        
        # æ¼”ç¤ºä»»åŠ¡ - ç”Ÿæˆæ­¥éª¤ä½†ä¸æ‰§è¡Œ
        print("\n" + "="*60 + " ä»»åŠ¡åˆ†ææ¼”ç¤º " + "="*60)
        task_description = "æœç´¢wireless earbudsï¼Œè®¾ç½®é‚®ç¼–10001ï¼ŒæŠ“å–å‰2é¡µæ•°æ®å¹¶å¯¼å‡ºCSVå’ŒJSON"
        
        print(f"ğŸ“ ç”¨æˆ·ä»»åŠ¡: {task_description}")
        
        # 1. LLM åˆ†æä»»åŠ¡
        print(f"\nğŸ§  æ­£åœ¨åˆ†æä»»åŠ¡...")
        current_task = await scraper.task_planner.analyze_task(task_description)
        
        print(f"ğŸ“‹ ä»»åŠ¡è§£æç»“æœ:")
        print(f"  ğŸ” æœç´¢å…³é”®è¯: {current_task.search_keyword}")
        print(f"  ğŸ“® é‚®ç¼–è®¾ç½®: {current_task.zip_code or 'æœªè®¾ç½®'}")
        print(f"  ğŸ“„ æŠ“å–é¡µæ•°: {current_task.pages_to_scrape}")
        print(f"  ğŸ’¾ å¯¼å‡ºæ ¼å¼: {current_task.export_format}")
        print(f"  ğŸ“ é™„åŠ æŒ‡ä»¤: {current_task.additional_instructions[:50]}...")
        
        # 2. LLM ç”Ÿæˆæ‰§è¡Œæ­¥éª¤
        print(f"\nğŸ”§ æ­£åœ¨ç”Ÿæˆæ‰§è¡Œæ­¥éª¤...")
        task_steps = await scraper.task_planner.generate_task_steps(current_task)
        scraper.task_steps = task_steps  # è®¾ç½®åˆ°æŠ“å–å™¨ä¸­
        scraper.current_task = current_task
        
        print(f"ğŸ“ ç”Ÿæˆäº† {len(task_steps)} ä¸ªæ‰§è¡Œæ­¥éª¤")
        
        # 3. ä½¿ç”¨æ–°çš„æ‰“å°å‡½æ•°å±•ç¤ºä»»åŠ¡
        print(f"\n" + "="*60 + " ä»»åŠ¡æ­¥éª¤æ€»è§ˆ " + "="*60)
        scraper.print_task_steps_summary()
        
        # 4. å±•ç¤ºæ¯ä¸ªæ­¥éª¤çš„è¯¦ç»†ä¿¡æ¯
        print(f"\n" + "="*60 + " è¯¦ç»†æ­¥éª¤ä¿¡æ¯ " + "="*60)
        for i, step in enumerate(task_steps, 1):
            scraper.print_task_step_detail(step)
            if i < len(task_steps):
                input("\nâ³ æŒ‰ Enter é”®æŸ¥çœ‹ä¸‹ä¸€ä¸ªæ­¥éª¤...")
        
        # 5. æ¨¡æ‹Ÿä¸€äº›æ­¥éª¤çš„æ‰§è¡ŒçŠ¶æ€
        print(f"\n" + "="*60 + " æ¨¡æ‹Ÿæ‰§è¡ŒçŠ¶æ€ " + "="*60)
        
        # æ¨¡æ‹Ÿç¬¬1æ­¥æˆåŠŸ
        task_steps[0].completed = True
        task_steps[0].result = {"message": "æˆåŠŸå¯¼èˆªåˆ°äºšé©¬é€Šé¦–é¡µ", "url": "https://www.amazon.com"}
        
        # æ¨¡æ‹Ÿç¬¬2æ­¥å¤±è´¥
        if len(task_steps) > 1:
            task_steps[1].error = "æ— æ³•æ‰¾åˆ°ä½ç½®è®¾ç½®æŒ‰é’®ï¼Œé¡µé¢ç»“æ„å¯èƒ½å‘ç”Ÿå˜åŒ–"
        
        # æ¨¡æ‹Ÿç¬¬3æ­¥æˆåŠŸ
        if len(task_steps) > 2:
            task_steps[2].completed = True
            task_steps[2].result = {"message": "æœç´¢æˆåŠŸ", "keyword": "wireless earbuds"}
        
        # é‡æ–°æ˜¾ç¤ºæ›´æ–°åçš„çŠ¶æ€
        print("ğŸ”„ æ›´æ–°åçš„ä»»åŠ¡çŠ¶æ€:")
        scraper.print_task_steps_summary()
        
        # æ˜¾ç¤ºæ‰§è¡Œç»Ÿè®¡
        completed_steps = sum(1 for step in task_steps if step.completed)
        failed_steps = sum(1 for step in task_steps if step.error)
        pending_steps = len(task_steps) - completed_steps - failed_steps
        
        print(f"\nğŸ“Š æ‰§è¡Œç»Ÿè®¡:")
        print(f"  âœ… å·²å®Œæˆ: {completed_steps} æ­¥")
        print(f"  âŒ å¤±è´¥: {failed_steps} æ­¥")
        print(f"  â³ å¾…æ‰§è¡Œ: {pending_steps} æ­¥")
        print(f"  ğŸ“ˆ æˆåŠŸç‡: {completed_steps/len(task_steps)*100:.1f}%")
        
        print(f"\nğŸ‰ ä»»åŠ¡æ‰“å°æ¼”ç¤ºå®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        await scraper.cleanup()


async def demo_step_types():
    """æ¼”ç¤ºä¸åŒç±»å‹çš„ä»»åŠ¡æ­¥éª¤"""
    print("\n" + "="*80)
    print("ğŸ­ ä¸åŒç±»å‹ä»»åŠ¡æ­¥éª¤æ¼”ç¤º")
    print("=" * 80)
    
    from amazon_scraper_with_llm import TaskStep
    
    # åˆ›å»ºä¸åŒç±»å‹çš„ç¤ºä¾‹æ­¥éª¤
    sample_steps = [
        TaskStep(
            step_number=1,
            action="navigate",
            description="å¯¼èˆªåˆ°äºšé©¬é€Šé¦–é¡µ",
            mcp_tool="browser_navigate",
            parameters={"url": "https://www.amazon.com"},
            expected_result="æˆåŠŸè®¿é—®äºšé©¬é€Šé¦–é¡µ",
            completed=True
        ),
        TaskStep(
            step_number=2,
            action="set_zipcode",
            description="è®¾ç½®é…é€åœ°å€é‚®ç¼–ä¸º 10001",
            mcp_tool="browser_click",
            parameters={"element": "ä½ç½®è®¾ç½®", "ref": "#glow-ingress-line1"},
            expected_result="é‚®ç¼–è®¾ç½®ä¸º 10001",
            error="å®šä½å…ƒç´ å¤±è´¥ï¼šé¡µé¢ç»“æ„å‘ç”Ÿå˜åŒ–"
        ),
        TaskStep(
            step_number=3,
            action="search",
            description="æœç´¢å…³é”®è¯ 'wireless earbuds'",
            mcp_tool="browser_type",
            parameters={
                "element": "æœç´¢æ¡†",
                "ref": "#twotabsearchtextbox",
                "text": "wireless earbuds",
                "submit": True
            },
            expected_result="æ˜¾ç¤º wireless earbuds çš„æœç´¢ç»“æœ",
            completed=True
        ),
        TaskStep(
            step_number=4,
            action="extract_data",
            description="æå–ç¬¬ 1 é¡µäº§å“æ•°æ®",
            mcp_tool="browser_snapshot",
            parameters={},
            expected_result="æˆåŠŸæå–ç¬¬ 1 é¡µäº§å“ä¿¡æ¯"
        ),
        TaskStep(
            step_number=5,
            action="next_page",
            description="ç¿»åˆ°ç¬¬ 2 é¡µ",
            mcp_tool="browser_click",
            parameters={"element": "ä¸‹ä¸€é¡µæŒ‰é’®", "ref": ".a-pagination .a-last a"},
            expected_result="æˆåŠŸç¿»åˆ°ç¬¬ 2 é¡µ"
        )
    ]
    
    # åˆ›å»ºä¸´æ—¶æŠ“å–å™¨å®ä¾‹æ¥ä½¿ç”¨æ‰“å°å‡½æ•°
    scraper = AmazonScraperWithLLM(use_azure=True)
    scraper.task_steps = sample_steps
    
    # æ˜¾ç¤ºæ­¥éª¤æ‘˜è¦
    scraper.print_task_steps_summary()
    
    # æ˜¾ç¤ºæ¯ä¸ªæ­¥éª¤çš„è¯¦ç»†ä¿¡æ¯
    print(f"\nğŸ“‹ å„æ­¥éª¤è¯¦ç»†ä¿¡æ¯:")
    for step in sample_steps:
        scraper.print_task_step_detail(step)
    
    print(f"\nâœ¨ æ­¥éª¤ç±»å‹æ¼”ç¤ºå®Œæˆ!")


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ–¨ï¸  æ™ºèƒ½æŠ“å–å™¨ä»»åŠ¡æ‰“å°ç³»ç»Ÿ")
    print("å±•ç¤ºå¦‚ä½•è¯¦ç»†æ‰“å°å’Œè·Ÿè¸ªæ¯ä¸ªä»»åŠ¡æ­¥éª¤")
    print("=" * 80)
    
    # æ¼”ç¤º1ï¼šå®Œæ•´çš„ä»»åŠ¡æ‰“å°æµç¨‹
    await demo_task_printing()
    
    # æ¼”ç¤º2ï¼šä¸åŒç±»å‹çš„æ­¥éª¤
    await demo_step_types()
    
    print("\nğŸŠ æ‰€æœ‰æ¼”ç¤ºå®Œæˆ!")


if __name__ == "__main__":
    print("ğŸš€ å¯åŠ¨ä»»åŠ¡æ‰“å°æ¼”ç¤º...")
    print("ğŸ’¡ è¿™ä¸ªæ¼”ç¤ºå°†å±•ç¤ºæ™ºèƒ½æŠ“å–å™¨å¦‚ä½•è¯¦ç»†æ‰“å°æ¯ä¸ªä»»åŠ¡æ­¥éª¤")
    print()
    
    asyncio.run(main())