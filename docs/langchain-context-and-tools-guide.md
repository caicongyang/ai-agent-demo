# LangChain ä¸Šä¸‹æ–‡ç®¡ç†ä¸å·¥å…·è°ƒç”¨å®Œæ•´æŒ‡å—

## ç›®å½•
1. [æ¦‚è¿°](#æ¦‚è¿°)
2. [ä¸Šä¸‹æ–‡ç®¡ç†](#ä¸Šä¸‹æ–‡ç®¡ç†)
3. [å·¥å…·è°ƒç”¨æœºåˆ¶](#å·¥å…·è°ƒç”¨æœºåˆ¶)
4. [çŠ¶æ€ç®¡ç†](#çŠ¶æ€ç®¡ç†)
5. [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)
6. [å®é™…åº”ç”¨æ¡ˆä¾‹](#å®é™…åº”ç”¨æ¡ˆä¾‹)

## æ¦‚è¿°

LangChain æ˜¯ä¸€ä¸ªå¼ºå¤§çš„æ¡†æ¶ï¼Œä¸“ä¸ºæ„å»ºåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„åº”ç”¨ç¨‹åºè€Œè®¾è®¡ã€‚æœ¬æ–‡æ¡£å°†æ·±å…¥æ¢è®¨ LangChain å¦‚ä½•å¤„ç†ä¸¤ä¸ªæ ¸å¿ƒæ¦‚å¿µï¼š**ä¸Šä¸‹æ–‡ç®¡ç†**å’Œ**å·¥å…·è°ƒç”¨**ã€‚

### æ ¸å¿ƒæ¦‚å¿µ
- **ä¸Šä¸‹æ–‡ï¼ˆContextï¼‰**ï¼šåœ¨å¯¹è¯æˆ–ä»»åŠ¡æ‰§è¡Œè¿‡ç¨‹ä¸­ä¿æŒå’Œä¼ é€’çš„ä¿¡æ¯
- **å·¥å…·è°ƒç”¨ï¼ˆTool Callingï¼‰**ï¼šè®© AI èƒ½å¤Ÿè°ƒç”¨å¤–éƒ¨å‡½æ•°æˆ–æœåŠ¡æ¥å®Œæˆç‰¹å®šä»»åŠ¡
- **çŠ¶æ€ç®¡ç†ï¼ˆState Managementï¼‰**ï¼šåœ¨å¤æ‚å·¥ä½œæµä¸­ç»´æŠ¤å’Œæ›´æ–°åº”ç”¨çŠ¶æ€

## ä¸Šä¸‹æ–‡ç®¡ç†

### 1. å¯¹è¯å†å²è®°å¿†

LangChain æä¾›å¤šç§è®°å¿†æœºåˆ¶æ¥ç®¡ç†å¯¹è¯ä¸Šä¸‹æ–‡ï¼š

#### ConversationBufferMemory - å®Œæ•´å†å²è®°å¿†

```python
from langchain.memory import ConversationBufferMemory
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

# åˆå§‹åŒ–å¯¹è¯å†å²è®°å¿†
buffer_memory = ConversationBufferMemory(
    return_messages=True,
    memory_key="chat_history"
)

# åˆ›å»ºå¸¦è®°å¿†çš„å¯¹è¯æ¨¡æ¿
chat_prompt = ChatPromptTemplate.from_messages([
    MessagesPlaceholder(variable_name="chat_history"),  # æ’å…¥å†å²å¯¹è¯
    ("human", "{input}"),
])

# ä½¿ç”¨è®°å¿†
memory_content = buffer_memory.load_memory_variables({})
response = chain.invoke({
    "input": "ä½ å¥½ï¼Œæˆ‘å«å°æ˜",
    **memory_content
})

# ä¿å­˜å¯¹è¯åˆ°è®°å¿†
buffer_memory.save_context(
    {"input": "ä½ å¥½ï¼Œæˆ‘å«å°æ˜"},
    {"output": response}
)
```

**ç‰¹ç‚¹ï¼š**
- âœ… ä¿ç•™å®Œæ•´çš„å¯¹è¯å†å²
- âœ… é€‚åˆçŸ­æœŸå¯¹è¯
- âŒ é•¿å¯¹è¯ä¼šæ¶ˆè€—å¤§é‡ token

#### ConversationSummaryMemory - æ‘˜è¦è®°å¿†

```python
from langchain.memory import ConversationSummaryMemory

# åˆå§‹åŒ–æ‘˜è¦è®°å¿†
summary_memory = ConversationSummaryMemory(
    llm=llm,
    memory_key="chat_summary"
)

# æ‘˜è¦è®°å¿†ä¼šè‡ªåŠ¨å°†é•¿å¯¹è¯å‹ç¼©ä¸ºæ‘˜è¦
chat_prompt = ChatPromptTemplate.from_messages([
    ("system", "å¯¹è¯å†å²æ‘˜è¦ï¼š{chat_summary}"),
    ("human", "{input}"),
])
```

**ç‰¹ç‚¹ï¼š**
- âœ… èŠ‚çœ token æ¶ˆè€—
- âœ… é€‚åˆé•¿æœŸå¯¹è¯
- âŒ å¯èƒ½ä¸¢å¤±ç»†èŠ‚ä¿¡æ¯

### 2. LangGraph ä¸­çš„çŠ¶æ€ç®¡ç†

LangGraph é€šè¿‡ TypedDict å®šä¹‰çŠ¶æ€ç»“æ„ï¼Œå®ç°æ›´ç²¾ç»†çš„ä¸Šä¸‹æ–‡æ§åˆ¶ï¼š

```python
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, END, START

class QAState(TypedDict):
    """é—®ç­”çŠ¶æ€å®šä¹‰"""
    question: str        # ç”¨æˆ·é—®é¢˜
    thoughts: str        # AI æ€è€ƒè¿‡ç¨‹
    final_answer: str    # æœ€ç»ˆç­”æ¡ˆ

class SimpleQADemo:
    def __init__(self):
        self.workflow = self._create_workflow()
    
    async def think_step(self, state: QAState) -> Dict:
        """æ€è€ƒæ­¥éª¤ - åˆ†æé—®é¢˜"""
        result = await self.llm.ainvoke([
            {"role": "system", "content": "ä»”ç»†æ€è€ƒå¦‚ä½•å›ç­”ç”¨æˆ·é—®é¢˜"},
            {"role": "user", "content": state["question"]}
        ])
        return {"thoughts": result.content}
    
    async def answer_step(self, state: QAState) -> Dict:
        """å›ç­”æ­¥éª¤ - åŸºäºæ€è€ƒç»™å‡ºç­”æ¡ˆ"""
        result = await self.llm.ainvoke([
            {"role": "system", "content": "æ ¹æ®æ€è€ƒè¿‡ç¨‹ç»™å‡ºæ¸…æ™°ç­”æ¡ˆ"},
            {"role": "user", "content": f"é—®é¢˜ï¼š{state['question']}\næ€è€ƒï¼š{state['thoughts']}"}
        ])
        return {"final_answer": result.content}
    
    def _create_workflow(self):
        """åˆ›å»ºå·¥ä½œæµå›¾"""
        workflow = StateGraph(QAState)
        
        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("think", self.think_step)
        workflow.add_node("answer", self.answer_step)
        
        # å®šä¹‰æ‰§è¡Œæµç¨‹
        workflow.add_edge(START, "think")
        workflow.add_edge("think", "answer")
        workflow.add_edge("answer", END)
        
        return workflow.compile()
```

**çŠ¶æ€ç®¡ç†ç‰¹ç‚¹ï¼š**
- ğŸ”„ **çŠ¶æ€ä¼ é€’**ï¼šæ¯ä¸ªèŠ‚ç‚¹éƒ½èƒ½è®¿é—®å’Œä¿®æ”¹å…¨å±€çŠ¶æ€
- ğŸ“Š **ç±»å‹å®‰å…¨**ï¼šé€šè¿‡ TypedDict ç¡®ä¿çŠ¶æ€ç»“æ„æ­£ç¡®
- ğŸ”€ **çµæ´»æ§åˆ¶**ï¼šå¯ä»¥æ ¹æ®çŠ¶æ€å†³å®šæ‰§è¡Œè·¯å¾„

### 3. ä¸Šä¸‹æ–‡åœ¨å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä¸­çš„åº”ç”¨

```python
class TeamState(TypedDict):
    """å›¢é˜Ÿåä½œçŠ¶æ€"""
    task: str           # ä»»åŠ¡æè¿°
    research: str       # ç ”ç©¶ç»“æœ
    draft: str         # åˆç¨¿å†…å®¹
    feedback: str      # å®¡æ ¸åé¦ˆ
    final_output: str  # æœ€ç»ˆè¾“å‡º

class TeamCollaborationDemo:
    async def research_step(self, state: TeamState) -> Dict:
        """ç ”ç©¶å‘˜æ”¶é›†ä¿¡æ¯"""
        result = await self.llm.ainvoke([
            {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šç ”ç©¶å‘˜ï¼Œæ”¶é›†åˆ†æç›¸å…³ä¿¡æ¯"},
            {"role": "user", "content": f"ç ”ç©¶ä¸»é¢˜ï¼š{state['task']}"}
        ])
        return {"research": result.content}
    
    async def write_step(self, state: TeamState) -> Dict:
        """ä½œå®¶åŸºäºç ”ç©¶æ’°å†™å†…å®¹"""
        result = await self.llm.ainvoke([
            {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šä½œå®¶ï¼Œåˆ›ä½œç»“æ„è‰¯å¥½çš„å†…å®¹"},
            {"role": "user", "content": f"ä»»åŠ¡ï¼š{state['task']}\nç ”ç©¶ç»“æœï¼š{state['research']}"}
        ])
        return {"draft": result.content}
    
    async def review_step(self, state: TeamState) -> Dict:
        """å®¡æ ¸å‘˜å®¡æŸ¥å†…å®¹è´¨é‡"""
        result = await self.llm.ainvoke([
            {"role": "system", "content": "ä½ æ˜¯ä¸¥æ ¼çš„å®¡æ ¸å‘˜ï¼Œè¯„ä¼°å†…å®¹è´¨é‡"},
            {"role": "user", "content": f"ä»»åŠ¡ï¼š{state['task']}\nå†…å®¹ï¼š{state['draft']}"}
        ])
        
        feedback = result.content
        if "é€šè¿‡" in feedback:
            return {"feedback": feedback, "final_output": state["draft"]}
        return {"feedback": feedback}
```

## å·¥å…·è°ƒç”¨æœºåˆ¶

### 1. åŸºç¡€å·¥å…·å®šä¹‰

LangChain æ”¯æŒå¤šç§æ–¹å¼å®šä¹‰å’Œä½¿ç”¨å·¥å…·ï¼š

#### ä½¿ç”¨ Pydantic å®šä¹‰å·¥å…·

```python
from pydantic import BaseModel, Field
from langchain_core.output_parsers import PydanticToolsParser

class Add(BaseModel):
    """åŠ æ³•å·¥å…·"""
    a: int = Field(..., description="ç¬¬ä¸€ä¸ªæ•´æ•°")
    b: int = Field(..., description="ç¬¬äºŒä¸ªæ•´æ•°")

class Multiply(BaseModel):
    """ä¹˜æ³•å·¥å…·"""
    a: int = Field(..., description="ç¬¬ä¸€ä¸ªæ•´æ•°")
    b: int = Field(..., description="ç¬¬äºŒä¸ªæ•´æ•°")

# å®é™…æ‰§è¡Œå‡½æ•°
def add(a: int, b: int) -> int:
    return a + b

def multiply(a: int, b: int) -> int:
    return a * b

class ToolCallingDemo:
    def __init__(self):
        self.tools = [Add, Multiply]
        self.llm_with_tools = self.llm.bind_tools(self.tools)
    
    def execute_tools(self, query: str) -> List[int]:
        """æ‰§è¡Œå·¥å…·å¹¶è¿”å›ç»“æœ"""
        # è§£æå·¥å…·è°ƒç”¨
        chain = self.llm_with_tools | PydanticToolsParser(tools=self.tools)
        tool_calls = chain.invoke(query)
        
        results = []
        for tool_call in tool_calls:
            if isinstance(tool_call, Add):
                results.append(add(tool_call.a, tool_call.b))
            elif isinstance(tool_call, Multiply):
                results.append(multiply(tool_call.a, tool_call.b))
        
        return results
```

#### ä½¿ç”¨ @tool è£…é¥°å™¨

```python
from langchain_core.tools import tool

@tool
def search_web(query: str) -> str:
    """æœç´¢ç½‘é¡µä¿¡æ¯"""
    # å®é™…æœç´¢é€»è¾‘
    return f"æœç´¢ç»“æœï¼š{query}"

@tool 
def calculate_math(expression: str) -> str:
    """è®¡ç®—æ•°å­¦è¡¨è¾¾å¼"""
    try:
        result = eval(expression)  # æ³¨æ„ï¼šå®é™…åº”ç”¨ä¸­éœ€è¦å®‰å…¨å¤„ç†
        return f"è®¡ç®—ç»“æœï¼š{result}"
    except:
        return "è®¡ç®—é”™è¯¯"

# ç»‘å®šå·¥å…·åˆ°æ¨¡å‹
tools = [search_web, calculate_math]
llm_with_tools = llm.bind_tools(tools)
```

### 2. æœç´¢å·¥å…·é›†æˆ

```python
from langchain_community.utilities import SerpAPIWrapper
from langchain.tools import Tool

def init_search_tool():
    """åˆå§‹åŒ–æœç´¢å·¥å…·"""
    search = SerpAPIWrapper(
        params={
            "engine": "google",
            "api_key": os.getenv("SERPAPI_API_KEY")
        }
    )
    
    return Tool(
        name="Search",
        func=search.run,
        description="ç”¨äºæœç´¢æœ€æ–°ä¿¡æ¯çš„å·¥å…·ã€‚è¾“å…¥åº”è¯¥æ˜¯æœç´¢æŸ¥è¯¢ã€‚"
    )

# åœ¨å·¥ä½œæµä¸­ä½¿ç”¨æœç´¢å·¥å…·
async def execute_step(self, state: PlanExecuteState) -> Dict:
    """æ‰§è¡Œè®¡åˆ’æ­¥éª¤"""
    task = state["plan"][0]
    
    if "æœç´¢" in task and search_tool:
        # æ‰§è¡Œæœç´¢
        search_query = task.replace("æœç´¢", "").strip()
        search_result = search_tool.run(search_query)
        result_content = f"æœç´¢ç»“æœ: {search_result}"
    else:
        # ä½¿ç”¨ LLM å¤„ç†
        result = await self.llm.ainvoke([
            {"role": "user", "content": task}
        ])
        result_content = result.content
    
    return {"past_steps": [(task, result_content)]}
```

### 3. è‡ªå®šä¹‰å·¥å…·é›†æˆ

```python
class CustomToolDemo:
    def __init__(self):
        self.tools = self._create_custom_tools()
        self.llm_with_tools = self.llm.bind_tools(self.tools)
    
    def _create_custom_tools(self):
        """åˆ›å»ºè‡ªå®šä¹‰å·¥å…·é›†"""
        
        @tool
        def get_weather(city: str) -> str:
            """è·å–æŒ‡å®šåŸå¸‚çš„å¤©æ°”ä¿¡æ¯"""
            # æ¨¡æ‹Ÿå¤©æ°” API è°ƒç”¨
            weather_data = {
                "åŒ—äº¬": "æ™´å¤©ï¼Œ15Â°C",
                "ä¸Šæµ·": "å¤šäº‘ï¼Œ18Â°C", 
                "å¹¿å·": "å°é›¨ï¼Œ22Â°C"
            }
            return weather_data.get(city, f"æ— æ³•è·å–{city}çš„å¤©æ°”ä¿¡æ¯")
        
        @tool
        def translate_text(text: str, target_lang: str = "è‹±è¯­") -> str:
            """ç¿»è¯‘æ–‡æœ¬åˆ°ç›®æ ‡è¯­è¨€"""
            # æ¨¡æ‹Ÿç¿»è¯‘æœåŠ¡
            return f"å·²å°†'{text}'ç¿»è¯‘ä¸º{target_lang}"
        
        @tool
        def save_note(content: str) -> str:
            """ä¿å­˜ç¬”è®°å†…å®¹"""
            # æ¨¡æ‹Ÿä¿å­˜æ“ä½œ
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            return f"ç¬”è®°å·²ä¿å­˜ [{timestamp}]: {content[:50]}..."
        
        return [get_weather, translate_text, save_note]
    
    async def process_with_tools(self, user_input: str) -> str:
        """ä½¿ç”¨å·¥å…·å¤„ç†ç”¨æˆ·è¾“å…¥"""
        # è®© LLM å†³å®šæ˜¯å¦éœ€è¦ä½¿ç”¨å·¥å…·
        response = self.llm_with_tools.invoke(user_input)
        
        # å¦‚æœæœ‰å·¥å…·è°ƒç”¨
        if response.tool_calls:
            tool_results = []
            for tool_call in response.tool_calls:
                tool_name = tool_call["name"]
                tool_args = tool_call["args"]
                
                # æ‰§è¡Œç›¸åº”çš„å·¥å…·
                for tool in self.tools:
                    if tool.name == tool_name:
                        result = tool.run(tool_args)
                        tool_results.append(f"{tool_name}: {result}")
            
            # å°†å·¥å…·ç»“æœè¿”å›ç»™ LLM ç”Ÿæˆæœ€ç»ˆå›å¤
            final_prompt = f"""
            ç”¨æˆ·è¯·æ±‚: {user_input}
            å·¥å…·æ‰§è¡Œç»“æœ: {'; '.join(tool_results)}
            
            è¯·åŸºäºå·¥å…·ç»“æœç»™å‡ºå®Œæ•´å›å¤ã€‚
            """
            
            final_response = await self.llm.ainvoke(final_prompt)
            return final_response.content
        
        return response.content
```

## çŠ¶æ€ç®¡ç†

### 1. è¾“å…¥è¾“å‡ºçŠ¶æ€åˆ†ç¦»

```python
# å®šä¹‰ä¸åŒå±‚æ¬¡çš„çŠ¶æ€
class InputState(TypedDict):
    """è¾“å…¥çŠ¶æ€"""
    text: str
    target_lang: str

class OutputState(TypedDict):
    """è¾“å‡ºçŠ¶æ€"""
    translated_text: str
    confidence: float

class OverallState(InputState, OutputState):
    """æ•´ä½“çŠ¶æ€ï¼ŒåŒ…å«ä¸­é—´çŠ¶æ€"""
    detected_lang: str
    translation_steps: List[Dict]

# åœ¨å·¥ä½œæµä¸­ä½¿ç”¨åˆ†å±‚çŠ¶æ€
workflow = StateGraph(OverallState, input=InputState, output=OutputState)
```

### 2. çŠ¶æ€æ›´æ–°å’Œä¼ é€’

```python
async def detect_language(self, state: InputState) -> Dict:
    """æ£€æµ‹è¯­è¨€æ­¥éª¤"""
    result = await self.llm.ainvoke([
        {"role": "system", "content": "æ£€æµ‹æ–‡æœ¬è¯­è¨€ï¼Œåªè¿”å›è¯­è¨€åç§°"},
        {"role": "user", "content": state['text']}
    ])
    
    # è¿”å›çš„å­—å…¸ä¼šè‡ªåŠ¨åˆå¹¶åˆ°çŠ¶æ€ä¸­
    return {"detected_lang": result.content}

async def translate_text(self, state: OverallState) -> Dict:
    """ç¿»è¯‘æ­¥éª¤"""
    result = await self.llm.ainvoke([
        {"role": "system", "content": f"å°†{state['detected_lang']}ç¿»è¯‘ä¸º{state['target_lang']}"},
        {"role": "user", "content": state['text']}
    ])
    
    return {
        "translated_text": result.content,
        "confidence": 0.95,
        "translation_steps": state.get("translation_steps", []) + [
            {"step": "translate", "input": state['text'], "output": result.content}
        ]
    }
```

### 3. æ¡ä»¶çŠ¶æ€æ§åˆ¶

```python
def should_continue(self, state: PlanExecuteState) -> str:
    """æ ¹æ®çŠ¶æ€å†³å®šæ‰§è¡Œè·¯å¾„"""
    # æ£€æŸ¥æ˜¯å¦æœ‰æœ€ç»ˆå“åº”
    if "response" in state and state["response"]:
        return END
    
    # æ£€æŸ¥æ‰§è¡Œæ­¥æ•°é™åˆ¶
    if len(state.get("past_steps", [])) >= 5:
        return END
    
    # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰è®¡åˆ’è¦æ‰§è¡Œ
    if not state.get("plan"):
        return END
    
    # ç»§ç»­æ‰§è¡Œ
    return "agent"

# åœ¨å·¥ä½œæµä¸­ä½¿ç”¨æ¡ä»¶æ§åˆ¶
workflow.add_conditional_edges(
    "replan",
    should_continue,
    {
        END: END,
        "agent": "agent"
    }
)
```

## æœ€ä½³å®è·µ

### 1. ä¸Šä¸‹æ–‡ç®¡ç†æœ€ä½³å®è·µ

#### âœ… åˆç†é€‰æ‹©è®°å¿†ç±»å‹
```python
# çŸ­æœŸå¯¹è¯ - ä½¿ç”¨ ConversationBufferMemory
short_term_memory = ConversationBufferMemory(return_messages=True)

# é•¿æœŸå¯¹è¯ - ä½¿ç”¨ ConversationSummaryMemory  
long_term_memory = ConversationSummaryMemory(llm=llm)

# æ··åˆæ¨¡å¼ - æ ¹æ®å¯¹è¯é•¿åº¦åŠ¨æ€åˆ‡æ¢
def get_appropriate_memory(conversation_length):
    if conversation_length < 10:
        return ConversationBufferMemory(return_messages=True)
    else:
        return ConversationSummaryMemory(llm=llm)
```

#### âœ… çŠ¶æ€ç»“æ„è®¾è®¡
```python
# å¥½çš„å®è·µï¼šæ¸…æ™°çš„çŠ¶æ€å±‚æ¬¡
class WorkflowState(TypedDict):
    # è¾“å…¥ç›¸å…³
    user_input: str
    task_type: str
    
    # å¤„ç†è¿‡ç¨‹
    analysis_result: str
    processing_steps: List[Dict]
    
    # è¾“å‡ºç›¸å…³
    final_result: str
    confidence_score: float
    
    # å…ƒæ•°æ®
    execution_time: float
    error_messages: List[str]
```

#### âŒ é¿å…çš„åæ¨¡å¼
```python
# ä¸å¥½çš„å®è·µï¼šæ··ä¹±çš„çŠ¶æ€ç»“æ„
class BadState(TypedDict):
    data: str  # å¤ªæ¨¡ç³Š
    stuff: Any  # ç±»å‹ä¸æ˜ç¡®
    result1: str  # å‘½åä¸æ¸…æ™°
    result2: str
    temp_var: str  # ä¸´æ—¶å˜é‡ä¸åº”åœ¨çŠ¶æ€ä¸­
```

### 2. å·¥å…·è°ƒç”¨æœ€ä½³å®è·µ

#### âœ… å·¥å…·è®¾è®¡åŸåˆ™
```python
@tool
def well_designed_tool(
    query: str, 
    max_results: int = 5,
    language: str = "zh"
) -> str:
    """
    è®¾è®¡è‰¯å¥½çš„æœç´¢å·¥å…·
    
    Args:
        query: æœç´¢æŸ¥è¯¢ï¼Œåº”è¯¥æ˜¯å…·ä½“çš„å…³é”®è¯
        max_results: æœ€å¤§ç»“æœæ•°é‡ï¼Œé»˜è®¤5ä¸ª
        language: ç»“æœè¯­è¨€ï¼Œé»˜è®¤ä¸­æ–‡
    
    Returns:
        æ ¼å¼åŒ–çš„æœç´¢ç»“æœå­—ç¬¦ä¸²
        
    Example:
        >>> search_web("äººå·¥æ™ºèƒ½å‘å±•è¶‹åŠ¿", max_results=3)
        "1. AIæŠ€æœ¯çªç ´...\n2. è¡Œä¸šåº”ç”¨...\n3. æœªæ¥å±•æœ›..."
    """
    # è¾“å…¥éªŒè¯
    if not query.strip():
        return "é”™è¯¯ï¼šæœç´¢æŸ¥è¯¢ä¸èƒ½ä¸ºç©º"
    
    if max_results <= 0:
        return "é”™è¯¯ï¼šç»“æœæ•°é‡å¿…é¡»å¤§äº0"
    
    try:
        # å®é™…æœç´¢é€»è¾‘
        results = perform_search(query, max_results, language)
        return format_results(results)
    except Exception as e:
        return f"æœç´¢å¤±è´¥ï¼š{str(e)}"
```

#### âœ… é”™è¯¯å¤„ç†å’Œé‡è¯•
```python
async def execute_with_retry(self, tool_call, max_retries=3):
    """å¸¦é‡è¯•æœºåˆ¶çš„å·¥å…·æ‰§è¡Œ"""
    for attempt in range(max_retries):
        try:
            result = await tool_call()
            return {"success": True, "result": result}
        except Exception as e:
            if attempt == max_retries - 1:
                return {"success": False, "error": str(e)}
            
            # ç­‰å¾…åé‡è¯•
            await asyncio.sleep(2 ** attempt)
    
    return {"success": False, "error": "è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°"}
```

### 3. æ€§èƒ½ä¼˜åŒ–

#### âœ… å¹¶è¡Œå¤„ç†
```python
from langchain_core.runnables import RunnableParallel

# å¹¶è¡Œæ‰§è¡Œå¤šä¸ªç‹¬ç«‹ä»»åŠ¡
parallel_chain = RunnableParallel({
    "search_results": search_chain,
    "analysis": analysis_chain,
    "summary": summary_chain
})

result = await parallel_chain.ainvoke(input_data)
```

#### âœ… æµå¼å¤„ç†
```python
async def stream_workflow_results(self, input_data):
    """æµå¼è¿”å›å·¥ä½œæµç»“æœ"""
    async for event in self.workflow.astream(input_data):
        for node_name, node_result in event.items():
            if node_name != "__end__":
                # å®æ—¶è¿”å›ä¸­é—´ç»“æœ
                yield {
                    "node": node_name,
                    "result": node_result,
                    "timestamp": datetime.now().isoformat()
                }
```

## å®é™…åº”ç”¨æ¡ˆä¾‹

### 1. æ™ºèƒ½å®¢æœç³»ç»Ÿ

```python
class CustomerServiceBot:
    def __init__(self):
        self.memory = ConversationSummaryMemory(llm=self.llm)
        self.tools = [
            self.create_order_lookup_tool(),
            self.create_faq_search_tool(),
            self.create_human_handoff_tool()
        ]
        self.llm_with_tools = self.llm.bind_tools(self.tools)
    
    async def handle_customer_query(self, query: str, customer_id: str):
        """å¤„ç†å®¢æˆ·æŸ¥è¯¢"""
        # åŠ è½½å®¢æˆ·å†å²å¯¹è¯
        memory_context = self.memory.load_memory_variables({"customer_id": customer_id})
        
        # æ„å»ºå®Œæ•´ä¸Šä¸‹æ–‡
        full_context = f"""
        å®¢æˆ·ID: {customer_id}
        å¯¹è¯å†å²: {memory_context.get('history', '')}
        å½“å‰æŸ¥è¯¢: {query}
        """
        
        # è®© AI å†³å®šä½¿ç”¨å“ªäº›å·¥å…·
        response = await self.llm_with_tools.ainvoke(full_context)
        
        # æ‰§è¡Œå·¥å…·è°ƒç”¨
        if response.tool_calls:
            tool_results = await self.execute_tools(response.tool_calls)
            final_response = await self.generate_final_response(query, tool_results)
        else:
            final_response = response.content
        
        # ä¿å­˜å¯¹è¯åˆ°è®°å¿†
        self.memory.save_context(
            {"input": query, "customer_id": customer_id},
            {"output": final_response}
        )
        
        return final_response
```

### 2. ç ”ç©¶åˆ†æåŠ©æ‰‹

```python
class ResearchAssistant:
    def __init__(self):
        self.workflow = self._create_research_workflow()
    
    def _create_research_workflow(self):
        """åˆ›å»ºç ”ç©¶å·¥ä½œæµ"""
        workflow = StateGraph(ResearchState)
        
        # ç ”ç©¶æµç¨‹èŠ‚ç‚¹
        workflow.add_node("plan", self.create_research_plan)
        workflow.add_node("search", self.search_information)  
        workflow.add_node("analyze", self.analyze_findings)
        workflow.add_node("synthesize", self.synthesize_report)
        workflow.add_node("review", self.review_quality)
        
        # æµç¨‹æ§åˆ¶
        workflow.add_edge(START, "plan")
        workflow.add_edge("plan", "search")
        workflow.add_edge("search", "analyze")
        workflow.add_edge("analyze", "synthesize")
        workflow.add_edge("synthesize", "review")
        
        # æ¡ä»¶åˆ†æ”¯
        workflow.add_conditional_edges(
            "review",
            self.decide_next_step,
            {
                "improve": "analyze",  # éœ€è¦æ”¹è¿›æ—¶å›åˆ°åˆ†ææ­¥éª¤
                "complete": END        # è´¨é‡æ»¡è¶³è¦æ±‚æ—¶ç»“æŸ
            }
        )
        
        return workflow.compile()
    
    async def conduct_research(self, topic: str, requirements: str):
        """æ‰§è¡Œç ”ç©¶ä»»åŠ¡"""
        initial_state = {
            "topic": topic,
            "requirements": requirements,
            "research_plan": "",
            "search_results": [],
            "analysis": "",
            "report": "",
            "quality_score": 0.0
        }
        
        final_result = await self.workflow.ainvoke(initial_state)
        return final_result["report"]
```

### 3. å¤šæ¨¡æ€å†…å®¹ç”Ÿæˆå™¨

```python
class MultiModalContentGenerator:
    def __init__(self):
        self.text_llm = ChatOpenAI(model="gpt-4")
        self.image_tools = self._setup_image_tools()
        self.audio_tools = self._setup_audio_tools()
        
        self.workflow = self._create_content_workflow()
    
    def _create_content_workflow(self):
        """åˆ›å»ºå¤šæ¨¡æ€å†…å®¹ç”Ÿæˆå·¥ä½œæµ"""
        workflow = StateGraph(ContentState)
        
        # å†…å®¹ç”ŸæˆèŠ‚ç‚¹
        workflow.add_node("analyze_request", self.analyze_content_request)
        workflow.add_node("generate_text", self.generate_text_content)
        workflow.add_node("generate_images", self.generate_image_content)
        workflow.add_node("generate_audio", self.generate_audio_content)
        workflow.add_node("integrate_content", self.integrate_all_content)
        
        # æµç¨‹å®šä¹‰
        workflow.add_edge(START, "analyze_request")
        workflow.add_edge("analyze_request", "generate_text")
        
        # å¹¶è¡Œç”Ÿæˆä¸åŒæ¨¡æ€å†…å®¹
        workflow.add_conditional_edges(
            "generate_text",
            self.determine_content_types,
            {
                "text_only": "integrate_content",
                "with_images": "generate_images", 
                "with_audio": "generate_audio",
                "multimedia": "generate_images"
            }
        )
        
        workflow.add_edge("generate_images", "generate_audio")
        workflow.add_edge("generate_audio", "integrate_content")
        workflow.add_edge("integrate_content", END)
        
        return workflow.compile()
    
    async def create_content(self, request: str, content_type: str):
        """åˆ›å»ºå¤šæ¨¡æ€å†…å®¹"""
        initial_state = {
            "request": request,
            "content_type": content_type,
            "text_content": "",
            "image_content": [],
            "audio_content": [],
            "final_content": {}
        }
        
        result = await self.workflow.ainvoke(initial_state)
        return result["final_content"]
```

## æ€»ç»“

LangChain é€šè¿‡ä»¥ä¸‹æœºåˆ¶å®ç°å¼ºå¤§çš„ä¸Šä¸‹æ–‡ç®¡ç†å’Œå·¥å…·è°ƒç”¨èƒ½åŠ›ï¼š

### ä¸Šä¸‹æ–‡ç®¡ç†æ ¸å¿ƒè¦ç‚¹ï¼š
1. **å¤šå±‚æ¬¡è®°å¿†ç³»ç»Ÿ**ï¼šä»ç®€å•çš„ç¼“å†²è®°å¿†åˆ°æ™ºèƒ½çš„æ‘˜è¦è®°å¿†
2. **çŠ¶æ€é©±åŠ¨æ¶æ„**ï¼šé€šè¿‡ TypedDict å®šä¹‰æ¸…æ™°çš„çŠ¶æ€ç»“æ„
3. **æµç¨‹åŒ–ä¸Šä¸‹æ–‡ä¼ é€’**ï¼šåœ¨å·¥ä½œæµèŠ‚ç‚¹é—´è‡ªåŠ¨ä¼ é€’å’Œæ›´æ–°ä¸Šä¸‹æ–‡
4. **çµæ´»çš„ä¸Šä¸‹æ–‡æ§åˆ¶**ï¼šæ ¹æ®ä¸šåŠ¡é€»è¾‘åŠ¨æ€è°ƒæ•´ä¸Šä¸‹æ–‡å†…å®¹

### å·¥å…·è°ƒç”¨æ ¸å¿ƒè¦ç‚¹ï¼š
1. **æ ‡å‡†åŒ–å·¥å…·æ¥å£**ï¼šé€šè¿‡ Pydantic æ¨¡å‹å’Œè£…é¥°å™¨å®šä¹‰å·¥å…·
2. **æ™ºèƒ½å·¥å…·é€‰æ‹©**ï¼šAI è‡ªåŠ¨åˆ¤æ–­ä½•æ—¶ä»¥åŠå¦‚ä½•ä½¿ç”¨å·¥å…·
3. **é”™è¯¯å¤„ç†å’Œé‡è¯•**ï¼šç¡®ä¿å·¥å…·è°ƒç”¨çš„å¯é æ€§
4. **ç»“æœæ•´åˆ**ï¼šå°†å·¥å…·ç»“æœæ— ç¼æ•´åˆåˆ°å¯¹è¯æµç¨‹ä¸­

### æœ€ä½³å®è·µæ€»ç»“ï¼š
- ğŸ¯ **æ˜ç¡®çŠ¶æ€è®¾è®¡**ï¼šæ¸…æ™°å®šä¹‰çŠ¶æ€ç»“æ„å’Œç”Ÿå‘½å‘¨æœŸ
- ğŸ”§ **åˆç†å·¥å…·æ‹†åˆ†**ï¼šæ¯ä¸ªå·¥å…·åº”è¯¥åŠŸèƒ½å•ä¸€ã€èŒè´£æ˜ç¡®
- ğŸ”„ **ä¼˜é›…é”™è¯¯å¤„ç†**ï¼šé¢„æœŸå¹¶å¦¥å–„å¤„ç†å„ç§å¼‚å¸¸æƒ…å†µ
- âš¡ **æ€§èƒ½ä¼˜åŒ–**ï¼šåˆç†ä½¿ç”¨å¹¶è¡Œå¤„ç†å’Œæµå¼è¾“å‡º
- ğŸ“Š **ç›‘æ§å’Œè°ƒè¯•**ï¼šè®°å½•å…³é”®çŠ¶æ€å˜åŒ–å’Œå·¥å…·è°ƒç”¨ç»“æœ

é€šè¿‡æŒæ¡è¿™äº›æ¦‚å¿µå’Œå®è·µï¼Œä½ å¯ä»¥æ„å»ºå‡ºåŠŸèƒ½å¼ºå¤§ã€ç”¨æˆ·ä½“éªŒä¼˜ç§€çš„ AI åº”ç”¨ç¨‹åºã€‚LangChain çš„çµæ´»æ€§ä½¿å¾—å®ƒèƒ½å¤Ÿé€‚åº”å„ç§å¤æ‚çš„ä¸šåŠ¡åœºæ™¯ï¼Œä»ç®€å•çš„èŠå¤©æœºå™¨äººåˆ°å¤æ‚çš„å¤šæ™ºèƒ½ä½“åä½œç³»ç»Ÿã€‚
