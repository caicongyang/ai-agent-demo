# LangGraph Human-in-the-Loop å®Œæ•´æŒ‡å—

## ç›®å½•
1. [æ¦‚è¿°](#æ¦‚è¿°)
2. [åŸºç¡€æ¦‚å¿µ](#åŸºç¡€æ¦‚å¿µ)
3. [åŸºæœ¬äººæœºäº¤äº’æ¨¡å¼](#åŸºæœ¬äººæœºäº¤äº’æ¨¡å¼)
4. [æµå¼è¾“å‡ºä¸ç”¨æˆ·æ‰“æ–­](#æµå¼è¾“å‡ºä¸ç”¨æˆ·æ‰“æ–­)
5. [é«˜çº§æµå¼äººæœºäº¤äº’](#é«˜çº§æµå¼äººæœºäº¤äº’)
6. [ä¸Šä¸‹æ–‡æ›´æ–°æœºåˆ¶](#ä¸Šä¸‹æ–‡æ›´æ–°æœºåˆ¶)
7. [å®ç°åŸç†è¯¦è§£](#å®ç°åŸç†è¯¦è§£)
8. [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)
9. [å®é™…åº”ç”¨åœºæ™¯](#å®é™…åº”ç”¨åœºæ™¯)

## æ¦‚è¿°

Human-in-the-Loopï¼ˆäººæœºäº¤äº’å¾ªç¯ï¼‰æ˜¯AIç³»ç»Ÿè®¾è®¡ä¸­çš„é‡è¦æ¨¡å¼ï¼Œå…è®¸äººå·¥åœ¨AIå¤„ç†è¿‡ç¨‹ä¸­ä»‹å…¥ã€æŒ‡å¯¼å’Œä¿®æ­£ã€‚LangGraph æä¾›äº†å¼ºå¤§çš„äººæœºäº¤äº’åŠŸèƒ½ï¼Œæ”¯æŒå¤šç§äº¤äº’æ¨¡å¼ï¼Œä»ç®€å•çš„äººå·¥å®¡æ ¸åˆ°å¤æ‚çš„å®æ—¶æ‰“æ–­å’Œä¸Šä¸‹æ–‡æ›´æ–°ã€‚

### æ ¸å¿ƒä»·å€¼
- **æé«˜å‡†ç¡®æ€§**ï¼šäººå·¥ä»‹å…¥å¯ä»¥çº æ­£AIçš„é”™è¯¯åˆ¤æ–­
- **å¢å¼ºå¯æ§æ€§**ï¼šç”¨æˆ·å¯ä»¥å®æ—¶è°ƒæ•´AIçš„è¡Œä¸ºæ–¹å‘
- **æ”¹å–„ç”¨æˆ·ä½“éªŒ**ï¼šæ”¯æŒæ›´è‡ªç„¶çš„äººæœºå¯¹è¯äº¤äº’
- **ç¡®ä¿å®‰å…¨æ€§**ï¼šåœ¨å…³é”®å†³ç­–ç‚¹å¼•å…¥äººå·¥å®¡æ ¸

## åŸºç¡€æ¦‚å¿µ

### 1. Human-in-the-Loop çš„åŸºæœ¬ç±»å‹

#### ğŸ”„ å®¡æ ¸æ¨¡å¼ï¼ˆReview Modeï¼‰
AIå®Œæˆä»»åŠ¡åï¼Œç­‰å¾…äººå·¥å®¡æ ¸å’Œåé¦ˆ

#### âš¡ å®æ—¶æ‰“æ–­æ¨¡å¼ï¼ˆReal-time Interrupt Modeï¼‰
AIæ‰§è¡Œè¿‡ç¨‹ä¸­ï¼Œç”¨æˆ·å¯ä»¥å®æ—¶æ‰“æ–­å¹¶æä¾›æ–°çš„æŒ‡å¯¼

#### ğŸ¯ åä½œæ¨¡å¼ï¼ˆCollaboration Modeï¼‰
AIå’Œäººå·¥åœ¨ä»»åŠ¡æ‰§è¡Œè¿‡ç¨‹ä¸­å¯†åˆ‡åä½œï¼Œäº’ç›¸è¡¥å……

### 2. å…³é”®ç»„ä»¶

```python
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, START, END
from langgraph.types import Command, interrupt

class HumanInLoopState(TypedDict):
    """äººæœºäº¤äº’çŠ¶æ€å®šä¹‰"""
    task: str                    # ç”¨æˆ·ä»»åŠ¡
    ai_response: str            # AIå›å¤
    human_input_needed: bool    # æ˜¯å¦éœ€è¦äººå·¥è¾“å…¥
    human_feedback: str         # äººå·¥åé¦ˆ
    final_output: str          # æœ€ç»ˆè¾“å‡º
    interrupt_requested: bool   # æ˜¯å¦è¯·æ±‚ä¸­æ–­
    context_updated: bool      # ä¸Šä¸‹æ–‡æ˜¯å¦å·²æ›´æ–°
```

## åŸºæœ¬äººæœºäº¤äº’æ¨¡å¼

### 1. ç®€å•å®¡æ ¸æ¨¡å¼å®ç°

è¿™æ˜¯æœ€åŸºç¡€çš„äººæœºäº¤äº’æ¨¡å¼ï¼ŒAIå®Œæˆåˆ†æåç­‰å¾…äººå·¥åé¦ˆï¼š

```python
class BasicHumanInLoopDemo:
    def __init__(self):
        self.llm = ChatOpenAI(
            model="deepseek-chat",
            openai_api_key=os.getenv("LLM_API_KEY"),
            base_url=os.getenv("LLM_BASE_URL")
        )
        
        # AIåˆ†ææç¤º
        self.ai_prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œè´Ÿè´£åˆ†æç”¨æˆ·çš„è¯·æ±‚ã€‚
            å½“ç”¨æˆ·è¯·æ±‚ä¸“å®¶å»ºè®®æˆ–éœ€è¦äººå·¥ååŠ©æ—¶ï¼Œä½ åº”è¯¥è¯†åˆ«å‡ºè¿™ç§éœ€æ±‚ã€‚
            å¦‚æœéœ€è¦äººå·¥ååŠ©ï¼Œè¯·åœ¨å›å¤ä¸­åŒ…å«"éœ€è¦äººå·¥ååŠ©"è¿™ä¸ªçŸ­è¯­ã€‚"""),
            ("user", "{task}")
        ])
        
        # æœ€ç»ˆå›å¤æç¤º
        self.final_prompt = ChatPromptTemplate.from_messages([
            ("system", """åŸºäºäººå·¥ä¸“å®¶çš„åé¦ˆç»™ç”¨æˆ·æœ€ç»ˆå›å¤ã€‚
            è¯·æ•´åˆä¸“å®¶çš„å»ºè®®ï¼Œç»™å‡ºå®Œæ•´ã€æœ‰ç”¨çš„å›ç­”ã€‚"""),
            ("user", "ç”¨æˆ·ä»»åŠ¡ï¼š{task}\n\nä¸“å®¶åé¦ˆï¼š{human_feedback}")
        ])
    
    async def ai_analysis_step(self, state: HumanInLoopState) -> Dict:
        """AIåˆ†ææ­¥éª¤"""
        try:
            analysis_chain = self.ai_prompt | self.llm
            result = await analysis_chain.ainvoke({"task": state["task"]})
            
            ai_response = result.content
            # æ£€æŸ¥æ˜¯å¦éœ€è¦äººå·¥ååŠ©
            need_human = "éœ€è¦äººå·¥ååŠ©" in ai_response or "ä¸“å®¶å»ºè®®" in state["task"]
            
            return {
                "ai_response": ai_response,
                "human_input_needed": need_human
            }
        except Exception as e:
            return {
                "ai_response": f"åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}",
                "human_input_needed": False
            }
    
    def human_input_step(self, state: HumanInLoopState) -> Dict:
        """äººå·¥è¾“å…¥æ­¥éª¤"""
        print(f"ğŸ¤– AIåˆ†æç»“æœ: {state['ai_response']}")
        print("ğŸ¤– ç³»ç»Ÿæç¤º: AIä»£ç†è¯·æ±‚äººå·¥ååŠ©")
        print(f"ğŸ“ ç”¨æˆ·ä»»åŠ¡: {state['task']}")
        
        # æ¨¡æ‹Ÿä¸“å®¶åé¦ˆï¼ˆå®é™…åº”ç”¨ä¸­ä¼šç­‰å¾…çœŸå®äººå·¥è¾“å…¥ï¼‰
        human_feedback = self._get_expert_feedback(state["task"])
        print(f"ğŸ‘¨â€ğŸ’¼ ä¸“å®¶åé¦ˆ: {human_feedback}")
        
        return {"human_feedback": human_feedback}
    
    async def final_response_step(self, state: HumanInLoopState) -> Dict:
        """æœ€ç»ˆå›å¤æ­¥éª¤"""
        try:
            final_chain = self.final_prompt | self.llm
            result = await final_chain.ainvoke({
                "task": state["task"],
                "human_feedback": state["human_feedback"]
            })
            
            return {"final_output": result.content}
        except Exception as e:
            return {"final_output": f"ç”Ÿæˆæœ€ç»ˆå›å¤æ—¶å‡ºç°é”™è¯¯: {str(e)}"}
    
    def decide_next_step(self, state: HumanInLoopState) -> str:
        """å†³å®šä¸‹ä¸€æ­¥æ“ä½œ"""
        if state.get("human_input_needed", False) and not state.get("human_feedback"):
            return "human_input"
        else:
            return "final_response"
    
    def _create_workflow(self):
        """åˆ›å»ºå·¥ä½œæµ"""
        workflow = StateGraph(HumanInLoopState)
        
        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("ai_analysis", self.ai_analysis_step)
        workflow.add_node("human_input", self.human_input_step)
        workflow.add_node("final_response", self.final_response_step)
        
        # å®šä¹‰æµç¨‹
        workflow.add_edge(START, "ai_analysis")
        workflow.add_conditional_edges(
            "ai_analysis",
            self.decide_next_step,
            {
                "human_input": "human_input",
                "final_response": "final_response"
            }
        )
        workflow.add_edge("human_input", "final_response")
        workflow.add_edge("final_response", END)
        
        return workflow.compile()
```

### 2. å·¥ä½œæµç¨‹å›¾

```mermaid
graph TD
    A[å¼€å§‹] --> B[AIåˆ†æ]
    B --> C{éœ€è¦äººå·¥ååŠ©?}
    C -->|æ˜¯| D[ç­‰å¾…äººå·¥è¾“å…¥]
    C -->|å¦| E[ç”Ÿæˆæœ€ç»ˆå›å¤]
    D --> E
    E --> F[ç»“æŸ]
```

## æµå¼è¾“å‡ºä¸ç”¨æˆ·æ‰“æ–­

### 1. åŸºç¡€æµå¼æ‰“æ–­å®ç°

æµå¼è¾“å‡ºå…è®¸ç”¨æˆ·çœ‹åˆ°AIçš„å®æ—¶ç”Ÿæˆè¿‡ç¨‹ï¼Œå¹¶æ”¯æŒä¸­é€”æ‰“æ–­ï¼š

```python
class StreamingInterruptDemo:
    def __init__(self):
        self.llm = ChatOpenAI(
            model="deepseek-chat",
            streaming=True  # å¯ç”¨æµå¼è¾“å‡º
        )
        
        # ç”¨æˆ·è¾“å…¥é˜Ÿåˆ—å’Œä¸­æ–­äº‹ä»¶
        self.user_input_queue = Queue()
        self.interrupt_event = threading.Event()
    
    async def streaming_response_node(self, state: StreamingInterruptState) -> Dict:
        """æµå¼å“åº”èŠ‚ç‚¹"""
        try:
            streaming_content = ""
            interrupt_detected = False
            user_interrupt_message = ""
            
            print(f"ğŸ¤– AIæ­£åœ¨å›å¤: ", end="", flush=True)
            print(f"ğŸ’¡ æç¤º: æ‚¨å¯ä»¥ç›´æ¥è¾“å…¥æ–°æ¶ˆæ¯å¹¶æŒ‰å›è½¦æ¥æ‰“æ–­AIå›å¤")
            
            # é‡ç½®ä¸­æ–­çŠ¶æ€
            self.interrupt_event.clear()
            self._clear_input_queue()
            
            # å¯åŠ¨ç”¨æˆ·è¾“å…¥ç›‘å¬çº¿ç¨‹
            input_thread = threading.Thread(
                target=self._listen_for_user_input, 
                daemon=True
            )
            input_thread.start()
            
            # æµå¼ç”Ÿæˆå“åº”
            async for chunk in self.response_chain.astream({
                "conversation_history": self._format_history(state),
                "user_input": state["user_input"]
            }):
                if hasattr(chunk, 'content') and chunk.content:
                    content = chunk.content
                    streaming_content += content
                    print(content, end="", flush=True)
                    
                    # æ£€æŸ¥ç”¨æˆ·æ‰“æ–­
                    if self.interrupt_event.is_set():
                        try:
                            user_interrupt_message = self.user_input_queue.get_nowait()
                            interrupt_detected = True
                            break
                        except Empty:
                            pass
                    
                    # æ§åˆ¶è¾“å‡ºé€Ÿåº¦
                    await asyncio.sleep(0.1)
            
            print()  # æ¢è¡Œ
            
            if interrupt_detected:
                print("ğŸ”„ æ£€æµ‹åˆ°ç”¨æˆ·æ‰“æ–­ï¼Œæ­£åœ¨å¤„ç†...")
                return {
                    "ai_response": streaming_content,
                    "streaming_content": streaming_content,
                    "interrupt_requested": True,
                    "user_interrupt": user_interrupt_message,
                    "context_updated": False
                }
            else:
                return {
                    "ai_response": streaming_content,
                    "interrupt_requested": False,
                    "context_updated": False
                }
                
        except Exception as e:
            return {
                "ai_response": f"ç”Ÿæˆå›å¤æ—¶å‡ºç°é”™è¯¯: {str(e)}",
                "interrupt_requested": False,
                "context_updated": False
            }
    
    def _listen_for_user_input(self):
        """ç›‘å¬ç”¨æˆ·è¾“å…¥çš„åå°çº¿ç¨‹"""
        import select
        import sys
        
        try:
            input_buffer = ""
            while not self.interrupt_event.is_set():
                if hasattr(select, 'select'):
                    ready, _, _ = select.select([sys.stdin], [], [], 0.1)
                    if ready:
                        char = sys.stdin.read(1)
                        if char == '\r' or char == '\n':  # å›è½¦é”®
                            if input_buffer.strip():
                                self.user_input_queue.put(input_buffer.strip())
                                self.interrupt_event.set()
                                print(f"\nâš ï¸ æ£€æµ‹åˆ°ç”¨æˆ·æ‰“æ–­: {input_buffer.strip()}")
                                break
                        elif char.isprintable():
                            input_buffer += char
                            print(char, end='', flush=True)
                else:
                    time.sleep(0.1)
        except Exception as e:
            logger.debug(f"è¾“å…¥ç›‘å¬å¼‚å¸¸: {e}")
```

### 2. æ‰“æ–­å¤„ç†æµç¨‹

```mermaid
sequenceDiagram
    participant U as ç”¨æˆ·
    participant AI as AIç³»ç»Ÿ
    participant T as ç›‘å¬çº¿ç¨‹
    participant W as å·¥ä½œæµ
    
    U->>AI: å‘é€é—®é¢˜
    AI->>W: å¼€å§‹æµå¼ç”Ÿæˆ
    AI->>T: å¯åŠ¨è¾“å…¥ç›‘å¬
    
    loop æµå¼è¾“å‡º
        AI->>U: è¾“å‡ºå†…å®¹ç‰‡æ®µ
        T->>T: æ£€æŸ¥ç”¨æˆ·è¾“å…¥
        alt ç”¨æˆ·æ‰“æ–­
            U->>T: è¾“å…¥æ‰“æ–­æ¶ˆæ¯
            T->>AI: è®¾ç½®ä¸­æ–­æ ‡å¿—
            AI->>W: åœæ­¢ç”Ÿæˆï¼Œè¿”å›ä¸­æ–­çŠ¶æ€
        end
    end
    
    alt æœ‰ä¸­æ–­
        W->>AI: å¤„ç†ä¸­æ–­ï¼Œæ›´æ–°ä¸Šä¸‹æ–‡
        AI->>U: è¾“å‡ºæ›´æ–°åçš„å›å¤
    else æ­£å¸¸å®Œæˆ
        AI->>U: å®Œæˆå›å¤
    end
```

## é«˜çº§æµå¼äººæœºäº¤äº’

### 1. è·¨å¹³å°è¾“å…¥ç›‘å¬

é«˜çº§ç‰ˆæœ¬æ”¯æŒWindowså’ŒUnix/Linuxç³»ç»Ÿçš„é”®ç›˜è¾“å…¥ç›‘å¬ï¼š

```python
class InterruptHandler:
    """ä¸­æ–­å¤„ç†å™¨ - è·¨å¹³å°è¾“å…¥ç›‘å¬"""
    
    def __init__(self):
        self.input_queue = Queue()
        self.interrupt_event = threading.Event()
        self.listening = False
        self.input_thread = None
        self._stop_listening = threading.Event()
    
    def start_listening(self):
        """å¼€å§‹ç›‘å¬ç”¨æˆ·è¾“å…¥"""
        if self.listening:
            return
        
        self.listening = True
        self._stop_listening.clear()
        self.interrupt_event.clear()
        
        # æ¸…ç©ºè¾“å…¥é˜Ÿåˆ—
        self._clear_queue()
        
        # å¯åŠ¨ç›‘å¬çº¿ç¨‹
        self.input_thread = threading.Thread(
            target=self._listen_loop, 
            daemon=True
        )
        self.input_thread.start()
    
    def _listen_loop(self):
        """ç›‘å¬å¾ªç¯ - è·¨å¹³å°å®ç°"""
        print("ğŸ’¡ æç¤º: åœ¨AIå›å¤è¿‡ç¨‹ä¸­è¾“å…¥æ¶ˆæ¯å¯ä»¥æ‰“æ–­AI (æŒ‰Enterç¡®è®¤)")
        
        while not self._stop_listening.is_set():
            try:
                import sys
                import platform
                
                if platform.system() == "Windows":
                    # Windowsç³»ç»Ÿå®ç°
                    self._windows_input_handler()
                else:
                    # Unix/Linuxç³»ç»Ÿå®ç°
                    self._unix_input_handler()
                    
            except Exception as e:
                logger.debug(f"ç›‘å¬è¾“å…¥æ—¶å‡ºé”™: {e}")
                time.sleep(0.1)
    
    def _windows_input_handler(self):
        """Windowsç³»ç»Ÿçš„è¾“å…¥å¤„ç†"""
        try:
            import msvcrt
            if msvcrt.kbhit():
                chars = []
                while msvcrt.kbhit():
                    char = msvcrt.getch().decode('utf-8', errors='ignore')
                    if char == '\r':  # å›è½¦é”®
                        break
                    elif char == '\b':  # é€€æ ¼é”®
                        if chars:
                            chars.pop()
                            print('\b \b', end='', flush=True)
                    elif ord(char) >= 32:  # å¯æ‰“å°å­—ç¬¦
                        chars.append(char)
                        print(char, end='', flush=True)
                
                user_input = ''.join(chars).strip()
                if user_input:
                    self.input_queue.put(user_input)
                    self.interrupt_event.set()
                    print()  # æ¢è¡Œ
            
            time.sleep(0.1)
        except ImportError:
            time.sleep(0.1)
    
    def _unix_input_handler(self):
        """Unix/Linuxç³»ç»Ÿçš„è¾“å…¥å¤„ç†"""
        import select
        import sys
        
        if hasattr(select, 'select'):
            ready, _, _ = select.select([sys.stdin], [], [], 0.1)
            if ready and not self._stop_listening.is_set():
                user_input = sys.stdin.readline().strip()
                if user_input:
                    self.input_queue.put(user_input)
                    self.interrupt_event.set()
        else:
            time.sleep(0.1)
```

### 2. ä¼šè¯ç®¡ç†

é«˜çº§ç‰ˆæœ¬æ”¯æŒå¤šä¼šè¯ç®¡ç†å’ŒçŠ¶æ€æŒä¹…åŒ–ï¼š

```python
class AdvancedStreamingInterruptDemo:
    def __init__(self):
        # ... å…¶ä»–åˆå§‹åŒ–ä»£ç  ...
        
        # æ£€æŸ¥ç‚¹ä¿å­˜å™¨
        self.checkpointer = MemorySaver()
        self.workflow = self._create_workflow()
        
        # ä¸­æ–­å¤„ç†å™¨
        self.interrupt_handler = InterruptHandler()
        
        # ä¼šè¯ç®¡ç†
        self.sessions = {}
    
    def create_session(self, session_id: Optional[str] = None) -> str:
        """åˆ›å»ºæ–°çš„å¯¹è¯ä¼šè¯"""
        if not session_id:
            session_id = f"session_{int(time.time())}"
        
        self.sessions[session_id] = {
            "created_at": datetime.now().isoformat(),
            "message_count": 0,
            "last_activity": datetime.now().isoformat()
        }
        
        return session_id
    
    async def chat(self, user_input: str, session_id: str) -> Dict[str, Any]:
        """è¿›è¡Œå¯¹è¯"""
        # æ›´æ–°ä¼šè¯ä¿¡æ¯
        if session_id in self.sessions:
            self.sessions[session_id]["message_count"] += 1
            self.sessions[session_id]["last_activity"] = datetime.now().isoformat()
        
        config = {"configurable": {"thread_id": session_id}}
        
        # è·å–å¯¹è¯å†å²
        try:
            current_state = self.workflow.get_state(config)
            conversation_history = current_state.values.get("conversation_history", []) if current_state.values else []
        except:
            conversation_history = []
        
        # åˆ›å»ºåˆå§‹çŠ¶æ€
        inputs = AdvancedStreamingState(
            user_input=user_input,
            ai_response="",
            conversation_history=conversation_history,
            interrupt_requested=False,
            user_interrupt="",
            streaming_content="",
            context_updated=False,
            session_id=session_id,
            timestamp=datetime.now().isoformat(),
            metadata={}
        )
        
        # è¿è¡Œå·¥ä½œæµ
        result_steps = []
        async for event in self.workflow.astream(inputs, config):
            for k, v in event.items():
                if k != "__end__":
                    result_steps.append({k: v})
                    
                    # æ£€æŸ¥ä¸­æ–­
                    if k == "streaming_response" and v.get("interrupt_requested"):
                        return {
                            "status": "interrupted",
                            "session_id": session_id,
                            "interrupt_data": {
                                "original_input": user_input,
                                "streaming_content": v.get("streaming_content", ""),
                                "user_interrupt": v.get("user_interrupt", "")
                            },
                            "steps": result_steps
                        }
        
        return {
            "status": "completed",
            "session_id": session_id,
            "steps": result_steps
        }
```

## ä¸Šä¸‹æ–‡æ›´æ–°æœºåˆ¶

### 1. åŸºäºæ‰“æ–­çš„ä¸Šä¸‹æ–‡æ›´æ–°

å½“ç”¨æˆ·æ‰“æ–­AIæ—¶ï¼Œç³»ç»Ÿéœ€è¦ç†è§£ç”¨æˆ·çš„æ„å›¾å¹¶ç›¸åº”æ›´æ–°å›å¤ï¼š

```python
async def context_update_node(self, state: AdvancedStreamingState) -> Dict:
    """ä¸Šä¸‹æ–‡æ›´æ–°èŠ‚ç‚¹"""
    try:
        # æ„å»ºä¸Šä¸‹æ–‡æ›´æ–°æç¤º
        update_prompt = ChatPromptTemplate.from_messages([
            ("system", """ç”¨æˆ·åœ¨ä½ å›å¤è¿‡ç¨‹ä¸­æ‰“æ–­äº†ä½ ã€‚è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯é‡æ–°ç»„ç»‡å›å¤ï¼š

åŸå§‹é—®é¢˜ï¼š{original_input}
å·²è¾“å‡ºå†…å®¹ï¼š{streaming_content}
ç”¨æˆ·æ‰“æ–­ï¼š{user_interrupt}
å¯¹è¯å†å²ï¼š{conversation_history}

è¯·æ ¹æ®ç”¨æˆ·çš„æ‰“æ–­è¦æ±‚é‡æ–°ç»„ç»‡å›å¤ï¼š
1. å¦‚æœç”¨æˆ·è¯´"ç®€å•ç‚¹"ï¼Œè¯·ç”¨æ›´ç®€æ´çš„æ–¹å¼å›ç­”åŸå§‹é—®é¢˜
2. å¦‚æœç”¨æˆ·è¯´"è¯¦ç»†ç‚¹"ï¼Œè¯·æä¾›æ›´è¯¦ç»†çš„ä¿¡æ¯
3. å¦‚æœç”¨æˆ·æå‡ºäº†æ–°é—®é¢˜ï¼Œè¯·å›ç­”æ–°é—®é¢˜
4. å¦‚æœç”¨æˆ·è¦æ±‚ä¿®æ­£æˆ–è¡¥å……ï¼Œè¯·ç›¸åº”è°ƒæ•´å†…å®¹

è¯·æä¾›ä¸€ä¸ªå®Œæ•´çš„ã€ç¬¦åˆç”¨æˆ·è¦æ±‚çš„å›å¤ã€‚""")
        ])
        
        # æ ¼å¼åŒ–å¯¹è¯å†å²
        history_str = self._format_conversation_history(
            state.get("conversation_history", [])
        )
        
        # ç”Ÿæˆæ›´æ–°åçš„å›å¤
        update_chain = update_prompt | self.llm
        result = await update_chain.ainvoke({
            "original_input": state["user_input"],
            "conversation_history": history_str,
            "streaming_content": state.get("streaming_content", ""),
            "user_interrupt": state.get("user_interrupt", "")
        })
        
        new_response = result.content
        
        print(f"\nğŸ”„ åŸºäºæ‚¨çš„æ‰“æ–­ç”Ÿæˆçš„æ–°å›å¤:")
        print(f"ğŸ¤– AI: {new_response}")
        
        return {
            "ai_response": new_response,
            "context_updated": True,
            "interrupt_requested": False,
            "metadata": {
                "context_updated_at": datetime.now().isoformat(),
                "update_reason": "user_interrupt"
            }
        }
        
    except Exception as e:
        return {
            "ai_response": f"æ›´æ–°ä¸Šä¸‹æ–‡æ—¶å‡ºç°é”™è¯¯: {str(e)}",
            "context_updated": True,
            "interrupt_requested": False,
            "metadata": {"error": str(e)}
        }
```

### 2. å¯¹è¯å†å²ç®¡ç†

```python
def conversation_update_node(self, state: AdvancedStreamingState) -> Dict:
    """å¯¹è¯æ›´æ–°èŠ‚ç‚¹"""
    conversation_history = state.get("conversation_history", []).copy()
    
    # æ·»åŠ ç”¨æˆ·è¾“å…¥
    conversation_history.append({
        "role": "user",
        "content": state["user_input"],
        "timestamp": state.get("timestamp", datetime.now().isoformat())
    })
    
    # æ·»åŠ AIå›å¤
    conversation_history.append({
        "role": "assistant", 
        "content": state["ai_response"],
        "timestamp": datetime.now().isoformat(),
        "metadata": state.get("metadata", {})
    })
    
    # å¦‚æœæœ‰ç”¨æˆ·æ‰“æ–­ï¼Œæ·»åŠ æ‰“æ–­ä¿¡æ¯
    if state.get("user_interrupt"):
        conversation_history.append({
            "role": "user",
            "content": state["user_interrupt"],
            "timestamp": datetime.now().isoformat(),
            "type": "interrupt"
        })
    
    # é™åˆ¶å†å²é•¿åº¦ï¼ˆä¿ç•™æœ€è¿‘20æ¡æ¶ˆæ¯ï¼‰
    if len(conversation_history) > 20:
        conversation_history = conversation_history[-20:]
    
    return {"conversation_history": conversation_history}

def _format_conversation_history(self, history: List[Dict]) -> str:
    """æ ¼å¼åŒ–å¯¹è¯å†å²"""
    if not history:
        return "æš‚æ— å¯¹è¯å†å²"
    
    formatted = []
    for msg in history[-10:]:  # åªå¤„ç†æœ€è¿‘10æ¡æ¶ˆæ¯
        role = msg.get("role", "unknown")
        content = msg.get("content", "")
        msg_type = msg.get("type", "")
        
        if msg_type == "interrupt":
            formatted.append(f"[ç”¨æˆ·æ‰“æ–­] {content}")
        elif role == "user":
            formatted.append(f"ç”¨æˆ·: {content}")
        elif role == "assistant":
            formatted.append(f"åŠ©æ‰‹: {content}")
    
    return "\n".join(formatted)
```

### 3. æ™ºèƒ½è·¯ç”±å†³ç­–

```python
def decide_next_step(self, state: AdvancedStreamingState) -> str:
    """å†³å®šå·¥ä½œæµçš„ä¸‹ä¸€æ­¥æ“ä½œ"""
    if state.get("interrupt_requested", False) and not state.get("context_updated", False):
        # éœ€è¦æ›´æ–°ä¸Šä¸‹æ–‡
        return "context_update"
    else:
        # ç›´æ¥æ›´æ–°å¯¹è¯å†å²
        return "conversation_update"
```

## å®ç°åŸç†è¯¦è§£

### 1. å¤šçº¿ç¨‹æ¶æ„

```mermaid
graph TB
    A[ä¸»çº¿ç¨‹] --> B[å·¥ä½œæµæ‰§è¡Œ]
    A --> C[è¾“å…¥ç›‘å¬çº¿ç¨‹]
    
    B --> D[æµå¼ç”Ÿæˆ]
    C --> E[é”®ç›˜ç›‘å¬]
    
    D --> F[è¾“å‡ºå†…å®¹]
    E --> G[æ£€æµ‹æ‰“æ–­]
    
    F --> H[æ£€æŸ¥ä¸­æ–­æ ‡å¿—]
    G --> I[è®¾ç½®ä¸­æ–­äº‹ä»¶]
    
    H --> J{æœ‰ä¸­æ–­?}
    J -->|æ˜¯| K[åœæ­¢ç”Ÿæˆ]
    J -->|å¦| L[ç»§ç»­ç”Ÿæˆ]
    
    K --> M[å¤„ç†ä¸­æ–­]
    L --> N[å®Œæˆç”Ÿæˆ]
```

### 2. çŠ¶æ€æµè½¬

```python
# çŠ¶æ€æµè½¬ç¤ºä¾‹
initial_state = {
    "user_input": "è¯·ä»‹ç»äººå·¥æ™ºèƒ½",
    "ai_response": "",
    "interrupt_requested": False,
    "user_interrupt": "",
    "context_updated": False
}

# æµå¼ç”Ÿæˆè¿‡ç¨‹ä¸­
streaming_state = {
    "user_input": "è¯·ä»‹ç»äººå·¥æ™ºèƒ½", 
    "ai_response": "äººå·¥æ™ºèƒ½æ˜¯ä¸€é—¨ç»¼åˆæ€§å­¦ç§‘...",
    "streaming_content": "äººå·¥æ™ºèƒ½æ˜¯ä¸€é—¨ç»¼åˆæ€§å­¦ç§‘...",
    "interrupt_requested": True,  # ç”¨æˆ·æ‰“æ–­
    "user_interrupt": "ç®€å•ç‚¹",
    "context_updated": False
}

# ä¸Šä¸‹æ–‡æ›´æ–°å
updated_state = {
    "user_input": "è¯·ä»‹ç»äººå·¥æ™ºèƒ½",
    "ai_response": "äººå·¥æ™ºèƒ½å°±æ˜¯è®©æœºå™¨å…·å¤‡æ™ºèƒ½çš„æŠ€æœ¯ã€‚",
    "interrupt_requested": False,
    "user_interrupt": "ç®€å•ç‚¹", 
    "context_updated": True
}
```

### 3. å¼‚æ­¥å¤„ç†æœºåˆ¶

```python
async def streaming_with_interrupt():
    """æµå¼å¤„ç†ä¸ä¸­æ–­çš„å¼‚æ­¥åè°ƒ"""
    
    # å¯åŠ¨ç›‘å¬çº¿ç¨‹
    interrupt_handler.start_listening()
    
    try:
        # å¼‚æ­¥æµå¼ç”Ÿæˆ
        async for chunk in llm_stream:
            # è¾“å‡ºå†…å®¹
            print(chunk.content, end="", flush=True)
            
            # æ£€æŸ¥ä¸­æ–­
            if interrupt_handler.check_interrupt():
                # å¤„ç†ä¸­æ–­
                break
            
            # æ§åˆ¶é€Ÿåº¦
            await asyncio.sleep(0.03)
    
    finally:
        # ç¡®ä¿åœæ­¢ç›‘å¬
        interrupt_handler.stop_listening()
```

## æœ€ä½³å®è·µ

### 1. é”™è¯¯å¤„ç†ä¸æ¢å¤

```python
class RobustHumanInLoop:
    async def safe_streaming_response(self, state):
        """å®‰å…¨çš„æµå¼å“åº”å¤„ç†"""
        try:
            return await self.streaming_response_node(state)
        except KeyboardInterrupt:
            # ç”¨æˆ·å¼ºåˆ¶ä¸­æ–­
            return {
                "ai_response": "ç”¨æˆ·å¼ºåˆ¶ä¸­æ–­äº†å¯¹è¯",
                "interrupt_requested": True,
                "user_interrupt": "[å¼ºåˆ¶ä¸­æ–­]"
            }
        except Exception as e:
            logger.error(f"æµå¼å“åº”å‡ºé”™: {e}")
            return {
                "ai_response": f"ç”Ÿæˆå›å¤æ—¶å‡ºç°é”™è¯¯: {str(e)}",
                "interrupt_requested": False
            }
    
    def with_timeout(self, func, timeout=30):
        """ä¸ºäººå·¥è¾“å…¥æ·»åŠ è¶…æ—¶æœºåˆ¶"""
        import signal
        
        def timeout_handler(signum, frame):
            raise TimeoutError("äººå·¥è¾“å…¥è¶…æ—¶")
        
        signal.signal(signal.SIGALRM, timeout_handler)
        signal.alarm(timeout)
        
        try:
            result = func()
            signal.alarm(0)  # å–æ¶ˆè¶…æ—¶
            return result
        except TimeoutError:
            return {"human_feedback": "è¾“å…¥è¶…æ—¶ï¼Œä½¿ç”¨é»˜è®¤å¤„ç†"}
```

### 2. æ€§èƒ½ä¼˜åŒ–

```python
class OptimizedHumanInLoop:
    def __init__(self):
        # ä½¿ç”¨è¿æ¥æ± 
        self.llm = ChatOpenAI(
            model="deepseek-chat",
            streaming=True,
            max_retries=3,
            request_timeout=30
        )
        
        # é™åˆ¶å†å²é•¿åº¦
        self.max_history_length = 20
        
        # ç¼“å­˜å¸¸ç”¨å“åº”
        self.response_cache = {}
    
    def _optimize_history(self, history):
        """ä¼˜åŒ–å¯¹è¯å†å²"""
        if len(history) > self.max_history_length:
            # ä¿ç•™æœ€è¿‘çš„æ¶ˆæ¯
            return history[-self.max_history_length:]
        return history
    
    async def _cached_response(self, prompt_key, inputs):
        """ç¼“å­˜å“åº”"""
        cache_key = f"{prompt_key}_{hash(str(inputs))}"
        
        if cache_key in self.response_cache:
            return self.response_cache[cache_key]
        
        result = await self.llm.ainvoke(inputs)
        self.response_cache[cache_key] = result
        
        return result
```

### 3. ç”¨æˆ·ä½“éªŒä¼˜åŒ–

```python
class UserFriendlyHumanInLoop:
    def __init__(self):
        self.typing_speed = 0.03  # æ‰“å­—æœºæ•ˆæœé€Ÿåº¦
        self.show_progress = True
        
    async def enhanced_streaming_output(self, content_stream):
        """å¢å¼ºçš„æµå¼è¾“å‡º"""
        print("ğŸ¤– AI: ", end="", flush=True)
        
        if self.show_progress:
            print("ğŸ’­ æ€è€ƒä¸­...", end="", flush=True)
            await asyncio.sleep(0.5)
            print("\rğŸ¤– AI: ", end="", flush=True)
        
        total_content = ""
        async for chunk in content_stream:
            if chunk.content:
                total_content += chunk.content
                print(chunk.content, end="", flush=True)
                await asyncio.sleep(self.typing_speed)
        
        print()  # æ¢è¡Œ
        return total_content
    
    def show_interrupt_help(self):
        """æ˜¾ç¤ºæ‰“æ–­å¸®åŠ©ä¿¡æ¯"""
        print("\n" + "="*50)
        print("ğŸ’¡ æ‰“æ–­æç¤º:")
        print("  - è¾“å…¥ 'ç®€å•ç‚¹' è¦æ±‚ç®€åŒ–å›ç­”")
        print("  - è¾“å…¥ 'è¯¦ç»†ç‚¹' è¦æ±‚è¯¦ç»†è¯´æ˜") 
        print("  - è¾“å…¥æ–°é—®é¢˜åˆ‡æ¢è¯é¢˜")
        print("  - è¾“å…¥ 'åœæ­¢' ç»“æŸå½“å‰å›å¤")
        print("="*50)
```

### 4. ç›‘æ§ä¸æ—¥å¿—

```python
class MonitoredHumanInLoop:
    def __init__(self):
        self.metrics = {
            "total_conversations": 0,
            "interrupted_conversations": 0,
            "average_response_time": 0,
            "user_satisfaction": []
        }
    
    def log_conversation_start(self, session_id, user_input):
        """è®°å½•å¯¹è¯å¼€å§‹"""
        logger.info(f"ä¼šè¯ {session_id} å¼€å§‹: {user_input[:50]}...")
        self.metrics["total_conversations"] += 1
    
    def log_interrupt(self, session_id, interrupt_reason):
        """è®°å½•ç”¨æˆ·æ‰“æ–­"""
        logger.info(f"ä¼šè¯ {session_id} è¢«æ‰“æ–­: {interrupt_reason}")
        self.metrics["interrupted_conversations"] += 1
    
    def calculate_interrupt_rate(self):
        """è®¡ç®—æ‰“æ–­ç‡"""
        if self.metrics["total_conversations"] == 0:
            return 0
        return self.metrics["interrupted_conversations"] / self.metrics["total_conversations"]
    
    def get_performance_report(self):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        return {
            "æ€»å¯¹è¯æ•°": self.metrics["total_conversations"],
            "æ‰“æ–­æ¬¡æ•°": self.metrics["interrupted_conversations"], 
            "æ‰“æ–­ç‡": f"{self.calculate_interrupt_rate():.2%}",
            "å¹³å‡å“åº”æ—¶é—´": f"{self.metrics['average_response_time']:.2f}ç§’"
        }
```

## å®é™…åº”ç”¨åœºæ™¯

### 1. æ™ºèƒ½å®¢æœç³»ç»Ÿ

```python
class CustomerServiceBot:
    """æ™ºèƒ½å®¢æœæœºå™¨äºº"""
    
    async def handle_customer_query(self, query, customer_id):
        """å¤„ç†å®¢æˆ·æŸ¥è¯¢"""
        # åˆ†ææŸ¥è¯¢å¤æ‚åº¦
        if self._is_complex_query(query):
            # éœ€è¦äººå·¥å®¢æœä»‹å…¥
            return await self._escalate_to_human(query, customer_id)
        else:
            # AIè‡ªåŠ¨å¤„ç†
            return await self._ai_handle_query(query, customer_id)
    
    def _is_complex_query(self, query):
        """åˆ¤æ–­æ˜¯å¦ä¸ºå¤æ‚æŸ¥è¯¢"""
        complex_keywords = ["æŠ•è¯‰", "é€€æ¬¾", "æ³•å¾‹", "ç´§æ€¥", "ç»ç†"]
        return any(keyword in query for keyword in complex_keywords)
    
    async def _escalate_to_human(self, query, customer_id):
        """å‡çº§åˆ°äººå·¥å®¢æœ"""
        # åˆ›å»ºäººå·¥ä»‹å…¥å·¥ä½œæµ
        workflow = self._create_human_escalation_workflow()
        
        initial_state = {
            "customer_query": query,
            "customer_id": customer_id,
            "urgency_level": self._assess_urgency(query),
            "ai_analysis": await self._analyze_query(query)
        }
        
        return await workflow.ainvoke(initial_state)
```

### 2. ä»£ç å®¡æŸ¥åŠ©æ‰‹

```python
class CodeReviewAssistant:
    """ä»£ç å®¡æŸ¥åŠ©æ‰‹"""
    
    async def review_code(self, code_diff, reviewer_id):
        """å®¡æŸ¥ä»£ç å˜æ›´"""
        # AIåˆæ­¥åˆ†æ
        ai_analysis = await self._ai_analyze_code(code_diff)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦äººå·¥å®¡æŸ¥
        if self._needs_human_review(ai_analysis):
            # è¯·æ±‚äººå·¥å®¡æŸ¥
            human_review = await self._request_human_review(
                code_diff, ai_analysis, reviewer_id
            )
            
            # æ•´åˆAIå’Œäººå·¥çš„å®¡æŸ¥æ„è§
            final_review = await self._merge_reviews(ai_analysis, human_review)
            return final_review
        else:
            return ai_analysis
    
    def _needs_human_review(self, ai_analysis):
        """åˆ¤æ–­æ˜¯å¦éœ€è¦äººå·¥å®¡æŸ¥"""
        risk_indicators = [
            ai_analysis.get("security_issues", []),
            ai_analysis.get("performance_concerns", []),
            ai_analysis.get("architecture_changes", False)
        ]
        return any(risk_indicators)
```

### 3. æ•™è‚²è¾…å¯¼ç³»ç»Ÿ

```python
class TutoringSystem:
    """æ•™è‚²è¾…å¯¼ç³»ç»Ÿ"""
    
    async def tutor_student(self, question, student_id):
        """è¾…å¯¼å­¦ç”Ÿ"""
        # åˆ†æå­¦ç”Ÿé—®é¢˜
        analysis = await self._analyze_student_question(question)
        
        if analysis["difficulty_level"] > 8:
            # å¤æ‚é—®é¢˜éœ€è¦è€å¸ˆä»‹å…¥
            return await self._teacher_intervention_workflow(
                question, student_id, analysis
            )
        else:
            # AIå¯ä»¥å¤„ç†çš„é—®é¢˜
            return await self._ai_tutoring_workflow(
                question, student_id, analysis
            )
    
    async def _teacher_intervention_workflow(self, question, student_id, analysis):
        """è€å¸ˆä»‹å…¥å·¥ä½œæµ"""
        workflow = StateGraph(TutoringState)
        
        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("ai_initial_help", self._ai_initial_help)
        workflow.add_node("request_teacher", self._request_teacher_help)
        workflow.add_node("teacher_guidance", self._teacher_guidance)
        workflow.add_node("final_explanation", self._final_explanation)
        
        # å®šä¹‰æµç¨‹
        workflow.add_edge(START, "ai_initial_help")
        workflow.add_edge("ai_initial_help", "request_teacher")
        workflow.add_edge("request_teacher", "teacher_guidance")
        workflow.add_edge("teacher_guidance", "final_explanation")
        workflow.add_edge("final_explanation", END)
        
        compiled_workflow = workflow.compile()
        
        return await compiled_workflow.ainvoke({
            "question": question,
            "student_id": student_id,
            "analysis": analysis
        })
```

### 4. åŒ»ç–—è¯Šæ–­è¾…åŠ©

```python
class MedicalDiagnosisAssistant:
    """åŒ»ç–—è¯Šæ–­è¾…åŠ©ç³»ç»Ÿ"""
    
    async def assist_diagnosis(self, symptoms, patient_data):
        """è¾…åŠ©è¯Šæ–­"""
        # AIåˆæ­¥åˆ†æ
        ai_assessment = await self._ai_symptom_analysis(symptoms, patient_data)
        
        # é£é™©è¯„ä¼°
        risk_level = self._assess_risk(ai_assessment)
        
        if risk_level >= 7:  # é«˜é£é™©æƒ…å†µ
            # å¿…é¡»æœ‰åŒ»ç”Ÿä»‹å…¥
            return await self._doctor_review_workflow(
                symptoms, patient_data, ai_assessment
            )
        elif risk_level >= 4:  # ä¸­ç­‰é£é™©
            # AIæä¾›å»ºè®®ï¼Œä½†æ ‡è®°éœ€è¦åŒ»ç”Ÿç¡®è®¤
            return await self._ai_with_doctor_confirmation(
                symptoms, patient_data, ai_assessment
            )
        else:  # ä½é£é™©
            # AIå¯ä»¥æä¾›åŸºç¡€å»ºè®®
            return await self._ai_basic_advice(ai_assessment)
    
    def _assess_risk(self, assessment):
        """è¯„ä¼°é£é™©ç­‰çº§ (1-10)"""
        risk_factors = [
            assessment.get("urgent_symptoms", []),
            assessment.get("chronic_conditions", []),
            assessment.get("medication_interactions", []),
            assessment.get("age_risk_factors", False)
        ]
        
        # åŸºäºé£é™©å› ç´ è®¡ç®—é£é™©ç­‰çº§
        risk_score = sum([
            len(assessment.get("urgent_symptoms", [])) * 3,
            len(assessment.get("chronic_conditions", [])) * 2,
            len(assessment.get("medication_interactions", [])) * 2,
            2 if assessment.get("age_risk_factors") else 0
        ])
        
        return min(risk_score, 10)
```

## æ€»ç»“

LangGraph çš„ Human-in-the-Loop åŠŸèƒ½æä¾›äº†å¼ºå¤§è€Œçµæ´»çš„äººæœºäº¤äº’èƒ½åŠ›ï¼š

### æ ¸å¿ƒç‰¹æ€§
1. **å¤šç§äº¤äº’æ¨¡å¼**ï¼šä»ç®€å•å®¡æ ¸åˆ°å®æ—¶æ‰“æ–­
2. **æµå¼è¾“å‡ºæ”¯æŒ**ï¼šå®æ—¶æ˜¾ç¤ºAIç”Ÿæˆè¿‡ç¨‹
3. **æ™ºèƒ½ä¸Šä¸‹æ–‡æ›´æ–°**ï¼šæ ¹æ®ç”¨æˆ·åé¦ˆè°ƒæ•´å›å¤
4. **ä¼šè¯çŠ¶æ€ç®¡ç†**ï¼šæ”¯æŒå¤šä¼šè¯å’ŒçŠ¶æ€æŒä¹…åŒ–
5. **è·¨å¹³å°å…¼å®¹**ï¼šæ”¯æŒä¸åŒæ“ä½œç³»ç»Ÿçš„è¾“å…¥ç›‘å¬

### æŠ€æœ¯ä¼˜åŠ¿
- **å¼‚æ­¥å¤„ç†**ï¼šé«˜æ•ˆçš„å¹¶å‘å¤„ç†èƒ½åŠ›
- **çŠ¶æ€ç®¡ç†**ï¼šå®Œæ•´çš„çŠ¶æ€æµè½¬å’ŒæŒä¹…åŒ–
- **é”™è¯¯æ¢å¤**ï¼šå¥å£®çš„é”™è¯¯å¤„ç†æœºåˆ¶
- **æ€§èƒ½ä¼˜åŒ–**ï¼šæµå¼è¾“å‡ºå’Œç¼“å­˜æœºåˆ¶
- **ç”¨æˆ·å‹å¥½**ï¼šç›´è§‚çš„äº¤äº’ç•Œé¢å’Œæç¤º

### åº”ç”¨ä»·å€¼
- **æé«˜å‡†ç¡®æ€§**ï¼šäººå·¥ä»‹å…¥çº æ­£AIé”™è¯¯
- **å¢å¼ºå¯æ§æ€§**ï¼šç”¨æˆ·å®æ—¶æ§åˆ¶AIè¡Œä¸º
- **æ”¹å–„ä½“éªŒ**ï¼šè‡ªç„¶æµç•…çš„äººæœºå¯¹è¯
- **ç¡®ä¿å®‰å…¨**ï¼šå…³é”®å†³ç­–çš„äººå·¥å®¡æ ¸

é€šè¿‡åˆç†è¿ç”¨è¿™äº›Human-in-the-Loopæ¨¡å¼ï¼Œå¯ä»¥æ„å»ºå‡ºæ—¢æ™ºèƒ½åˆå¯æ§çš„AIåº”ç”¨ç³»ç»Ÿï¼Œåœ¨è‡ªåŠ¨åŒ–å’Œäººå·¥æ§åˆ¶ä¹‹é—´æ‰¾åˆ°æœ€ä½³å¹³è¡¡ç‚¹ã€‚
