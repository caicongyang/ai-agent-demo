"""
LangChain è°ƒç”¨ MCP (Model Context Protocol) æ•°å­¦æœåŠ¡å™¨æ¼”ç¤º
ç»“åˆäº† LangChain å’Œ MCP çš„å¼ºå¤§åŠŸèƒ½ï¼Œå±•ç¤ºå¦‚ä½•ä½¿ç”¨ MCP æ•°å­¦å·¥å…·ä¸ LangChain Agent

ğŸ†• æ–°å¢åŠŸèƒ½ï¼šHTTP é£æ ¼çš„å¤§æ¨¡å‹äº¤äº’æ—¥å¿—
- ğŸ“¤ HTTP REQUEST: å®Œæ•´çš„è¯·æ±‚ JSONï¼ˆç±»ä¼¼ OpenAI API è°ƒç”¨ï¼‰
- ğŸ“¥ HTTP RESPONSE: å®Œæ•´çš„å“åº” JSONï¼ˆç±»ä¼¼ OpenAI API è¿”å›ï¼‰
- ğŸ”§ TOOL CALL: å·¥å…·è°ƒç”¨è¿‡ç¨‹çš„ç®€æ´è®°å½•
- ğŸ¯ AGENT ACTION: Agent å†³ç­–è¿‡ç¨‹çš„å¯è§†åŒ–

æ—¥å¿—æ ¼å¼è¯´æ˜ï¼š
1. ğŸ“¤ HTTP REQUEST: æ˜¾ç¤ºå‘é€ç»™å¤§æ¨¡å‹çš„å®Œæ•´è¯·æ±‚ JSON
   - åŒ…å« messages, model, temperature, tools, tool_choice ç­‰
   - ç±»ä¼¼ POST /v1/chat/completions çš„è¯·æ±‚ä½“
2. ğŸ“¥ HTTP RESPONSE: æ˜¾ç¤ºå¤§æ¨¡å‹è¿”å›çš„å®Œæ•´å“åº” JSON
   - åŒ…å« choices, message, tool_calls, finish_reason ç­‰
   - ç±»ä¼¼ OpenAI API çš„å“åº”æ ¼å¼
3. ğŸ”§ TOOL CALL: å·¥å…·æ‰§è¡Œçš„è¾“å…¥è¾“å‡º
4. ğŸ¯ AGENT ACTION: Agent çš„å†³ç­–å’Œå‚æ•°

ä¾èµ–å®‰è£…:
pip install mcp langchain langchain-openai langchain-mcp-adapters
"""

import asyncio
import os
import json
from typing import Dict, Any, List, Optional
from contextlib import AsyncExitStack
from dataclasses import dataclass
from datetime import datetime
from dotenv import load_dotenv

# MCP ç›¸å…³å¯¼å…¥
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

# LangChain MCP å®˜æ–¹é€‚é…å™¨
from langchain_mcp_adapters.tools import load_mcp_tools

# LangChain ç›¸å…³å¯¼å…¥
from langchain_openai import AzureChatOpenAI
from langchain.tools import BaseTool
from langchain.schema import BaseMessage, HumanMessage, AIMessage
from langchain.agents import AgentExecutor, create_openai_tools_agent
from langchain.prompts import ChatPromptTemplate
from langchain.callbacks.base import BaseCallbackHandler
from langchain.callbacks.manager import CallbackManagerForLLMRun
from pydantic import BaseModel, Field

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


@dataclass
class MCPToolResult:
    """MCP å·¥å…·æ‰§è¡Œç»“æœ"""
    success: bool
    data: Any = None
    error: str = ""
    timestamp: str = ""
    
    def __post_init__(self):
        if not self.timestamp:
            self.timestamp = datetime.now().isoformat()


class DetailedInteractionLogger(BaseCallbackHandler):
    """HTTP é£æ ¼çš„äº¤äº’æ—¥å¿—è®°å½•å™¨"""
    
    def __init__(self, tools=None):
        super().__init__()
        self.interaction_count = 0
        self.tools = tools or []
        self.current_messages = []  # å­˜å‚¨å½“å‰å¯¹è¯çš„æ¶ˆæ¯å†å²
        
    def on_llm_start(self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any) -> None:
        """LLM å¼€å§‹æ—¶çš„å›è°ƒ - HTTP è¯·æ±‚é£æ ¼"""
        self.interaction_count += 1
        
        # è§£ææç¤ºä¿¡æ¯æ„å»ºæ¶ˆæ¯
        self.current_messages = []
        for prompt in prompts:
            # è§£ææç¤ºä¸­çš„æ¶ˆæ¯
            messages = self._parse_prompt_to_messages(prompt)
            self.current_messages.extend(messages)
        
        # æ„å»ºå®Œæ•´çš„è¯·æ±‚ JSON
        request_json = {
            "messages": self.current_messages,
            "model": serialized.get('name', 'Unknown'),
            "temperature": 0,
            "stream": False,
            "tools": self._build_tools_definitions(),
            "tool_choice": "auto",
            "parallel_tool_calls": True
        }
        
        print(f"\n{'='*80}")
        print(f"ğŸ“¤ HTTP REQUEST #{self.interaction_count}")
        print(f"{'='*80}")
        print("POST /v1/chat/completions")
        print("Content-Type: application/json")
        print()
        print(json.dumps(request_json, indent=2, ensure_ascii=False))
        print(f"{'='*80}")
    
    def _parse_prompt_to_messages(self, prompt: str) -> List[Dict[str, Any]]:
        """è§£ææç¤ºæ–‡æœ¬ä¸ºæ¶ˆæ¯åˆ—è¡¨"""
        messages = []
        lines = prompt.split('\n')
        current_role = None
        current_content = []
        
        for line in lines:
            if line.startswith('System: '):
                if current_role and current_content:
                    messages.append({"role": current_role, "content": '\n'.join(current_content)})
                current_role = "system"
                current_content = [line[8:]]  # å»æ‰ "System: "
            elif line.startswith('Human: '):
                if current_role and current_content:
                    messages.append({"role": current_role, "content": '\n'.join(current_content)})
                current_role = "user"
                current_content = [line[7:]]  # å»æ‰ "Human: "
            elif line.startswith('AI: '):
                if current_role and current_content:
                    messages.append({"role": current_role, "content": '\n'.join(current_content)})
                current_role = "assistant"
                current_content = [line[4:]]  # å»æ‰ "AI: "
            elif line.startswith('Tool: '):
                # å·¥å…·ç»“æœï¼Œæ·»åŠ åˆ°å½“å‰æ¶ˆæ¯
                if current_role == "assistant":
                    # è¿™æ˜¯å·¥å…·è°ƒç”¨çš„ç»“æœï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
                    pass
                else:
                    current_content.append(line)
            else:
                if current_content:
                    current_content.append(line)
        
        # æ·»åŠ æœ€åä¸€ä¸ªæ¶ˆæ¯
        if current_role and current_content:
            messages.append({"role": current_role, "content": '\n'.join(current_content)})
        
        return messages
    
    def _build_tools_definitions(self) -> List[Dict[str, Any]]:
        """æ„å»ºå·¥å…·å®šä¹‰"""
        tools_definitions = []
        for tool in self.tools:
            tool_def = {
                "type": "function",
                "function": {
                    "name": tool.name,
                    "description": tool.description,
                }
            }
            
            # å°è¯•è·å–å‚æ•° schema
            if hasattr(tool, 'args_schema') and tool.args_schema:
                try:
                    if hasattr(tool.args_schema, 'model_json_schema'):
                        schema = tool.args_schema.model_json_schema()
                        tool_def["function"]["parameters"] = schema
                    elif isinstance(tool.args_schema, dict):
                        tool_def["function"]["parameters"] = tool.args_schema
                    else:
                        schema = getattr(tool.args_schema, 'schema', None)
                        if schema:
                            tool_def["function"]["parameters"] = schema
                except:
                    pass
            
            tools_definitions.append(tool_def)
        
        return tools_definitions
    
    def on_llm_end(self, response, **kwargs: Any) -> None:
        """LLM ç»“æŸæ—¶çš„å›è°ƒ - HTTP å“åº”é£æ ¼"""
        
        # æ„å»ºå“åº” JSON
        response_json = {
            "id": f"chatcmpl-{self.interaction_count}",
            "object": "chat.completion",
            "created": int(datetime.now().timestamp()),
            "model": "gpt-4o",
            "choices": []
        }
        
        # å¤„ç†å“åº”å†…å®¹
        if hasattr(response, 'generations') and response.generations:
            for i, generation_list in enumerate(response.generations):
                for j, generation in enumerate(generation_list):
                    choice = {
                        "index": i,
                        "message": {
                            "role": "assistant",
                            "content": None,
                            "tool_calls": None
                        },
                        "finish_reason": "stop"
                    }
                    
                    if hasattr(generation, 'message'):
                        message = generation.message
                        choice["message"]["content"] = message.content
                        
                        # å¤„ç†å·¥å…·è°ƒç”¨
                        if hasattr(message, 'tool_calls') and message.tool_calls:
                            choice["finish_reason"] = "tool_calls"
                            tool_calls_list = []
                            
                            for k, tc in enumerate(message.tool_calls):
                                try:
                                    if isinstance(tc, dict):
                                        tool_call_item = {
                                            "id": tc.get('id', f"call_{k}"),
                                            "type": "function",
                                            "function": {
                                                "name": tc.get('name', tc.get('function', {}).get('name', 'unknown')),
                                                "arguments": json.dumps(tc.get('arguments', tc.get('function', {}).get('arguments', {})))
                                            }
                                        }
                                    else:
                                        # å¯¹è±¡ç±»å‹çš„ tool_call
                                        tool_name = getattr(tc, 'name', None)
                                        if not tool_name:
                                            func_attr = getattr(tc, 'function', None)
                                            if func_attr:
                                                tool_name = func_attr.get('name', 'unknown')
                                            else:
                                                tool_name = 'unknown'
                                        
                                        tool_args = getattr(tc, 'arguments', None)
                                        if tool_args is None:
                                            tool_args = getattr(tc, 'args', None)
                                        if tool_args is None:
                                            func_attr = getattr(tc, 'function', None)
                                            if func_attr:
                                                tool_args = func_attr.get('arguments', {})
                                        
                                        tool_call_item = {
                                            "id": getattr(tc, 'id', f"call_{k}"),
                                            "type": "function",
                                            "function": {
                                                "name": tool_name,
                                                "arguments": json.dumps(tool_args or {})
                                            }
                                        }
                                    tool_calls_list.append(tool_call_item)
                                except Exception as e:
                                    print(f"âš ï¸ æ„å»ºå·¥å…·è°ƒç”¨ #{k+1} JSON æ—¶å‡ºé”™: {e}")
                            
                            choice["message"]["tool_calls"] = tool_calls_list
                    else:
                        choice["message"]["content"] = generation.text
                    
                    response_json["choices"].append(choice)
        
        # æ·»åŠ ä½¿ç”¨ç»Ÿè®¡
        response_json["usage"] = {
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "total_tokens": 0
        }
        
        print(f"\n{'='*80}")
        print(f"ğŸ“¥ HTTP RESPONSE #{self.interaction_count}")
        print(f"{'='*80}")
        print("200 OK")
        print("Content-Type: application/json")
        print()
        print(json.dumps(response_json, indent=2, ensure_ascii=False))
        print(f"{'='*80}")
    
    def on_tool_start(self, serialized: Dict[str, Any], input_str: str, **kwargs: Any) -> None:
        """å·¥å…·å¼€å§‹æ‰§è¡Œæ—¶çš„å›è°ƒ"""
        tool_name = serialized.get('name', 'Unknown Tool')
        print(f"\nğŸ”§ [TOOL CALL] {tool_name}")
        print(f"   Input: {input_str}")
    
    def on_tool_end(self, output: str, **kwargs: Any) -> None:
        """å·¥å…·æ‰§è¡Œç»“æŸæ—¶çš„å›è°ƒ"""
        print(f"   Output: {output}")
        print(f"âœ… [TOOL CALL COMPLETED]")
    
    def on_tool_error(self, error: Exception, **kwargs: Any) -> None:
        """å·¥å…·æ‰§è¡Œå‡ºé”™æ—¶çš„å›è°ƒ"""
        print(f"âŒ [TOOL ERROR]: {error}")
    
    def on_agent_action(self, action, **kwargs: Any) -> None:
        """Agent æ‰§è¡ŒåŠ¨ä½œæ—¶çš„å›è°ƒ"""
        print(f"\nğŸ¯ [AGENT ACTION] {action.tool}")
        print(f"   Parameters: {action.tool_input}")
    
    def on_agent_finish(self, finish, **kwargs: Any) -> None:
        """Agent å®Œæˆæ—¶çš„å›è°ƒ"""
        print(f"\nğŸ [AGENT FINISHED]")
        print(f"   Final Output: {finish.return_values.get('output', 'N/A')}")


# æ³¨æ„ï¼šç°åœ¨ä½¿ç”¨ langchain_mcp_adapters.tools.load_mcp_tools å®˜æ–¹é€‚é…å™¨
# ä¸å†éœ€è¦è‡ªå®šä¹‰çš„ MCPToolWrapper å’Œ create_mcp_tool


class MCPLangChainDemo:
    """MCP + LangChain é›†æˆæ¼”ç¤º"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ¼”ç¤º"""
        print("ğŸ¯ å¯åŠ¨ MCP + LangChain æ•°å­¦æœåŠ¡å™¨æ¼”ç¤º")
        print("=" * 40)
        
        # åˆ›å»ºè¯¦ç»†äº¤äº’æ—¥å¿—è®°å½•å™¨ï¼ˆç¨åä¼šè®¾ç½®å·¥å…·ï¼‰
        self.interaction_logger = DetailedInteractionLogger()
        
        # Azure OpenAI é…ç½®
        model_name = os.getenv("AZURE_OPENAI_MODEL", "gpt-4o")
        print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {model_name}")
        
        # æ£€æŸ¥å¿…éœ€çš„ç¯å¢ƒå˜é‡
        api_key = os.getenv("AZURE_OPENAI_API_KEY")
        endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
        api_version = os.getenv("AZURE_OPENAI_API_VERSION", "2024-12-01-preview")
        
        if not api_key or not endpoint:
            missing = []
            if not api_key:
                missing.append("AZURE_OPENAI_API_KEY")
            if not endpoint:
                missing.append("AZURE_OPENAI_ENDPOINT")
            print(f"âŒ ç¼ºå°‘å¿…éœ€çš„ç¯å¢ƒå˜é‡: {', '.join(missing)}")
            raise ValueError(f"è¯·è®¾ç½®ç¯å¢ƒå˜é‡: {', '.join(missing)}")
        
        print("âœ… Azure OpenAI ç¯å¢ƒå˜é‡æ£€æŸ¥é€šè¿‡")
        print(f"ğŸ”— ç«¯ç‚¹: {endpoint}")
        print(f"ğŸ”¢ APIç‰ˆæœ¬: {api_version}")
        
        try:
            self.llm = AzureChatOpenAI(
                azure_endpoint=endpoint,
                api_key=api_key,
                api_version=api_version,
                model=model_name,
                temperature=0,
                callbacks=[self.interaction_logger]  # æ·»åŠ äº¤äº’æ—¥å¿—è®°å½•å™¨
            )
            print("âœ… Azure OpenAI å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸï¼ˆå·²å¯ç”¨è¯¦ç»†äº¤äº’æ—¥å¿—ï¼‰")
        except Exception as e:
            print(f"âŒ Azure OpenAI åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
        self.mcp_session: Optional[ClientSession] = None
        self.exit_stack = AsyncExitStack()
        self.tools = []
        self.agent = None
    
    async def initialize_mcp(self, server_command: str = None, server_args: List[str] = None) -> MCPToolResult:
        """
        åˆå§‹åŒ– MCP è¿æ¥
        
        :param server_command: MCP æœåŠ¡å™¨å‘½ä»¤ (é»˜è®¤è‡ªåŠ¨æ£€æµ‹)
        :param server_args: MCP æœåŠ¡å™¨å‚æ•° (é»˜è®¤: ["math_mcp_server.py"] æˆ– ["-m", "mcp_server_time"])
        :return: åˆå§‹åŒ–ç»“æœ
        """
        import subprocess
        import shutil
        
        # è‡ªåŠ¨æ£€æµ‹å¯ç”¨çš„å‘½ä»¤å’Œåˆ›å»ºæ•°å­¦æœåŠ¡å™¨
        if server_command is None:
            # é¦–å…ˆå°è¯•åˆ›å»ºæ•°å­¦ MCP æœåŠ¡å™¨
            math_server = SimpleMathMCPServer()
            server_file = math_server.create_server_script()
            
            if shutil.which("python") or shutil.which("python3"):
                server_command = "python"
                if server_args is None:
                    server_args = [server_file]
                print(f"âœ… åˆ›å»ºäº†æ•°å­¦ MCP æœåŠ¡å™¨: {server_file}")
            else:
                error_msg = "æœªæ‰¾åˆ°å¯ç”¨çš„ Python å‘½ä»¤"
                print(f"âŒ {error_msg}")
                return MCPToolResult(success=False, error=error_msg)
        
        if server_args is None:
            # é»˜è®¤ä½¿ç”¨æ•°å­¦æœåŠ¡å™¨
            math_server = SimpleMathMCPServer()
            server_file = math_server.create_server_script()
            server_args = [server_file]
        
        try:
            print(f"ğŸ”„ æ­£åœ¨åˆå§‹åŒ– MCP è¿æ¥... (ä½¿ç”¨ {server_command})")
            
            # é…ç½® MCP æœåŠ¡å™¨å‚æ•°
            server_params = StdioServerParameters(
                command=server_command,
                args=server_args,
                env=None
            )
            
            # å»ºç«‹ stdio ä¼ è¾“è¿æ¥
            stdio_transport = await self.exit_stack.enter_async_context(
                stdio_client(server_params)
            )
            stdio, write = stdio_transport
            
            # åˆ›å»ºå®¢æˆ·ç«¯ä¼šè¯
            self.mcp_session = await self.exit_stack.enter_async_context(
                ClientSession(stdio, write)
            )
            
            # åˆå§‹åŒ–ä¼šè¯
            await self.mcp_session.initialize()
            
            # è·å–å¯ç”¨å·¥å…·åˆ—è¡¨
            response = await self.mcp_session.list_tools()
            available_tools = response.tools
            
            print(f"âœ… æˆåŠŸè¿æ¥ï¼Œå‘ç° {len(available_tools)} ä¸ªå·¥å…·:")
            for tool in available_tools:
                print(f"  - {tool.name}: {tool.description}")
            
            # ä½¿ç”¨å®˜æ–¹é€‚é…å™¨åŠ è½½å·¥å…·
            self.tools = await load_mcp_tools(self.mcp_session)
            print(f"âœ… ä½¿ç”¨å®˜æ–¹é€‚é…å™¨åŠ è½½äº† {len(self.tools)} ä¸ªå·¥å…·")
            
            # æ›´æ–°äº¤äº’æ—¥å¿—è®°å½•å™¨çš„å·¥å…·åˆ—è¡¨
            self.interaction_logger.tools = self.tools
            
            # æ‰“å°å·¥å…·çš„è¯¦ç»†å®šä¹‰
            self.print_tools_definition()
            
            return MCPToolResult(
                success=True,
                data={
                    "tools_count": len(available_tools),
                    "tools": [tool.name for tool in available_tools]
                }
            )
            
        except Exception as e:
            error_msg = f"MCP åˆå§‹åŒ–å¤±è´¥: {str(e)}"
            print(f"âŒ {error_msg}")
            return MCPToolResult(success=False, error=error_msg)
    
    def print_tools_definition(self):
        """æ‰“å°å·¥å…·å®šä¹‰çš„è¯¦ç»†ä¿¡æ¯"""
        if not self.tools:
            return
            
        print(f"\nğŸ”§ å·¥å…·å®šä¹‰è¯¦æƒ…:")
        print("=" * 50)
        
        tools_json = []
        for tool in self.tools:
            tool_def = {
                "type": "function",
                "function": {
                    "name": tool.name,
                    "description": tool.description,
                }
            }
            
            # å°è¯•è·å–å‚æ•° schema
            if hasattr(tool, 'args_schema') and tool.args_schema:
                try:
                    # æ£€æŸ¥ args_schema çš„ç±»å‹
                    if hasattr(tool.args_schema, 'model_json_schema'):
                        # Pydantic æ¨¡å‹
                        schema = tool.args_schema.model_json_schema()
                        tool_def["function"]["parameters"] = schema
                    elif isinstance(tool.args_schema, dict):
                        # å·²ç»æ˜¯å­—å…¸æ ¼å¼çš„ schema
                        tool_def["function"]["parameters"] = tool.args_schema
                    else:
                        # å°è¯•å…¶ä»–æ–¹å¼è·å– schema
                        schema = getattr(tool.args_schema, 'schema', None)
                        if schema:
                            tool_def["function"]["parameters"] = schema
                        else:
                            print(f"âš ï¸ æ— æ³•è¯†åˆ«å·¥å…· {tool.name} çš„å‚æ•° schema æ ¼å¼: {type(tool.args_schema)}")
                except Exception as e:
                    print(f"âš ï¸ è·å–å·¥å…· {tool.name} çš„å‚æ•° schema å¤±è´¥: {e}")
            
            tools_json.append(tool_def)
            
            print(f"\nğŸ“‹ å·¥å…·: {tool.name}")
            print(f"æè¿°: {tool.description}")
            if "parameters" in tool_def["function"]:
                print(f"å‚æ•° Schema:")
                print(json.dumps(tool_def["function"]["parameters"], indent=2, ensure_ascii=False))
        
        print(f"\nğŸ“Š å®Œæ•´å·¥å…·å®šä¹‰ JSON:")
        print(json.dumps(tools_json, indent=2, ensure_ascii=False))
        print("=" * 50)
    
    def create_agent(self) -> None:
        """åˆ›å»º LangChain Agent"""
        if not self.tools:
            raise ValueError("è¯·å…ˆåˆå§‹åŒ– MCP å·¥å…·")
        
        # åˆ›å»º Agent æç¤ºæ¨¡æ¿ï¼ˆé’ˆå¯¹æ•°å­¦å·¥å…·ä¼˜åŒ–ï¼‰
        prompt = ChatPromptTemplate.from_messages([
            ("system", "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½æ•°å­¦åŠ©æ‰‹ï¼Œå¯ä»¥ä½¿ç”¨æ•°å­¦å·¥å…·æ¥è¿›è¡Œè®¡ç®—ã€‚ä½ å¯ä»¥è¿›è¡ŒåŠ æ³•ã€ä¹˜æ³•ã€è®¡ç®—çŸ©å½¢é¢ç§¯ã€ç”Ÿæˆæ–æ³¢é‚£å¥‘æ•°åˆ—ç­‰æ“ä½œã€‚è¯·ç”¨ä¸­æ–‡å›ç­”ç”¨æˆ·çš„æ•°å­¦é—®é¢˜ã€‚"),
            ("human", "{input}"),
            ("placeholder", "{agent_scratchpad}"),
        ])
        
        # åˆ›å»º Agent
        agent = create_openai_tools_agent(self.llm, self.tools, prompt)
        self.agent = AgentExecutor(
            agent=agent,
            tools=self.tools,
            verbose=True,
            handle_parsing_errors=True,
            callbacks=[self.interaction_logger]  # æ·»åŠ äº¤äº’æ—¥å¿—è®°å½•å™¨
        )
        print("âœ… åˆ›å»ºäº†æ•°å­¦ OpenAI Tools Agentï¼ˆæ”¯æŒ JSON schemaï¼Œå·²å¯ç”¨è¯¦ç»†äº¤äº’æ—¥å¿—ï¼‰")
    
    async def run_query(self, query: str) -> Dict[str, Any]:
        """
        è¿è¡ŒæŸ¥è¯¢
        
        :param query: ç”¨æˆ·æŸ¥è¯¢
        :return: æŸ¥è¯¢ç»“æœ
        """
        if not self.agent:
            raise ValueError("è¯·å…ˆåˆ›å»º Agent")
        
        try:
            print(f"\nâ“ æŸ¥è¯¢: {query}")
            print("ğŸš€ å¼€å§‹æ‰§è¡ŒæŸ¥è¯¢...")
            print()
            
            # è¿è¡Œ Agentï¼ˆä½¿ç”¨å¼‚æ­¥è°ƒç”¨ï¼‰
            result = await self.agent.ainvoke(
                {"input": query},
                config={"callbacks": [self.interaction_logger]}
            )
            
            print(f"\nğŸ¯ æœ€ç»ˆç»“æœ:")
            print(f"è¾“å‡º: {result.get('output', 'N/A')}")
            
            return {
                "success": True,
                "query": query,
                "result": result,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            error_msg = f"æŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {str(e)}"
            print(f"ğŸ” è¯¦ç»†é”™è¯¯ä¿¡æ¯: {e}")
            return {
                "success": False,
                "query": query,
                "error": error_msg,
                "timestamp": datetime.now().isoformat()
            }
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            await self.exit_stack.aclose()
            print("ğŸ§¹ èµ„æºæ¸…ç†å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸ èµ„æºæ¸…ç†æ—¶å‡ºç°è­¦å‘Š: {e}")


# æ•°å­¦å·¥å…·çš„ç®€å• MCP æœåŠ¡å™¨ç¤ºä¾‹
class SimpleMathMCPServer:
    """ç®€å•çš„æ•°å­¦è¿ç®— MCP æœåŠ¡å™¨ï¼ˆç”¨äºæ¼”ç¤ºï¼‰"""
    
    @staticmethod
    def create_server_script():
        """åˆ›å»ºä¸€ä¸ªç®€å•çš„ MCP æœåŠ¡å™¨è„šæœ¬"""
        server_code = '''#!/usr/bin/env python3
"""
ç®€å•çš„æ•°å­¦ MCP æœåŠ¡å™¨
æä¾›åŸºæœ¬çš„æ•°å­¦è®¡ç®—åŠŸèƒ½
"""

import asyncio
from mcp.server.fastmcp import FastMCP

# åˆ›å»º MCP æœåŠ¡å™¨
mcp = FastMCP("Simple Math Server")

@mcp.tool()
def add(a: int, b: int) -> int:
    """å°†ä¸¤ä¸ªæ•´æ•°ç›¸åŠ """
    result = a + b
    print(f"ğŸ“Š è®¡ç®—: {a} + {b} = {result}")
    return result

@mcp.tool()
def multiply(a: int, b: int) -> int:
    """å°†ä¸¤ä¸ªæ•´æ•°ç›¸ä¹˜"""
    result = a * b
    print(f"ğŸ“Š è®¡ç®—: {a} Ã— {b} = {result}")
    return result

@mcp.tool()
def calculate_area(length: float, width: float) -> float:
    """è®¡ç®—çŸ©å½¢é¢ç§¯"""
    area = length * width
    print(f"ğŸ“Š è®¡ç®—çŸ©å½¢é¢ç§¯: {length} Ã— {width} = {area} å¹³æ–¹ç±³")
    return area

@mcp.tool()
def fibonacci(n: int) -> list:
    """ç”Ÿæˆæ–æ³¢é‚£å¥‘æ•°åˆ—çš„å‰né¡¹"""
    if n <= 0:
        return []
    elif n == 1:
        return [0]
    elif n == 2:
        return [0, 1]
    
    fib = [0, 1]
    for i in range(2, n):
        fib.append(fib[i-1] + fib[i-2])
    
    print(f"ğŸ“Š ç”Ÿæˆæ–æ³¢é‚£å¥‘æ•°åˆ—å‰ {n} é¡¹: {fib}")
    return fib

@mcp.tool()
def power(base: int, exponent: int) -> int:
    """è®¡ç®—å¹‚æ¬¡æ–¹"""
    result = base ** exponent
    print(f"ğŸ“Š è®¡ç®—: {base}^{exponent} = {result}")
    return result

if __name__ == "__main__":
    print("ğŸš€ å¯åŠ¨æ•°å­¦ MCP æœåŠ¡å™¨...")
    mcp.run()
'''
        
        with open("math_mcp_server.py", "w", encoding="utf-8") as f:
            f.write(server_code)
        
        print("ğŸ“ å·²åˆ›å»ºå¢å¼ºç‰ˆ math_mcp_server.py")
        return "math_mcp_server.py"


async def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºå®Œæ•´çš„ MCP + LangChain é›†æˆ"""
    
    # åˆ›å»ºæ¼”ç¤ºå®ä¾‹
    demo = MCPLangChainDemo()
    
    try:
        # åˆå§‹åŒ– MCP è¿æ¥ï¼ˆä½¿ç”¨æ•°å­¦æœåŠ¡å™¨ï¼‰
        result = await demo.initialize_mcp()
        
        if not result.success:
            print(f"âŒ MCP åˆå§‹åŒ–å¤±è´¥: {result.error}")
            return
        
        # åˆ›å»º Agent
        demo.create_agent()
        
        # æµ‹è¯•æŸ¥è¯¢åˆ—è¡¨ï¼ˆæ•°å­¦ç›¸å…³ï¼‰
        test_queries = [
            "è®¡ç®— 15 + 27"
        ]
        
        print("\nğŸ§ª å¼€å§‹æµ‹è¯•:")
        print("-" * 30)
        
        # æ‰§è¡Œæµ‹è¯•æŸ¥è¯¢
        for query in test_queries:
            result = await demo.run_query(query)
            
            if result["success"]:
                print(f"ğŸ’¬ å›ç­”: {result['result']['output']}")
            else:
                print(f"ğŸ’¬ å›ç­”: æŸ¥è¯¢å¤±è´¥: {result['error']}")
            
            print()  # ç©ºè¡Œåˆ†éš”
        
        print("ğŸ‰ æ¼”ç¤ºå®Œæˆ!")
        print("\n" + "="*60)
        print("ğŸ“‹ äº¤äº’æ—¥å¿—è¯´æ˜:")
        print("- æ¯æ¬¡ LLM äº¤äº’éƒ½æœ‰è¯¦ç»†çš„å¼€å§‹/ç»“æŸæ ‡è®°")
        print("- ğŸ”§ è¡¨ç¤ºå·¥å…·è°ƒç”¨è¯·æ±‚å’Œæ‰§è¡Œ")
        print("- ğŸ“Š å®Œæ•´è¯·æ±‚ JSON åŒ…å«äº†æ‰€æœ‰å·¥å…·å®šä¹‰")
        print("- ğŸ¯ Agent åŠ¨ä½œå±•ç¤ºäº†å†³ç­–è¿‡ç¨‹")
        print("- ğŸ’¬ æœ€ç»ˆå“åº”æ˜¯ç”¨æˆ·çœ‹åˆ°çš„ç»“æœ")
        print("="*60)
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        
    finally:
        # æ¸…ç†èµ„æº
        await demo.cleanup()


def run_demo():
    """è¿è¡Œæ¼”ç¤ºçš„ä¾¿æ·å‡½æ•°"""
    asyncio.run(main())


if __name__ == "__main__":
    run_demo()
